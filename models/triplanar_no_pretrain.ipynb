{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to run on WSL2, hence the directml usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daru/code/projects/tbv/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: 1\n",
      "Current device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import nrrd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch_directml as tdml\n",
    "\n",
    "print(f'Available devices: {tdml.device_count()}')\n",
    "print(f'Current device: {tdml.device()}')\n",
    "dml = tdml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        raw_labels = [(*(os.path.splitext(file)[0].split(\"_\")[1:-1]), file) for file in os.listdir(data_dir)]\n",
    "\n",
    "        self.labels = pd.DataFrame(data=raw_labels, columns=[\"pid\", \"age\", \"tbv\", \"filename\"])\n",
    "        self.labels[[\"age\", \"tbv\"]] = self.labels[[\"age\", \"tbv\"]].astype(float)\n",
    "\n",
    "        self.age_minmax = MinMaxScaler()\n",
    "        self.tbv_minmax = MinMaxScaler()\n",
    "        self.age_std = StandardScaler()\n",
    "        self.tbv_std = StandardScaler()\n",
    "\n",
    "        self.labels[[\"age\"]] = self.age_minmax.fit_transform(self.labels[[\"age\"]])\n",
    "        self.labels[[\"tbv\"]] = self.tbv_minmax.fit_transform(self.labels[[\"tbv\"]])\n",
    "        self.labels[[\"age\"]] = self.age_std.fit_transform(self.labels[[\"age\"]])\n",
    "        self.labels[[\"tbv\"]] = self.tbv_std.fit_transform(self.labels[[\"tbv\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        entry = self.labels.iloc[idx]\n",
    "        raster = nrrd.read(os.path.join(self.data_dir, entry[\"filename\"]))[0]\n",
    "        raster = (raster - np.mean(raster)) / np.std(raster) # Standardize the data\n",
    "\n",
    "        return {\"pid\": entry[\"pid\"], \"age\": entry[\"age\"], \"tbv\": entry[\"tbv\"], \"raster\": raster}\n",
    "\n",
    "class RasterNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RasterNet, self).__init__()\n",
    "\n",
    "        # First layer is triplanar as used in S3PNet:\n",
    "        # https://www.sciencedirect.com/science/article/pii/S1077314219301791\n",
    "\n",
    "        self.trpl = nn.Sequential( # 128x128x128 -> 32x40x40\n",
    "            nn.Conv2d(128, 32, kernel_size=9, stride=3, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential( # 96x40x40 -> 192x10x10\n",
    "            nn.Conv2d(96, 192, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential( # 192x10x10 -> 384x5x5\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential( # 384x5x5 -> 768x3x3\n",
    "            nn.Conv2d(384, 768, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential( # 768x3x3 -> 1536x1x1\n",
    "            nn.Conv2d(768, 1536, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1536),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(1536, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, base):\n",
    "        base = base.squeeze().float().to(dml)\n",
    "\n",
    "        xy = self.trpl(base)\n",
    "        yz = self.trpl(torch.transpose(base, 2, 1))\n",
    "        xz = self.trpl(torch.transpose(base, 1, 3))\n",
    "\n",
    "        x = self.conv1(torch.cat((xy, yz, xz), dim=1))\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return self.linear(x.squeeze())\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Step [10/141], Loss: 1.6317\n",
      "Epoch [1/10000], Step [20/141], Loss: 1.4944\n",
      "Epoch [1/10000], Step [30/141], Loss: 0.8318\n",
      "Epoch [1/10000], Step [40/141], Loss: 1.2108\n",
      "Epoch [1/10000], Step [50/141], Loss: 0.9296\n",
      "Epoch [1/10000], Step [60/141], Loss: 1.7470\n",
      "Epoch [1/10000], Step [70/141], Loss: 0.8275\n",
      "Epoch [1/10000], Step [80/141], Loss: 1.2905\n",
      "Epoch [1/10000], Step [90/141], Loss: 1.2129\n",
      "Epoch [1/10000], Step [100/141], Loss: 1.4862\n",
      "Epoch [1/10000], Step [110/141], Loss: 1.8568\n",
      "Epoch [1/10000], Step [120/141], Loss: 1.3904\n",
      "Epoch [1/10000], Step [130/141], Loss: 1.1440\n",
      "Epoch [1/10000], Step [140/141], Loss: 0.5251\n",
      "Epoch [1/10000], Training Loss: 1.7479, Validation Loss: 1.3192\n",
      "Epoch [2/10000], Step [10/141], Loss: 1.3566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m training_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m     25\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     28\u001b[0m     rasters \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mraster\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(dml)\n\u001b[1;32m     29\u001b[0m     tbvs \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtbv\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(dml)\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mRasterDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m entry \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[idx]\n\u001b[1;32m     28\u001b[0m raster \u001b[39m=\u001b[39m nrrd\u001b[39m.\u001b[39mread(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, entry[\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m]))[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m raster \u001b[39m=\u001b[39m (raster \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmean(raster)) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39;49mstd(raster) \u001b[39m# Standardize the data\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m\"\u001b[39m: entry[\u001b[39m\"\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m: entry[\u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mtbv\u001b[39m\u001b[39m\"\u001b[39m: entry[\u001b[39m\"\u001b[39m\u001b[39mtbv\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mraster\u001b[39m\u001b[39m\"\u001b[39m: raster}\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3605\u001b[0m, in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3602\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3603\u001b[0m         \u001b[39mreturn\u001b[39;00m std(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, ddof\u001b[39m=\u001b[39mddof, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3605\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_std(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, ddof\u001b[39m=\u001b[39;49mddof,\n\u001b[1;32m   3606\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/numpy/core/_methods.py:269\u001b[0m, in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_std\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ddof\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[1;32m    268\u001b[0m          where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 269\u001b[0m     ret \u001b[39m=\u001b[39m _var(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, ddof\u001b[39m=\u001b[39;49mddof,\n\u001b[1;32m    270\u001b[0m                keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    273\u001b[0m         ret \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39msqrt(ret, out\u001b[39m=\u001b[39mret)\n",
      "File \u001b[0;32m~/code/projects/tbv/venv/lib/python3.8/site-packages/numpy/core/_methods.py:250\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39m# Most general case; includes handling object arrays containing imaginary\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m# numbers and complex types with non-native byteorder\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmultiply(x, um\u001b[39m.\u001b[39mconjugate(x), out\u001b[39m=\u001b[39mx)\u001b[39m.\u001b[39mreal\n\u001b[0;32m--> 250\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(x, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    252\u001b[0m \u001b[39m# Compute degrees of freedom and make sure it is not negative.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m rcount \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmaximum(rcount \u001b[39m-\u001b[39m ddof, \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = RasterDataset(data_dir=\"../aug_dataset\", pin_memory=True, non_blocking=True)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "model = RasterNet().to(dml)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10000\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=20)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        rasters = data[\"raster\"].float().unsqueeze(1).to(dml)\n",
    "        tbvs = data[\"tbv\"].float().to(dml)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rasters = rasters\n",
    "\n",
    "        predictions = model(rasters).squeeze()\n",
    "        loss = criterion(predictions, tbvs)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}              \", end=\"\\r\")\n",
    "\n",
    "    validation_loss = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            rasters = data[\"raster\"].float().unsqueeze(1).to(dml)\n",
    "            tbvs = data[\"tbv\"].float().to(dml)\n",
    "\n",
    "            predictions = model(rasters).squeeze()\n",
    "            loss = criterion(predictions, tbvs)\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    train_loss = training_loss/len(train_dataloader)\n",
    "    val_loss = validation_loss/len(test_dataloader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}               \")\n",
    "    if early_stopper(validation_loss):\n",
    "        break\n",
    "\n",
    "plt.plot(train_loss_list, label=\"Training Loss\", linewidth=3)\n",
    "plt.plot(val_loss_list, label=\"Validation Loss\", linewidth=3)\n",
    "plt.legend(\"Training Loss\", \"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), \"last_run_weights.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b07e8231a1c09af6c251322c63d0939bf4169cb77497fa1dd2c4891068f3259a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
