{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to run on WSL2, hence the directml usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import nrrd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch_directml as tdml\n",
    "\n",
    "print(f'Available devices: {tdml.device_count()}')\n",
    "print(f'Current device: {tdml.device()}')\n",
    "dml = tdml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        raw_labels = [(*(os.path.splitext(file)[0].split(\"_\")[1:-1]), file) for file in os.listdir(data_dir)]\n",
    "\n",
    "        self.labels = pd.DataFrame(data=raw_labels, columns=[\"pid\", \"age\", \"tbv\", \"filename\"])\n",
    "        self.labels[[\"age\", \"tbv\"]] = self.labels[[\"age\", \"tbv\"]].astype(float)\n",
    "\n",
    "        self.age_minmax = MinMaxScaler()\n",
    "        self.tbv_minmax = MinMaxScaler()\n",
    "        self.age_std = StandardScaler()\n",
    "        self.tbv_std = StandardScaler()\n",
    "\n",
    "        self.labels[[\"age\"]] = self.age_minmax.fit_transform(self.labels[[\"age\"]])\n",
    "        self.labels[[\"tbv\"]] = self.tbv_minmax.fit_transform(self.labels[[\"tbv\"]])\n",
    "        self.labels[[\"age\"]] = self.age_std.fit_transform(self.labels[[\"age\"]])\n",
    "        self.labels[[\"tbv\"]] = self.tbv_std.fit_transform(self.labels[[\"tbv\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        entry = self.labels.iloc[idx]\n",
    "        raster = nrrd.read(os.path.join(self.data_dir, entry[\"filename\"]))[0]\n",
    "        raster = (raster - np.mean(raster)) / np.std(raster) # Standardize the data\n",
    "\n",
    "        return {\"pid\": entry[\"pid\"], \"age\": entry[\"age\"], \"tbv\": entry[\"tbv\"], \"raster\": raster}\n",
    "\n",
    "    def get_unique_pids(self):\n",
    "        return self.labels[\"pid\"].unique()\n",
    "    \n",
    "    def get_indices_from_pids(self, pids):\n",
    "        return self.labels[self.labels[\"pid\"].isin(pids)].index\n",
    "        \n",
    "class RasterNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RasterNet, self).__init__()\n",
    "\n",
    "        # First layer is triplanar as used in S3PNet:\n",
    "        # https://www.sciencedirect.com/science/article/pii/S1077314219301791\n",
    "\n",
    "        self.trpl = nn.Sequential( # 128x128x128 -> 32x40x40\n",
    "            nn.Conv2d(128, 32, kernel_size=9, stride=3, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential( # 96x40x40 -> 192x10x10\n",
    "            nn.Conv2d(96, 192, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential( # 192x10x10 -> 384x5x5\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential( # 384x5x5 -> 768x3x3\n",
    "            nn.Conv2d(384, 768, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential( # 768x3x3 -> 1536x1x1\n",
    "            nn.Conv2d(768, 1536, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1536),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(1536, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, base):\n",
    "        base = base.squeeze().float().to(dml)\n",
    "\n",
    "        xy = self.trpl(base)\n",
    "        yz = self.trpl(torch.transpose(base, 2, 1))\n",
    "        xz = self.trpl(torch.transpose(base, 1, 3))\n",
    "\n",
    "        x = self.conv1(torch.cat((xy, yz, xz), dim=1))\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return self.linear(x.squeeze())\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=25, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model=None):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            if model: self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            if model: self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, tr_dataloader, val_dataloader, early_stopper, num_epochs, trace_func=print):\n",
    "    train_loss_list = pd.Series(dtype=np.float32)\n",
    "    val_loss_list = pd.Series(dtype=np.float32)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        training_loss = 0.\n",
    "        \n",
    "        model.train()\n",
    "        for i, data in enumerate(tr_dataloader):\n",
    "            rasters = data[\"raster\"].float().unsqueeze(1).to(dml)\n",
    "            tbvs = data[\"tbv\"].float().to(dml)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(rasters).squeeze()\n",
    "            loss = criterion(predictions, tbvs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                trace_func(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(tr_dataloader)}], Loss: {loss.item():.4f}              \", end=\"\\r\")\n",
    "\n",
    "        validation_loss = 0.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_dataloader):\n",
    "                rasters = data[\"raster\"].float().unsqueeze(1).to(dml)\n",
    "                tbvs = data[\"tbv\"].float().to(dml)\n",
    "\n",
    "                predictions = model(rasters).squeeze()\n",
    "                loss = criterion(predictions, tbvs)\n",
    "\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        train_loss = training_loss/len(tr_dataloader)\n",
    "        val_loss = validation_loss/len(val_dataloader)\n",
    "        train_loss_list.at[epoch] = train_loss\n",
    "        val_loss_list.at[epoch] = val_loss\n",
    "\n",
    "        trace_func(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}               \")\n",
    "        if early_stopper(validation_loss, model):\n",
    "            break\n",
    "    \n",
    "    return train_loss_list, val_loss_list\n",
    "\n",
    "def get_unscaled_loss(model, criterion, dataloader, dataset):\n",
    "    all_diffs = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        rasters = data[\"raster\"].float().unsqueeze(1)\n",
    "        tbvs = data[\"tbv\"].float()\n",
    "\n",
    "        predictions = model(rasters).squeeze().cpu()\n",
    "        predictions = predictions.detach().numpy().reshape(-1, 1)\n",
    "        tbvs = tbvs.detach().numpy().reshape(-1, 1)\n",
    "        \n",
    "        # Revert the normalization and standardization\n",
    "        predictions = dataset.tbv_std.inverse_transform(predictions)\n",
    "        predictions = dataset.tbv_minmax.inverse_transform(predictions)\n",
    "        tbvs = dataset.tbv_std.inverse_transform(tbvs)\n",
    "        tbvs = dataset.tbv_minmax.inverse_transform(tbvs)\n",
    "\n",
    "        all_diffs.append(np.abs(predictions - tbvs))\n",
    "\n",
    "    all_diffs = np.concatenate(all_diffs)\n",
    "    \n",
    "    avg_diff = np.mean(all_diffs)\n",
    "    std_diff = np.std(all_diffs)\n",
    "\n",
    "    return avg_diff, std_diff\n",
    "\n",
    "def cross_validator(model, dataset, k_fold=5, num_epochs=100, patience=25, learning_rate=0.001, verbose=False, trace_func=print):\n",
    "    # Save model to reset it after each fold\n",
    "    torch.save(model.state_dict(), \"base_weights.pt\")\n",
    "\n",
    "    train_score = pd.Series(dtype=np.float32)\n",
    "    val_score = pd.Series(dtype=np.float32)\n",
    "    unscaled_loss = pd.Series(dtype=np.float32)\n",
    "    \n",
    "    unique_pids = dataset.get_unique_pids()\n",
    "    total_pids = len(unique_pids)\n",
    "    fraction = 1/k_fold\n",
    "    seg = int(total_pids*fraction)\n",
    "    \n",
    "    # tr:train, val:valid; r:right,l:left;  eg: trrr: right index of right side train subset \n",
    "    # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
    "    for i in range(k_fold):\n",
    "        trace_func(f\"Fold {i+1}/{k_fold}\")\n",
    "\n",
    "        model.load_state_dict(torch.load(\"base_weights.pt\"))\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        early_stopper = EarlyStopper(patience=patience, verbose=verbose, path=\"best_weights.pt\", trace_func=trace_func)\n",
    "\n",
    "        trll = 0\n",
    "        trlr = i * seg\n",
    "        vall = trlr\n",
    "        valr = i * seg + seg\n",
    "        trrl = valr\n",
    "        trrr = total_pids\n",
    "        \n",
    "        train_left_indices = list(range(trll,trlr))\n",
    "        train_right_indices = list(range(trrl,trrr))\n",
    "        \n",
    "        train_indices = train_left_indices + train_right_indices\n",
    "        val_indices = list(range(vall,valr))\n",
    "\n",
    "        train_indices = dataset.get_indices_from_pids(unique_pids[train_indices])\n",
    "        val_indices = dataset.get_indices_from_pids(unique_pids[val_indices])\n",
    "\n",
    "        train_set = torch.utils.data.dataset.Subset(dataset, train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(dataset, val_indices)\n",
    "\n",
    "        train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "        test_dataloader = DataLoader(val_set, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "        \n",
    "        train_loss_list, val_loss_list = train(model, criterion, optimizer, train_dataloader, test_dataloader, early_stopper, num_epochs, trace_func)\n",
    "        \n",
    "        model.load_state_dict(torch.load(\"best_weights.pt\"))\n",
    "\n",
    "        train_score.at[i] = train_loss_list\n",
    "        val_score.at[i] = val_loss_list\n",
    "        unscaled_loss.at[i] = get_unscaled_loss(model, criterion, test_dataloader, dataset)\n",
    "    \n",
    "    os.remove(\"base_weights.pt\")\n",
    "    os.remove(\"best_weights.pt\")\n",
    "    \n",
    "    return train_score, val_score, unscaled_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RasterDataset(data_dir=\"../aug_dataset\")\n",
    "model = RasterNet().to(dml)\n",
    "\n",
    "tr_score, val_score, unscaled_loss = cross_validator(model, dataset, k_fold=6, num_epochs=150, patience=15, learning_rate=0.001, verbose=True, trace_func=print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "figure, axis = plt.subplots(3, 2)\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        axis[i][j].plot(tr_score[i*2+j], label=\"Training Loss\", linewidth=1.5)\n",
    "        axis[i][j].plot(val_score[i*2+j], label=\"Validation Loss\", linewidth=1.5)\n",
    "        axis[i][j].set_xlabel(\"Epochs\")\n",
    "        axis[i][j].set_ylabel(\"Loss\")\n",
    "\n",
    "handles, labels = axis[i][j].get_legend_handles_labels()\n",
    "figure.legend(handles, labels, loc='lower center')\n",
    "\n",
    "figure.tight_layout()\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_loss = pd.DataFrame(unscaled_loss.tolist(), columns=['avg','std'], index=unscaled_loss.index)\n",
    "avg_loss = unscaled_loss['avg']\n",
    "std_loss = unscaled_loss['std']\n",
    "\n",
    "plt.errorbar(x=range(6), y=avg_loss, yerr=std_loss, fmt='o')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"TBV Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unscaled_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc74c91ca771926126db323d1115e4d4168b334acf6229be91f4cee6eb3e3cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
