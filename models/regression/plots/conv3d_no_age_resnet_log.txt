 - - - Fold 1/6 - - -

Train volume count: 294
Validation volume count: 76
Validation volumes: ['24' '28' '15' '78' '82' '25' '40' '31' '29' '7' '41' '33' '76' '30' '87'
 '77' '79' '43' '47' '35' '16' '44' '26' '81' '46' '45' '50' '18' '32'
 '21' '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.2025, Validation Loss: 0.4763, Dropout p: 0.30, Patience: 0/50            
Epoch [0002/0300], Training Loss: 1.1715, Validation Loss: 0.3844, Dropout p: 0.30, Patience: 0/50            
Epoch [0003/0300], Training Loss: 1.0220, Validation Loss: 0.6100, Dropout p: 0.30, Patience: 0/50            
Epoch [0004/0300], Training Loss: 0.9381, Validation Loss: 1.0143, Dropout p: 0.30, Patience: 0/50            
Epoch [0005/0300], Training Loss: 0.8570, Validation Loss: 0.7642, Dropout p: 0.30, Patience: 0/50            
Epoch [0006/0300], Training Loss: 0.7588, Validation Loss: 3.3570, Dropout p: 0.30, Patience: 0/50            
Epoch [0007/0300], Training Loss: 0.8164, Validation Loss: 4.4494, Dropout p: 0.30, Patience: 0/50            
Epoch [0008/0300], Training Loss: 0.6854, Validation Loss: 5.8225, Dropout p: 0.30, Patience: 0/50            
Epoch [0009/0300], Training Loss: 0.5367, Validation Loss: 11.7264, Dropout p: 0.30, Patience: 0/50            
Epoch [0010/0300], Training Loss: 0.6270, Validation Loss: 8.5640, Dropout p: 0.30, Patience: 0/50            
Epoch [0011/0300], Training Loss: 0.5190, Validation Loss: 1.3499, Dropout p: 0.30, Patience: 0/50            
Epoch [0012/0300], Training Loss: 0.5795, Validation Loss: 4.2022, Dropout p: 0.30, Patience: 0/50            
Epoch [0013/0300], Training Loss: 0.4682, Validation Loss: 6.8719, Dropout p: 0.30, Patience: 0/50            
Epoch [0014/0300], Training Loss: 0.3959, Validation Loss: 4.8728, Dropout p: 0.30, Patience: 0/50            
Epoch [0015/0300], Training Loss: 0.5336, Validation Loss: 1.9868, Dropout p: 0.30, Patience: 0/50            
Epoch [0016/0300], Training Loss: 0.5147, Validation Loss: 3.7653, Dropout p: 0.30, Patience: 0/50            
Epoch [0017/0300], Training Loss: 0.4784, Validation Loss: 1.1492, Dropout p: 0.30, Patience: 0/50            
Epoch [0018/0300], Training Loss: 0.3646, Validation Loss: 0.7501, Dropout p: 0.30, Patience: 0/50            
Epoch [0019/0300], Training Loss: 0.4422, Validation Loss: 3.2648, Dropout p: 0.30, Patience: 0/50            
Epoch [0020/0300], Training Loss: 0.4136, Validation Loss: 6.5182, Dropout p: 0.30, Patience: 0/50            
Epoch [0021/0300], Training Loss: 0.4056, Validation Loss: 1.1308, Dropout p: 0.30, Patience: 0/50            
Epoch [0022/0300], Training Loss: 0.3301, Validation Loss: 1.0377, Dropout p: 0.30, Patience: 0/50            
Epoch [0023/0300], Training Loss: 0.3810, Validation Loss: 1.6037, Dropout p: 0.30, Patience: 0/50            
Epoch [0024/0300], Training Loss: 0.3178, Validation Loss: 3.4402, Dropout p: 0.30, Patience: 0/50            
Epoch [0025/0300], Training Loss: 0.3214, Validation Loss: 1.9526, Dropout p: 0.30, Patience: 0/50            
Epoch [0026/0300], Training Loss: 0.2928, Validation Loss: 0.7889, Dropout p: 0.30, Patience: 0/50            
Epoch [0027/0300], Training Loss: 0.3653, Validation Loss: 1.4192, Dropout p: 0.30, Patience: 0/50            
Epoch [0028/0300], Training Loss: 0.3034, Validation Loss: 0.5451, Dropout p: 0.30, Patience: 0/50            
Epoch [0029/0300], Training Loss: 0.4020, Validation Loss: 1.0247, Dropout p: 0.30, Patience: 0/50            
Epoch [0030/0300], Training Loss: 0.4250, Validation Loss: 0.6498, Dropout p: 0.30, Patience: 0/50            
Epoch [0031/0300], Training Loss: 0.2871, Validation Loss: 0.4441, Dropout p: 0.30, Patience: 0/50            
Epoch [0032/0300], Training Loss: 0.3188, Validation Loss: 0.5942, Dropout p: 0.30, Patience: 0/50            
Epoch [0033/0300], Training Loss: 0.3327, Validation Loss: 0.5812, Dropout p: 0.30, Patience: 0/50            
Epoch [0034/0300], Training Loss: 0.3192, Validation Loss: 0.8745, Dropout p: 0.30, Patience: 0/50            
Epoch [0035/0300], Training Loss: 0.3066, Validation Loss: 0.5510, Dropout p: 0.30, Patience: 0/50            
Epoch [0036/0300], Training Loss: 0.2640, Validation Loss: 1.0439, Dropout p: 0.30, Patience: 0/50            
Epoch [0037/0300], Training Loss: 0.3584, Validation Loss: 0.4090, Dropout p: 0.30, Patience: 0/50            
Epoch [0038/0300], Training Loss: 0.3030, Validation Loss: 0.4465, Dropout p: 0.30, Patience: 0/50            
Epoch [0039/0300], Training Loss: 0.2988, Validation Loss: 0.3600, Dropout p: 0.30, Patience: 0/50            
Epoch [0040/0300], Training Loss: 0.2833, Validation Loss: 0.4253, Dropout p: 0.30, Patience: 0/50            
Epoch [0041/0300], Training Loss: 0.3049, Validation Loss: 0.5193, Dropout p: 0.30, Patience: 0/50            
Epoch [0042/0300], Training Loss: 0.2743, Validation Loss: 0.3656, Dropout p: 0.30, Patience: 0/50            
Epoch [0043/0300], Training Loss: 0.3149, Validation Loss: 0.3581, Dropout p: 0.30, Patience: 0/50            
Epoch [0044/0300], Training Loss: 0.2912, Validation Loss: 0.3762, Dropout p: 0.30, Patience: 0/50            
Epoch [0045/0300], Training Loss: 0.2608, Validation Loss: 0.2615, Dropout p: 0.30, Patience: 0/50            
Epoch [0046/0300], Training Loss: 0.2396, Validation Loss: 0.3414, Dropout p: 0.30, Patience: 0/50            
Epoch [0047/0300], Training Loss: 0.1959, Validation Loss: 0.2250, Dropout p: 0.30, Patience: 0/50            
Epoch [0048/0300], Training Loss: 0.2336, Validation Loss: 0.2131, Dropout p: 0.30, Patience: 0/50            
Epoch [0049/0300], Training Loss: 0.2178, Validation Loss: 0.4738, Dropout p: 0.30, Patience: 0/50            
Epoch [0050/0300], Training Loss: 0.2051, Validation Loss: 0.3214, Dropout p: 0.30, Patience: 0/50            
Epoch [0051/0300], Training Loss: 0.2361, Validation Loss: 0.2382, Dropout p: 0.30, Patience: 0/50            
Epoch [0052/0300], Training Loss: 0.2464, Validation Loss: 0.2450, Dropout p: 0.30, Patience: 0/50            
Epoch [0053/0300], Training Loss: 0.2313, Validation Loss: 0.2149, Dropout p: 0.30, Patience: 0/50            
Epoch [0054/0300], Training Loss: 0.2217, Validation Loss: 0.1938, Dropout p: 0.30, Patience: 0/50            
Epoch [0055/0300], Training Loss: 0.2974, Validation Loss: 0.3284, Dropout p: 0.30, Patience: 0/50            
Epoch [0056/0300], Training Loss: 0.2092, Validation Loss: 0.2409, Dropout p: 0.30, Patience: 0/50            
Epoch [0057/0300], Training Loss: 0.2041, Validation Loss: 0.2613, Dropout p: 0.30, Patience: 0/50            
Epoch [0058/0300], Training Loss: 0.1811, Validation Loss: 0.2072, Dropout p: 0.30, Patience: 0/50            
Epoch [0059/0300], Training Loss: 0.1743, Validation Loss: 0.2013, Dropout p: 0.30, Patience: 0/50            
Epoch [0060/0300], Training Loss: 0.1937, Validation Loss: 0.1994, Dropout p: 0.30, Patience: 0/50            
Epoch [0061/0300], Training Loss: 0.2029, Validation Loss: 0.3141, Dropout p: 0.30, Patience: 0/50            
Epoch [0062/0300], Training Loss: 0.2116, Validation Loss: 0.1956, Dropout p: 0.30, Patience: 0/50            
Epoch [0063/0300], Training Loss: 0.1719, Validation Loss: 0.1653, Dropout p: 0.30, Patience: 0/50            
Epoch [0064/0300], Training Loss: 0.2084, Validation Loss: 0.2028, Dropout p: 0.30, Patience: 0/50            
Epoch [0065/0300], Training Loss: 0.1750, Validation Loss: 0.1790, Dropout p: 0.30, Patience: 0/50            
Epoch [0066/0300], Training Loss: 0.1926, Validation Loss: 0.1506, Dropout p: 0.30, Patience: 0/50            
Epoch [0067/0300], Training Loss: 0.1649, Validation Loss: 0.1555, Dropout p: 0.30, Patience: 0/50            
Epoch [0068/0300], Training Loss: 0.1878, Validation Loss: 0.2793, Dropout p: 0.30, Patience: 0/50            
Epoch [0069/0300], Training Loss: 0.2040, Validation Loss: 0.3314, Dropout p: 0.30, Patience: 0/50            
Epoch [0070/0300], Training Loss: 0.1894, Validation Loss: 0.2151, Dropout p: 0.30, Patience: 0/50            
Epoch [0071/0300], Training Loss: 0.1429, Validation Loss: 0.1887, Dropout p: 0.30, Patience: 0/50            
Epoch [0072/0300], Training Loss: 0.1916, Validation Loss: 0.2824, Dropout p: 0.30, Patience: 0/50            
Epoch [0073/0300], Training Loss: 0.1591, Validation Loss: 0.2085, Dropout p: 0.30, Patience: 0/50            
Epoch [0074/0300], Training Loss: 0.1654, Validation Loss: 0.1538, Dropout p: 0.30, Patience: 0/50            
Epoch [0075/0300], Training Loss: 0.1489, Validation Loss: 0.1761, Dropout p: 0.30, Patience: 0/50            
Epoch [0076/0300], Training Loss: 0.1601, Validation Loss: 0.1278, Dropout p: 0.30, Patience: 0/50            
Epoch [0077/0300], Training Loss: 0.1312, Validation Loss: 0.1402, Dropout p: 0.30, Patience: 0/50            
Epoch [0078/0300], Training Loss: 0.1330, Validation Loss: 0.0992, Dropout p: 0.30, Patience: 0/50            
Epoch [0079/0300], Training Loss: 0.1183, Validation Loss: 0.2039, Dropout p: 0.30, Patience: 0/50            
Epoch [0080/0300], Training Loss: 0.2124, Validation Loss: 0.2350, Dropout p: 0.30, Patience: 0/50            
Epoch [0081/0300], Training Loss: 0.1252, Validation Loss: 0.0832, Dropout p: 0.30, Patience: 0/50            
Epoch [0082/0300], Training Loss: 0.1329, Validation Loss: 0.1235, Dropout p: 0.30, Patience: 0/50            
Epoch [0083/0300], Training Loss: 0.1654, Validation Loss: 0.1282, Dropout p: 0.30, Patience: 0/50            
Epoch [0084/0300], Training Loss: 0.1569, Validation Loss: 0.1640, Dropout p: 0.30, Patience: 0/50            
Epoch [0085/0300], Training Loss: 0.1265, Validation Loss: 0.0924, Dropout p: 0.30, Patience: 0/50            
Epoch [0086/0300], Training Loss: 0.1191, Validation Loss: 0.1292, Dropout p: 0.30, Patience: 0/50            
Epoch [0087/0300], Training Loss: 0.1944, Validation Loss: 0.2367, Dropout p: 0.30, Patience: 0/50            
Epoch [0088/0300], Training Loss: 0.1414, Validation Loss: 0.1436, Dropout p: 0.30, Patience: 0/50            
Epoch [0089/0300], Training Loss: 0.1246, Validation Loss: 0.1155, Dropout p: 0.30, Patience: 0/50            
Epoch [0090/0300], Training Loss: 0.1331, Validation Loss: 0.0859, Dropout p: 0.30, Patience: 0/50            
Epoch [0091/0300], Training Loss: 0.1134, Validation Loss: 0.1050, Dropout p: 0.30, Patience: 0/50            
Epoch [0092/0300], Training Loss: 0.1170, Validation Loss: 0.1060, Dropout p: 0.30, Patience: 0/50            
Epoch [0093/0300], Training Loss: 0.0863, Validation Loss: 0.1245, Dropout p: 0.30, Patience: 0/50            
Epoch [0094/0300], Training Loss: 0.1056, Validation Loss: 0.1569, Dropout p: 0.30, Patience: 0/50            
Epoch [0095/0300], Training Loss: 0.1148, Validation Loss: 0.1021, Dropout p: 0.30, Patience: 0/50            
Epoch [0096/0300], Training Loss: 0.1178, Validation Loss: 0.0901, Dropout p: 0.30, Patience: 0/50            
Epoch [0097/0300], Training Loss: 0.1228, Validation Loss: 0.1171, Dropout p: 0.30, Patience: 0/50            
Epoch [0098/0300], Training Loss: 0.0909, Validation Loss: 0.1188, Dropout p: 0.30, Patience: 0/50            
Epoch [0099/0300], Training Loss: 0.1057, Validation Loss: 0.1455, Dropout p: 0.30, Patience: 0/50            
Epoch [0100/0300], Training Loss: 0.1025, Validation Loss: 0.1379, Dropout p: 0.30, Patience: 0/50            
Epoch [0101/0300], Training Loss: 0.1083, Validation Loss: 0.1336, Dropout p: 0.30, Patience: 0/50            
Epoch [0102/0300], Training Loss: 0.0957, Validation Loss: 0.1090, Dropout p: 0.30, Patience: 0/50            
Epoch [0103/0300], Training Loss: 0.1207, Validation Loss: 0.1385, Dropout p: 0.30, Patience: 0/50            
Epoch [0104/0300], Training Loss: 0.1035, Validation Loss: 0.0858, Dropout p: 0.30, Patience: 0/50            
Epoch [0105/0300], Training Loss: 0.0903, Validation Loss: 0.0912, Dropout p: 0.30, Patience: 0/50            
Epoch [0106/0300], Training Loss: 0.0952, Validation Loss: 0.0992, Dropout p: 0.30, Patience: 0/50            
Epoch [0107/0300], Training Loss: 0.0984, Validation Loss: 0.1069, Dropout p: 0.30, Patience: 0/50            
Epoch [0108/0300], Training Loss: 0.0745, Validation Loss: 0.1033, Dropout p: 0.30, Patience: 0/50            
Epoch [0109/0300], Training Loss: 0.0934, Validation Loss: 0.0940, Dropout p: 0.30, Patience: 0/50            
Epoch [0110/0300], Training Loss: 0.0893, Validation Loss: 0.1074, Dropout p: 0.30, Patience: 0/50            
Epoch [0111/0300], Training Loss: 0.1099, Validation Loss: 0.0855, Dropout p: 0.30, Patience: 0/50            
Epoch [0112/0300], Training Loss: 0.0894, Validation Loss: 0.0935, Dropout p: 0.30, Patience: 0/50            
Epoch [0113/0300], Training Loss: 0.0823, Validation Loss: 0.0865, Dropout p: 0.30, Patience: 0/50            
Epoch [0114/0300], Training Loss: 0.0883, Validation Loss: 0.0857, Dropout p: 0.30, Patience: 0/50            
Epoch [0115/0300], Training Loss: 0.1032, Validation Loss: 0.0870, Dropout p: 0.30, Patience: 0/50            
Epoch [0116/0300], Training Loss: 0.0820, Validation Loss: 0.0975, Dropout p: 0.30, Patience: 0/50            
Epoch [0117/0300], Training Loss: 0.0700, Validation Loss: 0.0993, Dropout p: 0.30, Patience: 0/50            
Epoch [0118/0300], Training Loss: 0.0869, Validation Loss: 0.0991, Dropout p: 0.30, Patience: 0/50            
Epoch [0119/0300], Training Loss: 0.0848, Validation Loss: 0.1077, Dropout p: 0.30, Patience: 0/50            
Epoch [0120/0300], Training Loss: 0.0829, Validation Loss: 0.0902, Dropout p: 0.30, Patience: 0/50            
Epoch [0121/0300], Training Loss: 0.0887, Validation Loss: 0.0856, Dropout p: 0.30, Patience: 0/50            
Epoch [0122/0300], Training Loss: 0.0853, Validation Loss: 0.0882, Dropout p: 0.30, Patience: 0/50            
Epoch [0123/0300], Training Loss: 0.0706, Validation Loss: 0.0842, Dropout p: 0.30, Patience: 0/50            
Epoch [0124/0300], Training Loss: 0.0811, Validation Loss: 0.0919, Dropout p: 0.30, Patience: 0/50            
Epoch [0125/0300], Training Loss: 0.0832, Validation Loss: 0.0914, Dropout p: 0.30, Patience: 0/50            
Epoch [0126/0300], Training Loss: 0.0854, Validation Loss: 0.0906, Dropout p: 0.30, Patience: 0/50            
Epoch [0127/0300], Training Loss: 0.0934, Validation Loss: 0.0957, Dropout p: 0.30, Patience: 1/50            
Epoch [0128/0300], Training Loss: 0.0836, Validation Loss: 0.1054, Dropout p: 0.30, Patience: 2/50            
Epoch [0129/0300], Training Loss: 0.0836, Validation Loss: 0.0915, Dropout p: 0.30, Patience: 3/50            
Epoch [0130/0300], Training Loss: 0.0813, Validation Loss: 0.0962, Dropout p: 0.30, Patience: 4/50            
Epoch [0131/0300], Training Loss: 0.1034, Validation Loss: 0.1036, Dropout p: 0.30, Patience: 5/50            
Epoch [0132/0300], Training Loss: 0.0670, Validation Loss: 0.0934, Dropout p: 0.30, Patience: 6/50            
Epoch [0133/0300], Training Loss: 0.0842, Validation Loss: 0.0880, Dropout p: 0.30, Patience: 0/50            
Epoch [0134/0300], Training Loss: 0.0667, Validation Loss: 0.0886, Dropout p: 0.30, Patience: 1/50            
Epoch [0135/0300], Training Loss: 0.0820, Validation Loss: 0.0985, Dropout p: 0.30, Patience: 2/50            
Epoch [0136/0300], Training Loss: 0.0804, Validation Loss: 0.0973, Dropout p: 0.30, Patience: 3/50            
Epoch [0137/0300], Training Loss: 0.0750, Validation Loss: 0.1024, Dropout p: 0.30, Patience: 4/50            
Epoch [0138/0300], Training Loss: 0.0807, Validation Loss: 0.1010, Dropout p: 0.30, Patience: 5/50            
Epoch [0139/0300], Training Loss: 0.0693, Validation Loss: 0.0957, Dropout p: 0.30, Patience: 6/50            
Epoch [0140/0300], Training Loss: 0.0774, Validation Loss: 0.0947, Dropout p: 0.30, Patience: 7/50            
Epoch [0141/0300], Training Loss: 0.0768, Validation Loss: 0.0985, Dropout p: 0.30, Patience: 8/50            
Epoch [0142/0300], Training Loss: 0.0803, Validation Loss: 0.0959, Dropout p: 0.30, Patience: 9/50            
Epoch [0143/0300], Training Loss: 0.0706, Validation Loss: 0.0963, Dropout p: 0.30, Patience: 10/50            
Epoch [0144/0300], Training Loss: 0.0738, Validation Loss: 0.0959, Dropout p: 0.30, Patience: 11/50            
Epoch [0145/0300], Training Loss: 0.0983, Validation Loss: 0.1065, Dropout p: 0.30, Patience: 12/50            
Epoch [0146/0300], Training Loss: 0.0746, Validation Loss: 0.0972, Dropout p: 0.30, Patience: 13/50            
Epoch [0147/0300], Training Loss: 0.0590, Validation Loss: 0.0973, Dropout p: 0.30, Patience: 14/50            
Epoch [0148/0300], Training Loss: 0.0793, Validation Loss: 0.1017, Dropout p: 0.30, Patience: 15/50            
Epoch [0149/0300], Training Loss: 0.0682, Validation Loss: 0.0902, Dropout p: 0.30, Patience: 16/50            
Epoch [0150/0300], Training Loss: 0.0645, Validation Loss: 0.0983, Dropout p: 0.30, Patience: 17/50            
Epoch [0151/0300], Training Loss: 0.0723, Validation Loss: 0.0924, Dropout p: 0.30, Patience: 18/50            
Epoch [0152/0300], Training Loss: 0.0654, Validation Loss: 0.1021, Dropout p: 0.30, Patience: 19/50            
Epoch [0153/0300], Training Loss: 0.0826, Validation Loss: 0.0927, Dropout p: 0.30, Patience: 20/50            
Epoch [0154/0300], Training Loss: 0.0713, Validation Loss: 0.0965, Dropout p: 0.30, Patience: 21/50            
Epoch [0155/0300], Training Loss: 0.0749, Validation Loss: 0.0914, Dropout p: 0.30, Patience: 22/50            
Epoch [0156/0300], Training Loss: 0.0688, Validation Loss: 0.0930, Dropout p: 0.30, Patience: 23/50            
Epoch [0157/0300], Training Loss: 0.0711, Validation Loss: 0.0927, Dropout p: 0.30, Patience: 24/50            
Epoch [0158/0300], Training Loss: 0.0677, Validation Loss: 0.0940, Dropout p: 0.30, Patience: 25/50            
Epoch [0159/0300], Training Loss: 0.0822, Validation Loss: 0.1005, Dropout p: 0.30, Patience: 26/50            
Epoch [0160/0300], Training Loss: 0.0765, Validation Loss: 0.0936, Dropout p: 0.30, Patience: 27/50            
Epoch [0161/0300], Training Loss: 0.0753, Validation Loss: 0.1075, Dropout p: 0.30, Patience: 28/50            
Epoch [0162/0300], Training Loss: 0.0703, Validation Loss: 0.0978, Dropout p: 0.30, Patience: 29/50            
Epoch [0163/0300], Training Loss: 0.0789, Validation Loss: 0.1009, Dropout p: 0.30, Patience: 30/50            
Epoch [0164/0300], Training Loss: 0.0715, Validation Loss: 0.0898, Dropout p: 0.30, Patience: 31/50            
Epoch [0165/0300], Training Loss: 0.0831, Validation Loss: 0.0906, Dropout p: 0.30, Patience: 32/50            
Epoch [0166/0300], Training Loss: 0.0839, Validation Loss: 0.0981, Dropout p: 0.30, Patience: 33/50            
Epoch [0167/0300], Training Loss: 0.0660, Validation Loss: 0.0896, Dropout p: 0.30, Patience: 34/50            
Epoch [0168/0300], Training Loss: 0.0891, Validation Loss: 0.0919, Dropout p: 0.30, Patience: 35/50            
Epoch [0169/0300], Training Loss: 0.0746, Validation Loss: 0.0948, Dropout p: 0.30, Patience: 36/50            
Epoch [0170/0300], Training Loss: 0.0665, Validation Loss: 0.0940, Dropout p: 0.30, Patience: 37/50            
Epoch [0171/0300], Training Loss: 0.0942, Validation Loss: 0.1040, Dropout p: 0.30, Patience: 38/50            
Epoch [0172/0300], Training Loss: 0.0776, Validation Loss: 0.0984, Dropout p: 0.30, Patience: 39/50            
Epoch [0173/0300], Training Loss: 0.0798, Validation Loss: 0.0925, Dropout p: 0.30, Patience: 40/50            
Epoch [0174/0300], Training Loss: 0.0621, Validation Loss: 0.0998, Dropout p: 0.30, Patience: 41/50            
Epoch [0175/0300], Training Loss: 0.0732, Validation Loss: 0.0996, Dropout p: 0.30, Patience: 42/50            
Epoch [0176/0300], Training Loss: 0.0629, Validation Loss: 0.0853, Dropout p: 0.30, Patience: 0/50            
Epoch [0177/0300], Training Loss: 0.0776, Validation Loss: 0.0897, Dropout p: 0.30, Patience: 1/50            
Epoch [0178/0300], Training Loss: 0.0698, Validation Loss: 0.0930, Dropout p: 0.30, Patience: 2/50            
Epoch [0179/0300], Training Loss: 0.1030, Validation Loss: 0.0897, Dropout p: 0.30, Patience: 3/50            
Epoch [0180/0300], Training Loss: 0.0825, Validation Loss: 0.1024, Dropout p: 0.30, Patience: 4/50            
Epoch [0181/0300], Training Loss: 0.0738, Validation Loss: 0.1012, Dropout p: 0.30, Patience: 5/50            
Epoch [0182/0300], Training Loss: 0.0735, Validation Loss: 0.0909, Dropout p: 0.30, Patience: 6/50            
Epoch [0183/0300], Training Loss: 0.0640, Validation Loss: 0.0933, Dropout p: 0.30, Patience: 7/50            
Epoch [0184/0300], Training Loss: 0.0631, Validation Loss: 0.0957, Dropout p: 0.30, Patience: 8/50            
Epoch [0185/0300], Training Loss: 0.0636, Validation Loss: 0.0956, Dropout p: 0.30, Patience: 9/50            
Epoch [0186/0300], Training Loss: 0.0798, Validation Loss: 0.0946, Dropout p: 0.30, Patience: 10/50            
Epoch [0187/0300], Training Loss: 0.0737, Validation Loss: 0.1004, Dropout p: 0.30, Patience: 11/50            
Epoch [0188/0300], Training Loss: 0.0768, Validation Loss: 0.0928, Dropout p: 0.30, Patience: 12/50            
Epoch [0189/0300], Training Loss: 0.0772, Validation Loss: 0.0913, Dropout p: 0.30, Patience: 13/50            
Epoch [0190/0300], Training Loss: 0.0713, Validation Loss: 0.0962, Dropout p: 0.30, Patience: 14/50            
Epoch [0191/0300], Training Loss: 0.0743, Validation Loss: 0.0904, Dropout p: 0.30, Patience: 15/50            
Epoch [0192/0300], Training Loss: 0.0648, Validation Loss: 0.0985, Dropout p: 0.30, Patience: 16/50            
Epoch [0193/0300], Training Loss: 0.0755, Validation Loss: 0.0979, Dropout p: 0.30, Patience: 17/50            
Epoch [0194/0300], Training Loss: 0.0717, Validation Loss: 0.0997, Dropout p: 0.30, Patience: 18/50            
Epoch [0195/0300], Training Loss: 0.0645, Validation Loss: 0.0917, Dropout p: 0.30, Patience: 19/50            
Epoch [0196/0300], Training Loss: 0.0760, Validation Loss: 0.0894, Dropout p: 0.30, Patience: 20/50            
Epoch [0197/0300], Training Loss: 0.0679, Validation Loss: 0.0955, Dropout p: 0.30, Patience: 21/50            
Epoch [0198/0300], Training Loss: 0.0741, Validation Loss: 0.0972, Dropout p: 0.30, Patience: 22/50            
Epoch [0199/0300], Training Loss: 0.0739, Validation Loss: 0.0927, Dropout p: 0.30, Patience: 23/50            
Epoch [0200/0300], Training Loss: 0.0682, Validation Loss: 0.0976, Dropout p: 0.30, Patience: 24/50            
Epoch [0201/0300], Training Loss: 0.0760, Validation Loss: 0.0959, Dropout p: 0.30, Patience: 25/50            
Epoch [0202/0300], Training Loss: 0.0735, Validation Loss: 0.0989, Dropout p: 0.30, Patience: 26/50            
Epoch [0203/0300], Training Loss: 0.0694, Validation Loss: 0.0969, Dropout p: 0.30, Patience: 27/50            
Epoch [0204/0300], Training Loss: 0.0814, Validation Loss: 0.0914, Dropout p: 0.30, Patience: 28/50            
Epoch [0205/0300], Training Loss: 0.0721, Validation Loss: 0.0932, Dropout p: 0.30, Patience: 29/50            
Epoch [0206/0300], Training Loss: 0.0608, Validation Loss: 0.0990, Dropout p: 0.30, Patience: 30/50            
Epoch [0207/0300], Training Loss: 0.0735, Validation Loss: 0.0917, Dropout p: 0.30, Patience: 31/50            
Epoch [0208/0300], Training Loss: 0.0760, Validation Loss: 0.0918, Dropout p: 0.30, Patience: 32/50            
Epoch [0209/0300], Training Loss: 0.0814, Validation Loss: 0.0906, Dropout p: 0.30, Patience: 33/50            
Epoch [0210/0300], Training Loss: 0.0686, Validation Loss: 0.0918, Dropout p: 0.30, Patience: 34/50            
Epoch [0211/0300], Training Loss: 0.0903, Validation Loss: 0.0929, Dropout p: 0.30, Patience: 35/50            
Epoch [0212/0300], Training Loss: 0.0876, Validation Loss: 0.0922, Dropout p: 0.30, Patience: 36/50            
Epoch [0213/0300], Training Loss: 0.0586, Validation Loss: 0.0961, Dropout p: 0.30, Patience: 37/50            
Epoch [0214/0300], Training Loss: 0.0768, Validation Loss: 0.0887, Dropout p: 0.30, Patience: 38/50            
Epoch [0215/0300], Training Loss: 0.0616, Validation Loss: 0.0943, Dropout p: 0.30, Patience: 39/50            
Epoch [0216/0300], Training Loss: 0.0795, Validation Loss: 0.1009, Dropout p: 0.30, Patience: 40/50            
Epoch [0217/0300], Training Loss: 0.0855, Validation Loss: 0.0944, Dropout p: 0.30, Patience: 41/50            
Epoch [0218/0300], Training Loss: 0.0616, Validation Loss: 0.0926, Dropout p: 0.30, Patience: 42/50            
Epoch [0219/0300], Training Loss: 0.0602, Validation Loss: 0.0941, Dropout p: 0.30, Patience: 43/50            
Epoch [0220/0300], Training Loss: 0.0813, Validation Loss: 0.0937, Dropout p: 0.30, Patience: 44/50            
Epoch [0221/0300], Training Loss: 0.0715, Validation Loss: 0.0957, Dropout p: 0.30, Patience: 45/50            
Epoch [0222/0300], Training Loss: 0.0817, Validation Loss: 0.0964, Dropout p: 0.30, Patience: 46/50            
Epoch [0223/0300], Training Loss: 0.0855, Validation Loss: 0.1066, Dropout p: 0.30, Patience: 47/50            
Epoch [0224/0300], Training Loss: 0.0748, Validation Loss: 0.0993, Dropout p: 0.30, Patience: 48/50            
Epoch [0225/0300], Training Loss: 0.0683, Validation Loss: 0.1039, Dropout p: 0.30, Patience: 49/50            
Epoch [0226/0300], Training Loss: 0.0717, Validation Loss: 0.0985, Dropout p: 0.30, Patience: 50/50            

Evaluating model on test set...
Predicted TBV: 176.47	Actual TBV: 171.57	Difference: 4.90
Predicted TBV: 184.67	Actual TBV: 201.02	Difference: 16.35
Predicted TBV: 216.34	Actual TBV: 216.91	Difference: 0.57
Predicted TBV: 266.26	Actual TBV: 255.22	Difference: 11.04
Predicted TBV: 70.85	Actual TBV: 73.72	Difference: 2.87
Predicted TBV: 253.95	Actual TBV: 229.56	Difference: 24.39
Predicted TBV: 236.47	Actual TBV: 297.97	Difference: 61.50
Predicted TBV: 221.50	Actual TBV: 246.01	Difference: 24.51
Predicted TBV: 179.07	Actual TBV: 173.80	Difference: 5.27
Predicted TBV: 155.29	Actual TBV: 177.28	Difference: 21.99
Predicted TBV: 160.19	Actual TBV: 175.59	Difference: 15.40
Predicted TBV: 216.02	Actual TBV: 223.44	Difference: 7.42
Predicted TBV: 242.56	Actual TBV: 248.90	Difference: 6.34
Predicted TBV: 192.77	Actual TBV: 191.24	Difference: 1.53
Predicted TBV: 169.89	Actual TBV: 164.27	Difference: 5.62
Predicted TBV: 105.09	Actual TBV: 121.69	Difference: 16.60
Predicted TBV: 248.60	Actual TBV: 261.03	Difference: 12.43
Predicted TBV: 173.70	Actual TBV: 176.77	Difference: 3.07
Predicted TBV: 167.21	Actual TBV: 171.73	Difference: 4.52
Predicted TBV: 306.72	Actual TBV: 278.87	Difference: 27.85
Predicted TBV: 230.25	Actual TBV: 218.52	Difference: 11.73
Predicted TBV: 196.06	Actual TBV: 193.38	Difference: 2.68
Predicted TBV: 86.86	Actual TBV: 76.97	Difference: 9.89
Predicted TBV: 346.99	Actual TBV: 347.91	Difference: 0.92
Predicted TBV: 261.55	Actual TBV: 261.97	Difference: 0.42
Predicted TBV: 235.82	Actual TBV: 227.14	Difference: 8.68
Predicted TBV: 189.60	Actual TBV: 195.02	Difference: 5.42
Predicted TBV: 147.84	Actual TBV: 177.13	Difference: 29.29
Predicted TBV: 195.80	Actual TBV: 169.96	Difference: 25.84
Predicted TBV: 176.84	Actual TBV: 181.50	Difference: 4.66
Predicted TBV: 168.52	Actual TBV: 177.62	Difference: 9.10
Predicted TBV: 204.53	Actual TBV: 178.51	Difference: 26.02
Predicted TBV: 216.59	Actual TBV: 218.43	Difference: 1.84
Predicted TBV: 223.62	Actual TBV: 229.76	Difference: 6.14
Predicted TBV: 224.03	Actual TBV: 248.58	Difference: 24.55
Predicted TBV: 235.77	Actual TBV: 239.22	Difference: 3.45
Predicted TBV: 185.49	Actual TBV: 187.35	Difference: 1.86
Predicted TBV: 258.66	Actual TBV: 268.50	Difference: 9.84
Predicted TBV: 330.64	Actual TBV: 326.10	Difference: 4.54
Predicted TBV: 200.97	Actual TBV: 232.66	Difference: 31.69

Evaluating model with 30 Bayesian runs...
Refused raster with error 9.77. TBV: 0.57.
Refused raster with error 10.09. TBV: 61.50.
Refused raster with error 9.73. TBV: 12.43.
Refused raster with error 9.79. TBV: 27.85.
Refused raster with error 9.66. TBV: 0.92.
Refused raster with error 16.62. TBV: 25.84.
Refused raster with error 9.65. TBV: 9.84.
Refused raster with error 15.45. TBV: 4.54.
Refused raster with error 11.77. TBV: 31.69.
Refused raster with error 12.08. TBV: 7.74.
Refused raster with error 10.77. TBV: 1.20.
Refused raster with error 9.96. TBV: 36.38.
Refused raster with error 11.80. TBV: 9.55.
Refused raster with error 9.26. TBV: 11.36.
Refused raster with error 13.41. TBV: 5.28.
Refused raster with error 10.76. TBV: 23.33.
Refused raster with error 13.05. TBV: 18.06.
Refused raster with error 16.44. TBV: 18.45.

- - - - - -
Total Raster Count: 76
Refused Raster Count: 18
- - - - - -

- - - - - -
Non-bayesian prediction:
Mean Absolute Error: 12.29 cc
Standard Deviation: 10.73 cc
Big error count (>30): 5
Big error mean: 39.10 cc
Big error std: 11.41 cc
- - - - - -

- - - - - -
Bayesian prediction:
Mean Absolute Error: 10.82 cc
Standard Deviation: 8.41 cc
Big error count (>30): 2
Big error mean: 32.97 cc
Big error std: 2.39 cc
- - - - - -

 - - - Fold 2/6 - - -

Train volume count: 288
Validation volume count: 82
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '29' '7' '41' '33' '76' '30' '87'
 '77' '79' '43' '47' '35' '16' '44' '26' '81' '46' '45' '50' '18' '32'
 '21' '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.0032, Validation Loss: 1.4294, Dropout p: 0.30, Patience: 0/50            
Epoch [0002/0300], Training Loss: 0.9931, Validation Loss: 1.7001, Dropout p: 0.30, Patience: 0/50            
Epoch [0003/0300], Training Loss: 0.9001, Validation Loss: 1.5701, Dropout p: 0.30, Patience: 0/50            
Epoch [0004/0300], Training Loss: 0.8280, Validation Loss: 1.1931, Dropout p: 0.30, Patience: 0/50            
Epoch [0005/0300], Training Loss: 0.7124, Validation Loss: 0.7235, Dropout p: 0.30, Patience: 0/50            
Epoch [0006/0300], Training Loss: 0.6150, Validation Loss: 0.6736, Dropout p: 0.30, Patience: 0/50            
Epoch [0007/0300], Training Loss: 0.6400, Validation Loss: 0.3624, Dropout p: 0.30, Patience: 0/50            
Epoch [0008/0300], Training Loss: 0.4723, Validation Loss: 0.2851, Dropout p: 0.30, Patience: 0/50            
Epoch [0009/0300], Training Loss: 0.5175, Validation Loss: 0.2689, Dropout p: 0.30, Patience: 0/50            
Epoch [0010/0300], Training Loss: 0.4482, Validation Loss: 0.1164, Dropout p: 0.30, Patience: 0/50            
Epoch [0011/0300], Training Loss: 0.4151, Validation Loss: 0.1370, Dropout p: 0.30, Patience: 0/50            
Epoch [0012/0300], Training Loss: 0.3851, Validation Loss: 0.1146, Dropout p: 0.30, Patience: 0/50            
Epoch [0013/0300], Training Loss: 0.3858, Validation Loss: 0.2529, Dropout p: 0.30, Patience: 0/50            
Epoch [0014/0300], Training Loss: 0.4050, Validation Loss: 0.1481, Dropout p: 0.30, Patience: 0/50            
Epoch [0015/0300], Training Loss: 0.3364, Validation Loss: 0.1746, Dropout p: 0.30, Patience: 0/50            
Epoch [0016/0300], Training Loss: 0.3700, Validation Loss: 0.2200, Dropout p: 0.30, Patience: 0/50            
Epoch [0017/0300], Training Loss: 0.3000, Validation Loss: 0.1711, Dropout p: 0.30, Patience: 0/50            
Epoch [0018/0300], Training Loss: 0.2869, Validation Loss: 0.1842, Dropout p: 0.30, Patience: 0/50            
Epoch [0019/0300], Training Loss: 0.3038, Validation Loss: 0.2297, Dropout p: 0.30, Patience: 0/50            
Epoch [0020/0300], Training Loss: 0.3320, Validation Loss: 0.1145, Dropout p: 0.30, Patience: 0/50            
Epoch [0021/0300], Training Loss: 0.3464, Validation Loss: 0.0768, Dropout p: 0.30, Patience: 0/50            
Epoch [0022/0300], Training Loss: 0.3296, Validation Loss: 0.1036, Dropout p: 0.30, Patience: 0/50            
Epoch [0023/0300], Training Loss: 0.3779, Validation Loss: 0.1300, Dropout p: 0.30, Patience: 0/50            
Epoch [0024/0300], Training Loss: 0.3281, Validation Loss: 0.1922, Dropout p: 0.30, Patience: 0/50            
Epoch [0025/0300], Training Loss: 0.2455, Validation Loss: 0.4291, Dropout p: 0.30, Patience: 0/50            
Epoch [0026/0300], Training Loss: 0.3007, Validation Loss: 0.4641, Dropout p: 0.30, Patience: 0/50            
Epoch [0027/0300], Training Loss: 0.2830, Validation Loss: 0.1104, Dropout p: 0.30, Patience: 0/50            
Epoch [0028/0300], Training Loss: 0.2674, Validation Loss: 0.1388, Dropout p: 0.30, Patience: 0/50            
Epoch [0029/0300], Training Loss: 0.2581, Validation Loss: 0.1053, Dropout p: 0.30, Patience: 0/50            
Epoch [0030/0300], Training Loss: 0.2478, Validation Loss: 0.1131, Dropout p: 0.30, Patience: 0/50            
Epoch [0031/0300], Training Loss: 0.3059, Validation Loss: 0.0817, Dropout p: 0.30, Patience: 0/50            
Epoch [0032/0300], Training Loss: 0.2261, Validation Loss: 0.1465, Dropout p: 0.30, Patience: 0/50            
Epoch [0033/0300], Training Loss: 0.2249, Validation Loss: 0.0883, Dropout p: 0.30, Patience: 0/50            
Epoch [0034/0300], Training Loss: 0.2751, Validation Loss: 0.4194, Dropout p: 0.30, Patience: 0/50            
Epoch [0035/0300], Training Loss: 0.2603, Validation Loss: 0.1197, Dropout p: 0.30, Patience: 0/50            
Epoch [0036/0300], Training Loss: 0.2238, Validation Loss: 0.0817, Dropout p: 0.30, Patience: 0/50            
Epoch [0037/0300], Training Loss: 0.2231, Validation Loss: 0.0941, Dropout p: 0.30, Patience: 0/50            
Epoch [0038/0300], Training Loss: 0.2008, Validation Loss: 0.1100, Dropout p: 0.30, Patience: 0/50            
Epoch [0039/0300], Training Loss: 0.2285, Validation Loss: 0.0998, Dropout p: 0.30, Patience: 0/50            
Epoch [0040/0300], Training Loss: 0.2305, Validation Loss: 0.1144, Dropout p: 0.30, Patience: 0/50            
Epoch [0041/0300], Training Loss: 0.1791, Validation Loss: 0.0835, Dropout p: 0.30, Patience: 0/50            
Epoch [0042/0300], Training Loss: 0.1947, Validation Loss: 0.0654, Dropout p: 0.30, Patience: 0/50            
Epoch [0043/0300], Training Loss: 0.2084, Validation Loss: 0.1819, Dropout p: 0.30, Patience: 0/50            
Epoch [0044/0300], Training Loss: 0.1996, Validation Loss: 0.1008, Dropout p: 0.30, Patience: 0/50            
Epoch [0045/0300], Training Loss: 0.1645, Validation Loss: 0.0925, Dropout p: 0.30, Patience: 0/50            
Epoch [0046/0300], Training Loss: 0.1302, Validation Loss: 0.1750, Dropout p: 0.30, Patience: 0/50            
Epoch [0047/0300], Training Loss: 0.1956, Validation Loss: 0.0892, Dropout p: 0.30, Patience: 0/50            
Epoch [0048/0300], Training Loss: 0.1875, Validation Loss: 0.1422, Dropout p: 0.30, Patience: 0/50            
Epoch [0049/0300], Training Loss: 0.1439, Validation Loss: 0.0805, Dropout p: 0.30, Patience: 0/50            
Epoch [0050/0300], Training Loss: 0.2129, Validation Loss: 0.1489, Dropout p: 0.30, Patience: 0/50            
Epoch [0051/0300], Training Loss: 0.1650, Validation Loss: 0.0932, Dropout p: 0.30, Patience: 0/50            
Epoch [0052/0300], Training Loss: 0.1899, Validation Loss: 0.0799, Dropout p: 0.30, Patience: 0/50            
Epoch [0053/0300], Training Loss: 0.1291, Validation Loss: 0.0805, Dropout p: 0.30, Patience: 0/50            
Epoch [0054/0300], Training Loss: 0.2594, Validation Loss: 0.1161, Dropout p: 0.30, Patience: 0/50            
Epoch [0055/0300], Training Loss: 0.1473, Validation Loss: 0.0848, Dropout p: 0.30, Patience: 0/50            
Epoch [0056/0300], Training Loss: 0.1394, Validation Loss: 0.1887, Dropout p: 0.30, Patience: 0/50            
Epoch [0057/0300], Training Loss: 0.1835, Validation Loss: 0.0776, Dropout p: 0.30, Patience: 0/50            
Epoch [0058/0300], Training Loss: 0.1364, Validation Loss: 0.0857, Dropout p: 0.30, Patience: 0/50            
Epoch [0059/0300], Training Loss: 0.1345, Validation Loss: 0.0699, Dropout p: 0.30, Patience: 0/50            
Epoch [0060/0300], Training Loss: 0.1649, Validation Loss: 0.0900, Dropout p: 0.30, Patience: 0/50            
Epoch [0061/0300], Training Loss: 0.1131, Validation Loss: 0.0901, Dropout p: 0.30, Patience: 0/50            
Epoch [0062/0300], Training Loss: 0.1279, Validation Loss: 0.1481, Dropout p: 0.30, Patience: 0/50            
Epoch [0063/0300], Training Loss: 0.1071, Validation Loss: 0.0744, Dropout p: 0.30, Patience: 0/50            
Epoch [0064/0300], Training Loss: 0.1597, Validation Loss: 0.1317, Dropout p: 0.30, Patience: 0/50            
Epoch [0065/0300], Training Loss: 0.1235, Validation Loss: 0.0628, Dropout p: 0.30, Patience: 0/50            
Epoch [0066/0300], Training Loss: 0.1156, Validation Loss: 0.0958, Dropout p: 0.30, Patience: 0/50            
Epoch [0067/0300], Training Loss: 0.1438, Validation Loss: 0.0759, Dropout p: 0.30, Patience: 0/50            
Epoch [0068/0300], Training Loss: 0.1378, Validation Loss: 0.0981, Dropout p: 0.30, Patience: 0/50            
Epoch [0069/0300], Training Loss: 0.1110, Validation Loss: 0.1257, Dropout p: 0.30, Patience: 0/50            
Epoch [0070/0300], Training Loss: 0.1553, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 0/50            
Epoch [0071/0300], Training Loss: 0.1409, Validation Loss: 0.0980, Dropout p: 0.30, Patience: 0/50            
Epoch [0072/0300], Training Loss: 0.1308, Validation Loss: 0.0852, Dropout p: 0.30, Patience: 0/50            
Epoch [0073/0300], Training Loss: 0.1043, Validation Loss: 0.0982, Dropout p: 0.30, Patience: 0/50            
Epoch [0074/0300], Training Loss: 0.1064, Validation Loss: 0.0953, Dropout p: 0.30, Patience: 0/50            
Epoch [0075/0300], Training Loss: 0.1277, Validation Loss: 0.0733, Dropout p: 0.30, Patience: 0/50            
Epoch [0076/0300], Training Loss: 0.1050, Validation Loss: 0.0894, Dropout p: 0.30, Patience: 0/50            
Epoch [0077/0300], Training Loss: 0.1105, Validation Loss: 0.1016, Dropout p: 0.30, Patience: 0/50            
Epoch [0078/0300], Training Loss: 0.0957, Validation Loss: 0.1407, Dropout p: 0.30, Patience: 0/50            
Epoch [0079/0300], Training Loss: 0.1410, Validation Loss: 0.0856, Dropout p: 0.30, Patience: 0/50            
Epoch [0080/0300], Training Loss: 0.1077, Validation Loss: 0.1267, Dropout p: 0.30, Patience: 0/50            
Epoch [0081/0300], Training Loss: 0.1094, Validation Loss: 0.0736, Dropout p: 0.30, Patience: 0/50            
Epoch [0082/0300], Training Loss: 0.1361, Validation Loss: 0.1690, Dropout p: 0.30, Patience: 0/50            
Epoch [0083/0300], Training Loss: 0.1215, Validation Loss: 0.0811, Dropout p: 0.30, Patience: 0/50            
Epoch [0084/0300], Training Loss: 0.1033, Validation Loss: 0.1123, Dropout p: 0.30, Patience: 0/50            
Epoch [0085/0300], Training Loss: 0.1278, Validation Loss: 0.0908, Dropout p: 0.30, Patience: 0/50            
Epoch [0086/0300], Training Loss: 0.1554, Validation Loss: 0.1242, Dropout p: 0.30, Patience: 0/50            
Epoch [0087/0300], Training Loss: 0.1263, Validation Loss: 0.0756, Dropout p: 0.30, Patience: 0/50            
Epoch [0088/0300], Training Loss: 0.1003, Validation Loss: 0.0675, Dropout p: 0.30, Patience: 0/50            
Epoch [0089/0300], Training Loss: 0.0942, Validation Loss: 0.0898, Dropout p: 0.30, Patience: 0/50            
Epoch [0090/0300], Training Loss: 0.1373, Validation Loss: 0.1562, Dropout p: 0.30, Patience: 0/50            
Epoch [0091/0300], Training Loss: 0.0998, Validation Loss: 0.1088, Dropout p: 0.30, Patience: 0/50            
Epoch [0092/0300], Training Loss: 0.0916, Validation Loss: 0.0751, Dropout p: 0.30, Patience: 0/50            
Epoch [0093/0300], Training Loss: 0.1112, Validation Loss: 0.1240, Dropout p: 0.30, Patience: 0/50            
Epoch [0094/0300], Training Loss: 0.1151, Validation Loss: 0.1061, Dropout p: 0.30, Patience: 0/50            
Epoch [0095/0300], Training Loss: 0.0964, Validation Loss: 0.1059, Dropout p: 0.30, Patience: 0/50            
Epoch [0096/0300], Training Loss: 0.1298, Validation Loss: 0.1202, Dropout p: 0.30, Patience: 0/50            
Epoch [0097/0300], Training Loss: 0.0898, Validation Loss: 0.0940, Dropout p: 0.30, Patience: 0/50            
Epoch [0098/0300], Training Loss: 0.0960, Validation Loss: 0.1150, Dropout p: 0.30, Patience: 0/50            
Epoch [0099/0300], Training Loss: 0.0768, Validation Loss: 0.0691, Dropout p: 0.30, Patience: 0/50            
Epoch [0100/0300], Training Loss: 0.0874, Validation Loss: 0.1169, Dropout p: 0.30, Patience: 0/50            
Epoch [0101/0300], Training Loss: 0.1176, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 0/50            
Epoch [0102/0300], Training Loss: 0.1057, Validation Loss: 0.1140, Dropout p: 0.30, Patience: 0/50            
Epoch [0103/0300], Training Loss: 0.0977, Validation Loss: 0.1217, Dropout p: 0.30, Patience: 0/50            
Epoch [0104/0300], Training Loss: 0.1219, Validation Loss: 0.1357, Dropout p: 0.30, Patience: 0/50            
Epoch [0105/0300], Training Loss: 0.0831, Validation Loss: 0.0889, Dropout p: 0.30, Patience: 0/50            
Epoch [0106/0300], Training Loss: 0.0862, Validation Loss: 0.0846, Dropout p: 0.30, Patience: 0/50            
Epoch [0107/0300], Training Loss: 0.0763, Validation Loss: 0.0902, Dropout p: 0.30, Patience: 0/50            
Epoch [0108/0300], Training Loss: 0.0877, Validation Loss: 0.0876, Dropout p: 0.30, Patience: 0/50            
Epoch [0109/0300], Training Loss: 0.0809, Validation Loss: 0.1395, Dropout p: 0.30, Patience: 0/50            
Epoch [0110/0300], Training Loss: 0.0849, Validation Loss: 0.1150, Dropout p: 0.30, Patience: 0/50            
Epoch [0111/0300], Training Loss: 0.0949, Validation Loss: 0.0940, Dropout p: 0.30, Patience: 0/50            
Epoch [0112/0300], Training Loss: 0.0916, Validation Loss: 0.1327, Dropout p: 0.30, Patience: 0/50            
Epoch [0113/0300], Training Loss: 0.0820, Validation Loss: 0.1325, Dropout p: 0.30, Patience: 0/50            
Epoch [0114/0300], Training Loss: 0.0898, Validation Loss: 0.0872, Dropout p: 0.30, Patience: 0/50            
Epoch [0115/0300], Training Loss: 0.0727, Validation Loss: 0.1172, Dropout p: 0.30, Patience: 0/50            
Epoch [0116/0300], Training Loss: 0.0972, Validation Loss: 0.1083, Dropout p: 0.30, Patience: 0/50            
Epoch [0117/0300], Training Loss: 0.0884, Validation Loss: 0.2136, Dropout p: 0.30, Patience: 0/50            
Epoch [0118/0300], Training Loss: 0.0756, Validation Loss: 0.1103, Dropout p: 0.30, Patience: 0/50            
Epoch [0119/0300], Training Loss: 0.0918, Validation Loss: 0.2078, Dropout p: 0.30, Patience: 0/50            
Epoch [0120/0300], Training Loss: 0.0709, Validation Loss: 0.1055, Dropout p: 0.30, Patience: 0/50            
Epoch [0121/0300], Training Loss: 0.1034, Validation Loss: 0.1151, Dropout p: 0.30, Patience: 0/50            
Epoch [0122/0300], Training Loss: 0.0875, Validation Loss: 0.0955, Dropout p: 0.30, Patience: 0/50            
Epoch [0123/0300], Training Loss: 0.0841, Validation Loss: 0.0879, Dropout p: 0.30, Patience: 0/50            
Epoch [0124/0300], Training Loss: 0.0547, Validation Loss: 0.1511, Dropout p: 0.30, Patience: 0/50            
Epoch [0125/0300], Training Loss: 0.0619, Validation Loss: 0.1074, Dropout p: 0.30, Patience: 0/50            
Epoch [0126/0300], Training Loss: 0.0705, Validation Loss: 0.1175, Dropout p: 0.30, Patience: 0/50            
Epoch [0127/0300], Training Loss: 0.0872, Validation Loss: 0.0908, Dropout p: 0.30, Patience: 0/50            
Epoch [0128/0300], Training Loss: 0.0644, Validation Loss: 0.0989, Dropout p: 0.30, Patience: 1/50            
Epoch [0129/0300], Training Loss: 0.0599, Validation Loss: 0.0819, Dropout p: 0.30, Patience: 0/50            
Epoch [0130/0300], Training Loss: 0.0531, Validation Loss: 0.0889, Dropout p: 0.30, Patience: 1/50            
Epoch [0131/0300], Training Loss: 0.0597, Validation Loss: 0.1390, Dropout p: 0.30, Patience: 2/50            
Epoch [0132/0300], Training Loss: 0.0682, Validation Loss: 0.1157, Dropout p: 0.30, Patience: 3/50            
Epoch [0133/0300], Training Loss: 0.0652, Validation Loss: 0.0751, Dropout p: 0.30, Patience: 0/50            
Epoch [0134/0300], Training Loss: 0.0838, Validation Loss: 0.1723, Dropout p: 0.30, Patience: 1/50            
Epoch [0135/0300], Training Loss: 0.0774, Validation Loss: 0.1087, Dropout p: 0.30, Patience: 2/50            
Epoch [0136/0300], Training Loss: 0.0811, Validation Loss: 0.1331, Dropout p: 0.30, Patience: 3/50            
Epoch [0137/0300], Training Loss: 0.0636, Validation Loss: 0.0959, Dropout p: 0.30, Patience: 4/50            
Epoch [0138/0300], Training Loss: 0.0695, Validation Loss: 0.1239, Dropout p: 0.30, Patience: 5/50            
Epoch [0139/0300], Training Loss: 0.0549, Validation Loss: 0.1375, Dropout p: 0.30, Patience: 6/50            
Epoch [0140/0300], Training Loss: 0.0556, Validation Loss: 0.0929, Dropout p: 0.30, Patience: 7/50            
Epoch [0141/0300], Training Loss: 0.0858, Validation Loss: 0.1622, Dropout p: 0.30, Patience: 8/50            
Epoch [0142/0300], Training Loss: 0.0635, Validation Loss: 0.1281, Dropout p: 0.30, Patience: 9/50            
Epoch [0143/0300], Training Loss: 0.0570, Validation Loss: 0.0964, Dropout p: 0.30, Patience: 10/50            
Epoch [0144/0300], Training Loss: 0.0460, Validation Loss: 0.0998, Dropout p: 0.30, Patience: 11/50            
Epoch [0145/0300], Training Loss: 0.0493, Validation Loss: 0.1101, Dropout p: 0.30, Patience: 12/50            
Epoch [0146/0300], Training Loss: 0.0558, Validation Loss: 0.1232, Dropout p: 0.30, Patience: 13/50            
Epoch [0147/0300], Training Loss: 0.0556, Validation Loss: 0.1070, Dropout p: 0.30, Patience: 14/50            
Epoch [0148/0300], Training Loss: 0.0390, Validation Loss: 0.1306, Dropout p: 0.30, Patience: 15/50            
Epoch [0149/0300], Training Loss: 0.0528, Validation Loss: 0.1133, Dropout p: 0.30, Patience: 16/50            
Epoch [0150/0300], Training Loss: 0.0433, Validation Loss: 0.0958, Dropout p: 0.30, Patience: 17/50            
Epoch [0151/0300], Training Loss: 0.0440, Validation Loss: 0.1273, Dropout p: 0.30, Patience: 18/50            
Epoch [0152/0300], Training Loss: 0.0405, Validation Loss: 0.1026, Dropout p: 0.30, Patience: 19/50            
Epoch [0153/0300], Training Loss: 0.0413, Validation Loss: 0.0828, Dropout p: 0.30, Patience: 20/50            
Epoch [0154/0300], Training Loss: 0.0468, Validation Loss: 0.1125, Dropout p: 0.30, Patience: 21/50            
Epoch [0155/0300], Training Loss: 0.0523, Validation Loss: 0.1146, Dropout p: 0.30, Patience: 22/50            
Epoch [0156/0300], Training Loss: 0.0606, Validation Loss: 0.1017, Dropout p: 0.30, Patience: 23/50            
Epoch [0157/0300], Training Loss: 0.0410, Validation Loss: 0.0957, Dropout p: 0.30, Patience: 24/50            
Epoch [0158/0300], Training Loss: 0.0469, Validation Loss: 0.1242, Dropout p: 0.30, Patience: 25/50            
Epoch [0159/0300], Training Loss: 0.0446, Validation Loss: 0.1054, Dropout p: 0.30, Patience: 26/50            
Epoch [0160/0300], Training Loss: 0.0558, Validation Loss: 0.0861, Dropout p: 0.30, Patience: 27/50            
Epoch [0161/0300], Training Loss: 0.0366, Validation Loss: 0.1100, Dropout p: 0.30, Patience: 28/50            
Epoch [0162/0300], Training Loss: 0.0327, Validation Loss: 0.1117, Dropout p: 0.30, Patience: 29/50            
Epoch [0163/0300], Training Loss: 0.0497, Validation Loss: 0.0953, Dropout p: 0.30, Patience: 30/50            
Epoch [0164/0300], Training Loss: 0.0435, Validation Loss: 0.0908, Dropout p: 0.30, Patience: 31/50            
Epoch [0165/0300], Training Loss: 0.0440, Validation Loss: 0.1170, Dropout p: 0.30, Patience: 32/50            
Epoch [0166/0300], Training Loss: 0.0376, Validation Loss: 0.1151, Dropout p: 0.30, Patience: 33/50            
Epoch [0167/0300], Training Loss: 0.0384, Validation Loss: 0.0892, Dropout p: 0.30, Patience: 34/50            
Epoch [0168/0300], Training Loss: 0.0462, Validation Loss: 0.1144, Dropout p: 0.30, Patience: 35/50            
Epoch [0169/0300], Training Loss: 0.0339, Validation Loss: 0.1143, Dropout p: 0.30, Patience: 36/50            
Epoch [0170/0300], Training Loss: 0.0426, Validation Loss: 0.1008, Dropout p: 0.30, Patience: 37/50            
Epoch [0171/0300], Training Loss: 0.0542, Validation Loss: 0.0993, Dropout p: 0.30, Patience: 38/50            
Epoch [0172/0300], Training Loss: 0.0364, Validation Loss: 0.1106, Dropout p: 0.30, Patience: 39/50            
Epoch [0173/0300], Training Loss: 0.0535, Validation Loss: 0.1133, Dropout p: 0.30, Patience: 40/50            
Epoch [0174/0300], Training Loss: 0.0383, Validation Loss: 0.1118, Dropout p: 0.30, Patience: 41/50            
Epoch [0175/0300], Training Loss: 0.0414, Validation Loss: 0.1081, Dropout p: 0.30, Patience: 42/50            
Epoch [0176/0300], Training Loss: 0.0332, Validation Loss: 0.0980, Dropout p: 0.30, Patience: 43/50            
Epoch [0177/0300], Training Loss: 0.0642, Validation Loss: 0.1117, Dropout p: 0.30, Patience: 44/50            
Epoch [0178/0300], Training Loss: 0.0549, Validation Loss: 0.1126, Dropout p: 0.30, Patience: 45/50            
Epoch [0179/0300], Training Loss: 0.0416, Validation Loss: 0.1205, Dropout p: 0.30, Patience: 46/50            
Epoch [0180/0300], Training Loss: 0.0409, Validation Loss: 0.1090, Dropout p: 0.30, Patience: 47/50            
Epoch [0181/0300], Training Loss: 0.0357, Validation Loss: 0.1152, Dropout p: 0.30, Patience: 48/50            
Epoch [0182/0300], Training Loss: 0.0399, Validation Loss: 0.1163, Dropout p: 0.30, Patience: 49/50            
Epoch [0183/0300], Training Loss: 0.0361, Validation Loss: 0.1181, Dropout p: 0.30, Patience: 50/50            

Evaluating model on test set...
Predicted TBV: 254.70	Actual TBV: 278.68	Difference: 23.98
Predicted TBV: 182.39	Actual TBV: 185.38	Difference: 2.99
Predicted TBV: 204.20	Actual TBV: 232.09	Difference: 27.89
Predicted TBV: 244.51	Actual TBV: 255.49	Difference: 10.98
Predicted TBV: 251.12	Actual TBV: 270.60	Difference: 19.48
Predicted TBV: 421.82	Actual TBV: 436.77	Difference: 14.95
Predicted TBV: 108.65	Actual TBV: 103.80	Difference: 4.85
Predicted TBV: 256.87	Actual TBV: 261.07	Difference: 4.20
Predicted TBV: 333.11	Actual TBV: 326.98	Difference: 6.13
Predicted TBV: 329.87	Actual TBV: 310.82	Difference: 19.05
Predicted TBV: 225.32	Actual TBV: 203.67	Difference: 21.65
Predicted TBV: 198.96	Actual TBV: 203.15	Difference: 4.19
Predicted TBV: 185.71	Actual TBV: 173.72	Difference: 11.99
Predicted TBV: 232.93	Actual TBV: 199.16	Difference: 33.77
Predicted TBV: 188.95	Actual TBV: 185.07	Difference: 3.88
Predicted TBV: 176.57	Actual TBV: 167.47	Difference: 9.10
Predicted TBV: 170.13	Actual TBV: 169.57	Difference: 0.56
Predicted TBV: 127.34	Actual TBV: 128.96	Difference: 1.62
Predicted TBV: 429.33	Actual TBV: 389.83	Difference: 39.50
Predicted TBV: 147.50	Actual TBV: 159.59	Difference: 12.09
Predicted TBV: 312.09	Actual TBV: 346.51	Difference: 34.42
Predicted TBV: 188.33	Actual TBV: 185.31	Difference: 3.02
Predicted TBV: 354.21	Actual TBV: 381.77	Difference: 27.56
Predicted TBV: 198.87	Actual TBV: 185.04	Difference: 13.83
Predicted TBV: 248.23	Actual TBV: 246.80	Difference: 1.43
Predicted TBV: 170.85	Actual TBV: 174.98	Difference: 4.13
Predicted TBV: 209.23	Actual TBV: 209.68	Difference: 0.45
Predicted TBV: 245.75	Actual TBV: 236.49	Difference: 9.26
Predicted TBV: 294.14	Actual TBV: 282.55	Difference: 11.59
Predicted TBV: 192.86	Actual TBV: 220.23	Difference: 27.37
Predicted TBV: 176.01	Actual TBV: 170.82	Difference: 5.19
Predicted TBV: 235.17	Actual TBV: 228.85	Difference: 6.32
Predicted TBV: 169.45	Actual TBV: 179.03	Difference: 9.58
Predicted TBV: 258.18	Actual TBV: 281.36	Difference: 23.18
Predicted TBV: 204.83	Actual TBV: 203.92	Difference: 0.91
Predicted TBV: 220.76	Actual TBV: 203.03	Difference: 17.73
Predicted TBV: 354.26	Actual TBV: 337.52	Difference: 16.74
Predicted TBV: 188.73	Actual TBV: 180.88	Difference: 7.85
Predicted TBV: 196.68	Actual TBV: 179.91	Difference: 16.77
Predicted TBV: 195.02	Actual TBV: 184.50	Difference: 10.52

Evaluating model with 30 Bayesian runs...
Refused raster with error 10.45. TBV: 10.98.
Refused raster with error 12.51. TBV: 19.48.
Refused raster with error 10.40. TBV: 6.13.
Refused raster with error 16.99. TBV: 19.05.
Refused raster with error 11.84. TBV: 4.19.
Refused raster with error 9.32. TBV: 9.10.
Refused raster with error 9.04. TBV: 1.62.
Refused raster with error 16.41. TBV: 39.50.
Refused raster with error 9.62. TBV: 34.42.
Refused raster with error 10.45. TBV: 27.37.
Refused raster with error 10.11. TBV: 17.73.
Refused raster with error 12.89. TBV: 16.74.
Refused raster with error 13.85. TBV: 10.52.
Refused raster with error 9.32. TBV: 4.86.
Refused raster with error 10.19. TBV: 14.46.
Refused raster with error 9.65. TBV: 12.72.
Refused raster with error 11.23. TBV: 14.25.
Refused raster with error 11.03. TBV: 25.20.
Refused raster with error 16.13. TBV: 10.01.
Refused raster with error 12.04. TBV: 4.46.
Refused raster with error 13.42. TBV: 4.80.
Refused raster with error 9.77. TBV: 13.24.
Refused raster with error 11.18. TBV: 9.52.
Refused raster with error 14.65. TBV: 14.25.
Refused raster with error 9.33. TBV: 6.60.
Refused raster with error 12.30. TBV: 26.07.
Refused raster with error 14.14. TBV: 14.72.
Refused raster with error 9.42. TBV: 17.57.
Refused raster with error 10.71. TBV: 5.01.
Refused raster with error 17.39. TBV: 24.86.
Refused raster with error 11.43. TBV: 15.28.
Refused raster with error 13.64. TBV: 9.87.

- - - - - -
Total Raster Count: 82
Refused Raster Count: 32
- - - - - -

- - - - - -
Non-bayesian prediction:
Mean Absolute Error: 13.17 cc
Standard Deviation: 8.99 cc
Big error count (>30): 4
Big error mean: 35.34 cc
Big error std: 2.42 cc
- - - - - -

- - - - - -
Bayesian prediction:
Mean Absolute Error: 12.30 cc
Standard Deviation: 8.96 cc
Big error count (>30): 2
Big error mean: 33.72 cc
Big error std: 0.05 cc
- - - - - -

 - - - Fold 3/6 - - -

Train volume count: 312
Validation volume count: 58
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '79' '43' '47' '35' '16' '44' '26' '81' '46' '45' '50' '18' '32'
 '21' '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 0.9390, Validation Loss: 2.0203, Dropout p: 0.30, Patience: 0/50            
Epoch [0002/0300], Training Loss: 0.8424, Validation Loss: 2.6627, Dropout p: 0.30, Patience: 0/50            
Epoch [0003/0300], Training Loss: 0.7387, Validation Loss: 1.3509, Dropout p: 0.30, Patience: 0/50            
Epoch [0004/0300], Training Loss: 0.7074, Validation Loss: 1.4623, Dropout p: 0.30, Patience: 0/50            
Epoch [0005/0300], Training Loss: 0.6664, Validation Loss: 0.9442, Dropout p: 0.30, Patience: 0/50            
Epoch [0006/0300], Training Loss: 0.5052, Validation Loss: 0.5758, Dropout p: 0.30, Patience: 0/50            
Epoch [0007/0300], Training Loss: 0.4584, Validation Loss: 0.7735, Dropout p: 0.30, Patience: 0/50            
Epoch [0008/0300], Training Loss: 0.4573, Validation Loss: 0.2452, Dropout p: 0.30, Patience: 0/50            
Epoch [0009/0300], Training Loss: 0.3673, Validation Loss: 0.3599, Dropout p: 0.30, Patience: 0/50            
Epoch [0010/0300], Training Loss: 0.4355, Validation Loss: 0.1669, Dropout p: 0.30, Patience: 0/50            
Epoch [0011/0300], Training Loss: 0.2863, Validation Loss: 0.3611, Dropout p: 0.30, Patience: 0/50            
Epoch [0012/0300], Training Loss: 0.3299, Validation Loss: 0.3419, Dropout p: 0.30, Patience: 0/50            
Epoch [0013/0300], Training Loss: 0.3372, Validation Loss: 0.2936, Dropout p: 0.30, Patience: 0/50            
Epoch [0014/0300], Training Loss: 0.3481, Validation Loss: 0.1934, Dropout p: 0.30, Patience: 0/50            
Epoch [0015/0300], Training Loss: 0.3336, Validation Loss: 0.1783, Dropout p: 0.30, Patience: 0/50            
Epoch [0016/0300], Training Loss: 0.3417, Validation Loss: 0.1471, Dropout p: 0.30, Patience: 0/50            
Epoch [0017/0300], Training Loss: 0.3745, Validation Loss: 0.1478, Dropout p: 0.30, Patience: 0/50            
Epoch [0018/0300], Training Loss: 0.2874, Validation Loss: 0.1681, Dropout p: 0.30, Patience: 0/50            
Epoch [0019/0300], Training Loss: 0.2852, Validation Loss: 0.3128, Dropout p: 0.30, Patience: 0/50            
Epoch [0020/0300], Training Loss: 0.2947, Validation Loss: 0.2188, Dropout p: 0.30, Patience: 0/50            
Epoch [0021/0300], Training Loss: 0.2699, Validation Loss: 0.4185, Dropout p: 0.30, Patience: 0/50            
Epoch [0022/0300], Training Loss: 0.2608, Validation Loss: 0.1631, Dropout p: 0.30, Patience: 0/50            
Epoch [0023/0300], Training Loss: 0.3157, Validation Loss: 0.2185, Dropout p: 0.30, Patience: 0/50            
Epoch [0024/0300], Training Loss: 0.2390, Validation Loss: 0.2082, Dropout p: 0.30, Patience: 0/50            
Epoch [0025/0300], Training Loss: 0.2898, Validation Loss: 0.1056, Dropout p: 0.30, Patience: 0/50            
Epoch [0026/0300], Training Loss: 0.3021, Validation Loss: 0.1059, Dropout p: 0.30, Patience: 0/50            
Epoch [0027/0300], Training Loss: 0.2557, Validation Loss: 0.0916, Dropout p: 0.30, Patience: 0/50            
Epoch [0028/0300], Training Loss: 0.2192, Validation Loss: 0.1671, Dropout p: 0.30, Patience: 0/50            
Epoch [0029/0300], Training Loss: 0.2513, Validation Loss: 0.1710, Dropout p: 0.30, Patience: 0/50            
Epoch [0030/0300], Training Loss: 0.2074, Validation Loss: 0.0816, Dropout p: 0.30, Patience: 0/50            
Epoch [0031/0300], Training Loss: 0.2184, Validation Loss: 0.1125, Dropout p: 0.30, Patience: 0/50            
Epoch [0032/0300], Training Loss: 0.1843, Validation Loss: 0.1110, Dropout p: 0.30, Patience: 0/50            
Epoch [0033/0300], Training Loss: 0.2303, Validation Loss: 0.1140, Dropout p: 0.30, Patience: 0/50            
Epoch [0034/0300], Training Loss: 0.2616, Validation Loss: 0.1085, Dropout p: 0.30, Patience: 0/50            
Epoch [0035/0300], Training Loss: 0.2298, Validation Loss: 0.0943, Dropout p: 0.30, Patience: 0/50            
Epoch [0036/0300], Training Loss: 0.1924, Validation Loss: 0.0823, Dropout p: 0.30, Patience: 0/50            
Epoch [0037/0300], Training Loss: 0.2026, Validation Loss: 0.1269, Dropout p: 0.30, Patience: 0/50            
Epoch [0038/0300], Training Loss: 0.1902, Validation Loss: 0.0846, Dropout p: 0.30, Patience: 0/50            
Epoch [0039/0300], Training Loss: 0.2283, Validation Loss: 0.1268, Dropout p: 0.30, Patience: 0/50            
Epoch [0040/0300], Training Loss: 0.2270, Validation Loss: 0.1556, Dropout p: 0.30, Patience: 0/50            
Epoch [0041/0300], Training Loss: 0.1609, Validation Loss: 0.1137, Dropout p: 0.30, Patience: 0/50            
Epoch [0042/0300], Training Loss: 0.1993, Validation Loss: 0.0739, Dropout p: 0.30, Patience: 0/50            
Epoch [0043/0300], Training Loss: 0.1700, Validation Loss: 0.1607, Dropout p: 0.30, Patience: 0/50            
Epoch [0044/0300], Training Loss: 0.1674, Validation Loss: 0.0857, Dropout p: 0.30, Patience: 0/50            
Epoch [0045/0300], Training Loss: 0.1897, Validation Loss: 0.1069, Dropout p: 0.30, Patience: 0/50            
Epoch [0046/0300], Training Loss: 0.1626, Validation Loss: 0.1011, Dropout p: 0.30, Patience: 0/50            
Epoch [0047/0300], Training Loss: 0.1452, Validation Loss: 0.1236, Dropout p: 0.30, Patience: 0/50            
Epoch [0048/0300], Training Loss: 0.1615, Validation Loss: 0.1025, Dropout p: 0.30, Patience: 0/50            
Epoch [0049/0300], Training Loss: 0.1576, Validation Loss: 0.1142, Dropout p: 0.30, Patience: 0/50            
Epoch [0050/0300], Training Loss: 0.2080, Validation Loss: 0.0981, Dropout p: 0.30, Patience: 0/50            
Epoch [0051/0300], Training Loss: 0.1800, Validation Loss: 0.0712, Dropout p: 0.30, Patience: 0/50            
Epoch [0052/0300], Training Loss: 0.1454, Validation Loss: 0.1062, Dropout p: 0.30, Patience: 0/50            
Epoch [0053/0300], Training Loss: 0.1685, Validation Loss: 0.1026, Dropout p: 0.30, Patience: 0/50            
Epoch [0054/0300], Training Loss: 0.1138, Validation Loss: 0.1059, Dropout p: 0.30, Patience: 0/50            
Epoch [0055/0300], Training Loss: 0.2210, Validation Loss: 0.1224, Dropout p: 0.30, Patience: 0/50            
Epoch [0056/0300], Training Loss: 0.1245, Validation Loss: 0.0787, Dropout p: 0.30, Patience: 0/50            
Epoch [0057/0300], Training Loss: 0.1248, Validation Loss: 0.0726, Dropout p: 0.30, Patience: 0/50            
Epoch [0058/0300], Training Loss: 0.1308, Validation Loss: 0.1570, Dropout p: 0.30, Patience: 0/50            
Epoch [0059/0300], Training Loss: 0.1401, Validation Loss: 0.0762, Dropout p: 0.30, Patience: 0/50            
Epoch [0060/0300], Training Loss: 0.1568, Validation Loss: 0.1629, Dropout p: 0.30, Patience: 0/50            
Epoch [0061/0300], Training Loss: 0.1632, Validation Loss: 0.1131, Dropout p: 0.30, Patience: 0/50            
Epoch [0062/0300], Training Loss: 0.1408, Validation Loss: 0.1763, Dropout p: 0.30, Patience: 0/50            
Epoch [0063/0300], Training Loss: 0.1286, Validation Loss: 0.0566, Dropout p: 0.30, Patience: 0/50            
Epoch [0064/0300], Training Loss: 0.1510, Validation Loss: 0.0876, Dropout p: 0.30, Patience: 0/50            
Epoch [0065/0300], Training Loss: 0.1074, Validation Loss: 0.1036, Dropout p: 0.30, Patience: 0/50            
Epoch [0066/0300], Training Loss: 0.1339, Validation Loss: 0.2188, Dropout p: 0.30, Patience: 0/50            
Epoch [0067/0300], Training Loss: 0.1435, Validation Loss: 0.0497, Dropout p: 0.30, Patience: 0/50            
Epoch [0068/0300], Training Loss: 0.1067, Validation Loss: 0.1183, Dropout p: 0.30, Patience: 0/50            
Epoch [0069/0300], Training Loss: 0.1186, Validation Loss: 0.0819, Dropout p: 0.30, Patience: 0/50            
Epoch [0070/0300], Training Loss: 0.1285, Validation Loss: 0.2180, Dropout p: 0.30, Patience: 0/50            
Epoch [0071/0300], Training Loss: 0.1363, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 0/50            
Epoch [0072/0300], Training Loss: 0.1092, Validation Loss: 0.0609, Dropout p: 0.30, Patience: 0/50            
Epoch [0073/0300], Training Loss: 0.1106, Validation Loss: 0.1204, Dropout p: 0.30, Patience: 0/50            
Epoch [0074/0300], Training Loss: 0.1452, Validation Loss: 0.1045, Dropout p: 0.30, Patience: 0/50            
Epoch [0075/0300], Training Loss: 0.1090, Validation Loss: 0.1319, Dropout p: 0.30, Patience: 0/50            
Epoch [0076/0300], Training Loss: 0.1339, Validation Loss: 0.1064, Dropout p: 0.30, Patience: 0/50            
Epoch [0077/0300], Training Loss: 0.1070, Validation Loss: 0.0980, Dropout p: 0.30, Patience: 0/50            
Epoch [0078/0300], Training Loss: 0.0973, Validation Loss: 0.1073, Dropout p: 0.30, Patience: 0/50            
Epoch [0079/0300], Training Loss: 0.1593, Validation Loss: 0.0623, Dropout p: 0.30, Patience: 0/50            
Epoch [0080/0300], Training Loss: 0.0972, Validation Loss: 0.0574, Dropout p: 0.30, Patience: 0/50            
Epoch [0081/0300], Training Loss: 0.1179, Validation Loss: 0.0629, Dropout p: 0.30, Patience: 0/50            
Epoch [0082/0300], Training Loss: 0.0913, Validation Loss: 0.0854, Dropout p: 0.30, Patience: 0/50            
Epoch [0083/0300], Training Loss: 0.1178, Validation Loss: 0.0632, Dropout p: 0.30, Patience: 0/50            
Epoch [0084/0300], Training Loss: 0.0809, Validation Loss: 0.0874, Dropout p: 0.30, Patience: 0/50            
Epoch [0085/0300], Training Loss: 0.1141, Validation Loss: 0.0746, Dropout p: 0.30, Patience: 0/50            
Epoch [0086/0300], Training Loss: 0.0942, Validation Loss: 0.0896, Dropout p: 0.30, Patience: 0/50            
Epoch [0087/0300], Training Loss: 0.0999, Validation Loss: 0.0608, Dropout p: 0.30, Patience: 0/50            
Epoch [0088/0300], Training Loss: 0.0773, Validation Loss: 0.0538, Dropout p: 0.30, Patience: 0/50            
Epoch [0089/0300], Training Loss: 0.1098, Validation Loss: 0.0964, Dropout p: 0.30, Patience: 0/50            
Epoch [0090/0300], Training Loss: 0.0847, Validation Loss: 0.0766, Dropout p: 0.30, Patience: 0/50            
Epoch [0091/0300], Training Loss: 0.1019, Validation Loss: 0.1082, Dropout p: 0.30, Patience: 0/50            
Epoch [0092/0300], Training Loss: 0.1381, Validation Loss: 0.1035, Dropout p: 0.30, Patience: 0/50            
Epoch [0093/0300], Training Loss: 0.0862, Validation Loss: 0.1264, Dropout p: 0.30, Patience: 0/50            
Epoch [0094/0300], Training Loss: 0.0805, Validation Loss: 0.0928, Dropout p: 0.30, Patience: 0/50            
Epoch [0095/0300], Training Loss: 0.1067, Validation Loss: 0.1141, Dropout p: 0.30, Patience: 0/50            
Epoch [0096/0300], Training Loss: 0.0981, Validation Loss: 0.1086, Dropout p: 0.30, Patience: 0/50            
Epoch [0097/0300], Training Loss: 0.1000, Validation Loss: 0.1440, Dropout p: 0.30, Patience: 0/50            
Epoch [0098/0300], Training Loss: 0.1010, Validation Loss: 0.0886, Dropout p: 0.30, Patience: 0/50            
Epoch [0099/0300], Training Loss: 0.0983, Validation Loss: 0.0761, Dropout p: 0.30, Patience: 0/50            
Epoch [0100/0300], Training Loss: 0.0830, Validation Loss: 0.1127, Dropout p: 0.30, Patience: 0/50            
Epoch [0101/0300], Training Loss: 0.0507, Validation Loss: 0.0906, Dropout p: 0.30, Patience: 0/50            
Epoch [0102/0300], Training Loss: 0.0724, Validation Loss: 0.1153, Dropout p: 0.30, Patience: 0/50            
Epoch [0103/0300], Training Loss: 0.0628, Validation Loss: 0.0766, Dropout p: 0.30, Patience: 0/50            
Epoch [0104/0300], Training Loss: 0.0709, Validation Loss: 0.1392, Dropout p: 0.30, Patience: 0/50            
Epoch [0105/0300], Training Loss: 0.0634, Validation Loss: 0.1092, Dropout p: 0.30, Patience: 0/50            
Epoch [0106/0300], Training Loss: 0.0523, Validation Loss: 0.0708, Dropout p: 0.30, Patience: 0/50            
Epoch [0107/0300], Training Loss: 0.0744, Validation Loss: 0.1129, Dropout p: 0.30, Patience: 0/50            
Epoch [0108/0300], Training Loss: 0.0705, Validation Loss: 0.0822, Dropout p: 0.30, Patience: 0/50            
Epoch [0109/0300], Training Loss: 0.0768, Validation Loss: 0.1224, Dropout p: 0.30, Patience: 0/50            
Epoch [0110/0300], Training Loss: 0.0657, Validation Loss: 0.0999, Dropout p: 0.30, Patience: 0/50            
Epoch [0111/0300], Training Loss: 0.0786, Validation Loss: 0.1280, Dropout p: 0.30, Patience: 0/50            
Epoch [0112/0300], Training Loss: 0.0623, Validation Loss: 0.1145, Dropout p: 0.30, Patience: 0/50            
Epoch [0113/0300], Training Loss: 0.0604, Validation Loss: 0.0819, Dropout p: 0.30, Patience: 0/50            
Epoch [0114/0300], Training Loss: 0.0574, Validation Loss: 0.0969, Dropout p: 0.30, Patience: 0/50            
Epoch [0115/0300], Training Loss: 0.0666, Validation Loss: 0.0945, Dropout p: 0.30, Patience: 0/50            
Epoch [0116/0300], Training Loss: 0.0672, Validation Loss: 0.0917, Dropout p: 0.30, Patience: 0/50            
Epoch [0117/0300], Training Loss: 0.0604, Validation Loss: 0.0919, Dropout p: 0.30, Patience: 0/50            
Epoch [0118/0300], Training Loss: 0.0493, Validation Loss: 0.0742, Dropout p: 0.30, Patience: 0/50            
Epoch [0119/0300], Training Loss: 0.0535, Validation Loss: 0.0925, Dropout p: 0.30, Patience: 0/50            
Epoch [0120/0300], Training Loss: 0.0561, Validation Loss: 0.0868, Dropout p: 0.30, Patience: 0/50            
Epoch [0121/0300], Training Loss: 0.0602, Validation Loss: 0.0886, Dropout p: 0.30, Patience: 0/50            
Epoch [0122/0300], Training Loss: 0.0563, Validation Loss: 0.1090, Dropout p: 0.30, Patience: 0/50            
Epoch [0123/0300], Training Loss: 0.0689, Validation Loss: 0.0718, Dropout p: 0.30, Patience: 0/50            
Epoch [0124/0300], Training Loss: 0.0545, Validation Loss: 0.1110, Dropout p: 0.30, Patience: 0/50            
Epoch [0125/0300], Training Loss: 0.0484, Validation Loss: 0.0976, Dropout p: 0.30, Patience: 0/50            
Epoch [0126/0300], Training Loss: 0.0480, Validation Loss: 0.0928, Dropout p: 0.30, Patience: 0/50            
Epoch [0127/0300], Training Loss: 0.0445, Validation Loss: 0.0944, Dropout p: 0.30, Patience: 1/50            
Epoch [0128/0300], Training Loss: 0.0563, Validation Loss: 0.1139, Dropout p: 0.30, Patience: 2/50            
Epoch [0129/0300], Training Loss: 0.0566, Validation Loss: 0.0978, Dropout p: 0.30, Patience: 3/50            
Epoch [0130/0300], Training Loss: 0.0503, Validation Loss: 0.0934, Dropout p: 0.30, Patience: 4/50            
Epoch [0131/0300], Training Loss: 0.0528, Validation Loss: 0.1081, Dropout p: 0.30, Patience: 5/50            
Epoch [0132/0300], Training Loss: 0.0522, Validation Loss: 0.0981, Dropout p: 0.30, Patience: 6/50            
Epoch [0133/0300], Training Loss: 0.0541, Validation Loss: 0.1014, Dropout p: 0.30, Patience: 7/50            
Epoch [0134/0300], Training Loss: 0.0512, Validation Loss: 0.0975, Dropout p: 0.30, Patience: 8/50            
Epoch [0135/0300], Training Loss: 0.0522, Validation Loss: 0.0895, Dropout p: 0.30, Patience: 0/50            
Epoch [0136/0300], Training Loss: 0.0539, Validation Loss: 0.1118, Dropout p: 0.30, Patience: 1/50            
Epoch [0137/0300], Training Loss: 0.0576, Validation Loss: 0.0859, Dropout p: 0.30, Patience: 0/50            
Epoch [0138/0300], Training Loss: 0.0600, Validation Loss: 0.0945, Dropout p: 0.30, Patience: 1/50            
Epoch [0139/0300], Training Loss: 0.0627, Validation Loss: 0.0974, Dropout p: 0.30, Patience: 2/50            
Epoch [0140/0300], Training Loss: 0.0546, Validation Loss: 0.1114, Dropout p: 0.30, Patience: 3/50            
Epoch [0141/0300], Training Loss: 0.0531, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 4/50            
Epoch [0142/0300], Training Loss: 0.0480, Validation Loss: 0.1127, Dropout p: 0.30, Patience: 5/50            
Epoch [0143/0300], Training Loss: 0.0702, Validation Loss: 0.1040, Dropout p: 0.30, Patience: 6/50            
Epoch [0144/0300], Training Loss: 0.0548, Validation Loss: 0.0870, Dropout p: 0.30, Patience: 7/50            
Epoch [0145/0300], Training Loss: 0.0477, Validation Loss: 0.0964, Dropout p: 0.30, Patience: 8/50            
Epoch [0146/0300], Training Loss: 0.0559, Validation Loss: 0.0949, Dropout p: 0.30, Patience: 9/50            
Epoch [0147/0300], Training Loss: 0.0599, Validation Loss: 0.1136, Dropout p: 0.30, Patience: 10/50            
Epoch [0148/0300], Training Loss: 0.0442, Validation Loss: 0.0912, Dropout p: 0.30, Patience: 11/50            
Epoch [0149/0300], Training Loss: 0.0479, Validation Loss: 0.0899, Dropout p: 0.30, Patience: 12/50            
Epoch [0150/0300], Training Loss: 0.0514, Validation Loss: 0.0864, Dropout p: 0.30, Patience: 13/50            
Epoch [0151/0300], Training Loss: 0.0527, Validation Loss: 0.1030, Dropout p: 0.30, Patience: 14/50            
Epoch [0152/0300], Training Loss: 0.0492, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 15/50            
Epoch [0153/0300], Training Loss: 0.0422, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 16/50            
Epoch [0154/0300], Training Loss: 0.0582, Validation Loss: 0.0934, Dropout p: 0.30, Patience: 17/50            
Epoch [0155/0300], Training Loss: 0.0554, Validation Loss: 0.1020, Dropout p: 0.30, Patience: 18/50            
Epoch [0156/0300], Training Loss: 0.0436, Validation Loss: 0.1047, Dropout p: 0.30, Patience: 19/50            
Epoch [0157/0300], Training Loss: 0.0476, Validation Loss: 0.1046, Dropout p: 0.30, Patience: 20/50            
Epoch [0158/0300], Training Loss: 0.0539, Validation Loss: 0.1027, Dropout p: 0.30, Patience: 21/50            
Epoch [0159/0300], Training Loss: 0.0421, Validation Loss: 0.0981, Dropout p: 0.30, Patience: 22/50            
Epoch [0160/0300], Training Loss: 0.0524, Validation Loss: 0.1036, Dropout p: 0.30, Patience: 23/50            
Epoch [0161/0300], Training Loss: 0.0429, Validation Loss: 0.1197, Dropout p: 0.30, Patience: 24/50            
Epoch [0162/0300], Training Loss: 0.0422, Validation Loss: 0.1138, Dropout p: 0.30, Patience: 25/50            
Epoch [0163/0300], Training Loss: 0.0586, Validation Loss: 0.0976, Dropout p: 0.30, Patience: 26/50            
Epoch [0164/0300], Training Loss: 0.0406, Validation Loss: 0.0981, Dropout p: 0.30, Patience: 27/50            
Epoch [0165/0300], Training Loss: 0.0512, Validation Loss: 0.0911, Dropout p: 0.30, Patience: 28/50            
Epoch [0166/0300], Training Loss: 0.0506, Validation Loss: 0.0946, Dropout p: 0.30, Patience: 29/50            
Epoch [0167/0300], Training Loss: 0.0497, Validation Loss: 0.1122, Dropout p: 0.30, Patience: 30/50            
Epoch [0168/0300], Training Loss: 0.0449, Validation Loss: 0.0972, Dropout p: 0.30, Patience: 31/50            
Epoch [0169/0300], Training Loss: 0.0356, Validation Loss: 0.0998, Dropout p: 0.30, Patience: 32/50            
Epoch [0170/0300], Training Loss: 0.0474, Validation Loss: 0.1055, Dropout p: 0.30, Patience: 33/50            
Epoch [0171/0300], Training Loss: 0.0586, Validation Loss: 0.1130, Dropout p: 0.30, Patience: 34/50            
Epoch [0172/0300], Training Loss: 0.0414, Validation Loss: 0.1070, Dropout p: 0.30, Patience: 35/50            
Epoch [0173/0300], Training Loss: 0.0514, Validation Loss: 0.0916, Dropout p: 0.30, Patience: 36/50            
Epoch [0174/0300], Training Loss: 0.0548, Validation Loss: 0.0947, Dropout p: 0.30, Patience: 37/50            
Epoch [0175/0300], Training Loss: 0.0504, Validation Loss: 0.0892, Dropout p: 0.30, Patience: 38/50            
Epoch [0176/0300], Training Loss: 0.0500, Validation Loss: 0.0982, Dropout p: 0.30, Patience: 39/50            
Epoch [0177/0300], Training Loss: 0.0587, Validation Loss: 0.1058, Dropout p: 0.30, Patience: 40/50            
Epoch [0178/0300], Training Loss: 0.0373, Validation Loss: 0.1084, Dropout p: 0.30, Patience: 41/50            
Epoch [0179/0300], Training Loss: 0.0588, Validation Loss: 0.0975, Dropout p: 0.30, Patience: 42/50            
Epoch [0180/0300], Training Loss: 0.0425, Validation Loss: 0.1027, Dropout p: 0.30, Patience: 43/50            
Epoch [0181/0300], Training Loss: 0.0470, Validation Loss: 0.1135, Dropout p: 0.30, Patience: 44/50            
Epoch [0182/0300], Training Loss: 0.0475, Validation Loss: 0.1029, Dropout p: 0.30, Patience: 45/50            
Epoch [0183/0300], Training Loss: 0.0579, Validation Loss: 0.0966, Dropout p: 0.30, Patience: 46/50            
Epoch [0184/0300], Training Loss: 0.0505, Validation Loss: 0.0926, Dropout p: 0.30, Patience: 47/50            
Epoch [0185/0300], Training Loss: 0.0461, Validation Loss: 0.0893, Dropout p: 0.30, Patience: 48/50            
Epoch [0186/0300], Training Loss: 0.0369, Validation Loss: 0.0975, Dropout p: 0.30, Patience: 49/50            
Epoch [0187/0300], Training Loss: 0.0398, Validation Loss: 0.1140, Dropout p: 0.30, Patience: 50/50            

Evaluating model on test set...
Predicted TBV: 206.14	Actual TBV: 191.67	Difference: 14.47
Predicted TBV: 126.88	Actual TBV: 114.85	Difference: 12.03
Predicted TBV: 207.72	Actual TBV: 186.51	Difference: 21.21
Predicted TBV: 220.08	Actual TBV: 234.86	Difference: 14.78
Predicted TBV: 217.98	Actual TBV: 194.67	Difference: 23.31
Predicted TBV: 338.92	Actual TBV: 345.78	Difference: 6.86
Predicted TBV: 206.53	Actual TBV: 188.30	Difference: 18.23
Predicted TBV: 153.61	Actual TBV: 149.17	Difference: 4.44
Predicted TBV: 163.88	Actual TBV: 171.12	Difference: 7.24
Predicted TBV: 214.79	Actual TBV: 233.84	Difference: 19.05
Predicted TBV: 243.67	Actual TBV: 254.71	Difference: 11.04
Predicted TBV: 238.57	Actual TBV: 255.48	Difference: 16.91
Predicted TBV: 268.56	Actual TBV: 278.41	Difference: 9.85
Predicted TBV: 129.94	Actual TBV: 111.38	Difference: 18.56
Predicted TBV: 236.08	Actual TBV: 215.93	Difference: 20.15
Predicted TBV: 243.19	Actual TBV: 259.25	Difference: 16.06
Predicted TBV: 224.26	Actual TBV: 217.71	Difference: 6.55
Predicted TBV: 157.32	Actual TBV: 135.08	Difference: 22.24
Predicted TBV: 138.54	Actual TBV: 122.70	Difference: 15.84
Predicted TBV: 229.48	Actual TBV: 261.73	Difference: 32.25
Predicted TBV: 151.14	Actual TBV: 145.49	Difference: 5.65
Predicted TBV: 141.32	Actual TBV: 120.88	Difference: 20.44
Predicted TBV: 269.69	Actual TBV: 282.63	Difference: 12.94
Predicted TBV: 280.20	Actual TBV: 296.85	Difference: 16.65
Predicted TBV: 238.20	Actual TBV: 234.41	Difference: 3.79
Predicted TBV: 178.27	Actual TBV: 162.92	Difference: 15.35
Predicted TBV: 141.02	Actual TBV: 132.57	Difference: 8.45
Predicted TBV: 184.86	Actual TBV: 180.00	Difference: 4.86
Predicted TBV: 273.65	Actual TBV: 286.97	Difference: 13.32
Predicted TBV: 253.11	Actual TBV: 225.25	Difference: 27.86
Predicted TBV: 198.87	Actual TBV: 187.35	Difference: 11.52
Predicted TBV: 146.09	Actual TBV: 141.36	Difference: 4.73
Predicted TBV: 212.27	Actual TBV: 185.07	Difference: 27.20
Predicted TBV: 134.38	Actual TBV: 117.04	Difference: 17.34
Predicted TBV: 254.24	Actual TBV: 224.48	Difference: 29.76
Predicted TBV: 167.08	Actual TBV: 157.30	Difference: 9.78
Predicted TBV: 200.42	Actual TBV: 185.44	Difference: 14.98
Predicted TBV: 202.09	Actual TBV: 186.32	Difference: 15.77
Predicted TBV: 211.91	Actual TBV: 191.36	Difference: 20.55
Predicted TBV: 207.04	Actual TBV: 184.19	Difference: 22.85

Evaluating model with 30 Bayesian runs...
Refused raster with error 14.15. TBV: 6.86.
Refused raster with error 13.51. TBV: 11.04.
Refused raster with error 12.18. TBV: 22.24.
Refused raster with error 10.81. TBV: 12.94.
Refused raster with error 15.06. TBV: 11.52.
Refused raster with error 13.78. TBV: 14.98.
Refused raster with error 9.83. TBV: 22.85.
Refused raster with error 9.50. TBV: 37.70.
Refused raster with error 9.03. TBV: 10.15.
Refused raster with error 12.71. TBV: 10.02.
Refused raster with error 10.81. TBV: 3.69.
Refused raster with error 12.46. TBV: 8.13.
Refused raster with error 11.12. TBV: 2.73.
Refused raster with error 11.67. TBV: 3.51.

- - - - - -
Total Raster Count: 58
Refused Raster Count: 14
- - - - - -

- - - - - -
Non-bayesian prediction:
Mean Absolute Error: 13.82 cc
Standard Deviation: 8.41 cc
Big error count (>30): 2
Big error mean: 34.98 cc
Big error std: 2.72 cc
- - - - - -

- - - - - -
Bayesian prediction:
Mean Absolute Error: 14.16 cc
Standard Deviation: 8.14 cc
Big error count (>30): 1
Big error mean: 32.25 cc
Big error std: 0.00 cc
- - - - - -

 - - - Fold 4/6 - - -

Train volume count: 296
Validation volume count: 74
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '29' '7' '41' '33' '76' '30' '87' '77' '46' '45' '50' '18' '32' '21'
 '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.0873, Validation Loss: 0.9892, Dropout p: 0.30, Patience: 0/50            
Epoch [0002/0300], Training Loss: 0.9897, Validation Loss: 1.3284, Dropout p: 0.30, Patience: 0/50            
Epoch [0003/0300], Training Loss: 0.9260, Validation Loss: 1.3314, Dropout p: 0.30, Patience: 0/50            
Epoch [0004/0300], Training Loss: 0.7258, Validation Loss: 0.7385, Dropout p: 0.30, Patience: 0/50            
Epoch [0005/0300], Training Loss: 0.6753, Validation Loss: 0.5194, Dropout p: 0.30, Patience: 0/50            
Epoch [0006/0300], Training Loss: 0.5567, Validation Loss: 0.4193, Dropout p: 0.30, Patience: 0/50            
Epoch [0007/0300], Training Loss: 0.5091, Validation Loss: 0.3534, Dropout p: 0.30, Patience: 0/50            
Epoch [0008/0300], Training Loss: 0.4468, Validation Loss: 0.7649, Dropout p: 0.30, Patience: 0/50            
Epoch [0009/0300], Training Loss: 0.4502, Validation Loss: 0.4915, Dropout p: 0.30, Patience: 0/50            
Epoch [0010/0300], Training Loss: 0.4009, Validation Loss: 0.3163, Dropout p: 0.30, Patience: 0/50            
Epoch [0011/0300], Training Loss: 0.3502, Validation Loss: 0.3933, Dropout p: 0.30, Patience: 0/50            
Epoch [0012/0300], Training Loss: 0.3937, Validation Loss: 0.2374, Dropout p: 0.30, Patience: 0/50            
Epoch [0013/0300], Training Loss: 0.4093, Validation Loss: 0.2189, Dropout p: 0.30, Patience: 0/50            
Epoch [0014/0300], Training Loss: 0.3557, Validation Loss: 0.2672, Dropout p: 0.30, Patience: 0/50            
Epoch [0015/0300], Training Loss: 0.3870, Validation Loss: 0.4108, Dropout p: 0.30, Patience: 0/50            
Epoch [0016/0300], Training Loss: 0.3674, Validation Loss: 0.2882, Dropout p: 0.30, Patience: 0/50            
Epoch [0017/0300], Training Loss: 0.2852, Validation Loss: 0.3342, Dropout p: 0.30, Patience: 0/50            
Epoch [0018/0300], Training Loss: 0.3173, Validation Loss: 0.3215, Dropout p: 0.30, Patience: 0/50            
Epoch [0019/0300], Training Loss: 0.3675, Validation Loss: 0.2064, Dropout p: 0.30, Patience: 0/50            
Epoch [0020/0300], Training Loss: 0.2841, Validation Loss: 0.2741, Dropout p: 0.30, Patience: 0/50            
Epoch [0021/0300], Training Loss: 0.3161, Validation Loss: 0.1897, Dropout p: 0.30, Patience: 0/50            
Epoch [0022/0300], Training Loss: 0.3684, Validation Loss: 0.2574, Dropout p: 0.30, Patience: 0/50            
Epoch [0023/0300], Training Loss: 0.3467, Validation Loss: 0.3033, Dropout p: 0.30, Patience: 0/50            
Epoch [0024/0300], Training Loss: 0.2851, Validation Loss: 0.1717, Dropout p: 0.30, Patience: 0/50            
Epoch [0025/0300], Training Loss: 0.2374, Validation Loss: 0.2192, Dropout p: 0.30, Patience: 0/50            
Epoch [0026/0300], Training Loss: 0.2587, Validation Loss: 0.2291, Dropout p: 0.30, Patience: 0/50            
Epoch [0027/0300], Training Loss: 0.2489, Validation Loss: 0.1987, Dropout p: 0.30, Patience: 0/50            
Epoch [0028/0300], Training Loss: 0.2799, Validation Loss: 0.2005, Dropout p: 0.30, Patience: 0/50            
Epoch [0029/0300], Training Loss: 0.2332, Validation Loss: 0.2001, Dropout p: 0.30, Patience: 0/50            
Epoch [0030/0300], Training Loss: 0.2940, Validation Loss: 0.1843, Dropout p: 0.30, Patience: 0/50            
Epoch [0031/0300], Training Loss: 0.2417, Validation Loss: 0.1842, Dropout p: 0.30, Patience: 0/50            
Epoch [0032/0300], Training Loss: 0.2006, Validation Loss: 0.1679, Dropout p: 0.30, Patience: 0/50            
Epoch [0033/0300], Training Loss: 0.1926, Validation Loss: 0.2446, Dropout p: 0.30, Patience: 0/50            
Epoch [0034/0300], Training Loss: 0.2072, Validation Loss: 0.2439, Dropout p: 0.30, Patience: 0/50            
Epoch [0035/0300], Training Loss: 0.2148, Validation Loss: 0.1748, Dropout p: 0.30, Patience: 0/50            
Epoch [0036/0300], Training Loss: 0.2289, Validation Loss: 0.1886, Dropout p: 0.30, Patience: 0/50            
Epoch [0037/0300], Training Loss: 0.2455, Validation Loss: 0.2424, Dropout p: 0.30, Patience: 0/50            
Epoch [0038/0300], Training Loss: 0.1755, Validation Loss: 0.1784, Dropout p: 0.30, Patience: 0/50            
Epoch [0039/0300], Training Loss: 0.1909, Validation Loss: 0.2562, Dropout p: 0.30, Patience: 0/50            
Epoch [0040/0300], Training Loss: 0.2008, Validation Loss: 0.2307, Dropout p: 0.30, Patience: 0/50            
Epoch [0041/0300], Training Loss: 0.2029, Validation Loss: 0.1677, Dropout p: 0.30, Patience: 0/50            
Epoch [0042/0300], Training Loss: 0.1851, Validation Loss: 0.2347, Dropout p: 0.30, Patience: 0/50            
Epoch [0043/0300], Training Loss: 0.2008, Validation Loss: 0.1959, Dropout p: 0.30, Patience: 0/50            
Epoch [0044/0300], Training Loss: 0.1786, Validation Loss: 0.2355, Dropout p: 0.30, Patience: 0/50            
Epoch [0045/0300], Training Loss: 0.1816, Validation Loss: 0.2197, Dropout p: 0.30, Patience: 0/50            
Epoch [0046/0300], Training Loss: 0.1889, Validation Loss: 0.2478, Dropout p: 0.30, Patience: 0/50            
Epoch [0047/0300], Training Loss: 0.1674, Validation Loss: 0.1933, Dropout p: 0.30, Patience: 0/50            
Epoch [0048/0300], Training Loss: 0.1419, Validation Loss: 0.1908, Dropout p: 0.30, Patience: 0/50            
Epoch [0049/0300], Training Loss: 0.1290, Validation Loss: 0.2755, Dropout p: 0.30, Patience: 0/50            
Epoch [0050/0300], Training Loss: 0.1648, Validation Loss: 0.2535, Dropout p: 0.30, Patience: 0/50            
Epoch [0051/0300], Training Loss: 0.1482, Validation Loss: 0.1807, Dropout p: 0.30, Patience: 0/50            
Epoch [0052/0300], Training Loss: 0.1677, Validation Loss: 0.2171, Dropout p: 0.30, Patience: 0/50            
Epoch [0053/0300], Training Loss: 0.2039, Validation Loss: 0.1966, Dropout p: 0.30, Patience: 0/50            
Epoch [0054/0300], Training Loss: 0.1660, Validation Loss: 0.2005, Dropout p: 0.30, Patience: 0/50            
Epoch [0055/0300], Training Loss: 0.1895, Validation Loss: 0.2005, Dropout p: 0.30, Patience: 0/50            
Epoch [0056/0300], Training Loss: 0.1588, Validation Loss: 0.1691, Dropout p: 0.30, Patience: 0/50            
Epoch [0057/0300], Training Loss: 0.1790, Validation Loss: 0.1469, Dropout p: 0.30, Patience: 0/50            
Epoch [0058/0300], Training Loss: 0.1388, Validation Loss: 0.1948, Dropout p: 0.30, Patience: 0/50            
Epoch [0059/0300], Training Loss: 0.1455, Validation Loss: 0.1946, Dropout p: 0.30, Patience: 0/50            
Epoch [0060/0300], Training Loss: 0.1413, Validation Loss: 0.1740, Dropout p: 0.30, Patience: 0/50            
Epoch [0061/0300], Training Loss: 0.1353, Validation Loss: 0.1694, Dropout p: 0.30, Patience: 0/50            
Epoch [0062/0300], Training Loss: 0.1131, Validation Loss: 0.1421, Dropout p: 0.30, Patience: 0/50            
Epoch [0063/0300], Training Loss: 0.1307, Validation Loss: 0.1252, Dropout p: 0.30, Patience: 0/50            
Epoch [0064/0300], Training Loss: 0.1177, Validation Loss: 0.1369, Dropout p: 0.30, Patience: 0/50            
Epoch [0065/0300], Training Loss: 0.1126, Validation Loss: 0.1263, Dropout p: 0.30, Patience: 0/50            
Epoch [0066/0300], Training Loss: 0.1089, Validation Loss: 0.1432, Dropout p: 0.30, Patience: 0/50            
Epoch [0067/0300], Training Loss: 0.1181, Validation Loss: 0.1582, Dropout p: 0.30, Patience: 0/50            
Epoch [0068/0300], Training Loss: 0.1040, Validation Loss: 0.1566, Dropout p: 0.30, Patience: 0/50            
Epoch [0069/0300], Training Loss: 0.1185, Validation Loss: 0.1602, Dropout p: 0.30, Patience: 0/50            
Epoch [0070/0300], Training Loss: 0.1084, Validation Loss: 0.1576, Dropout p: 0.30, Patience: 0/50            
Epoch [0071/0300], Training Loss: 0.1021, Validation Loss: 0.1458, Dropout p: 0.30, Patience: 0/50            
Epoch [0072/0300], Training Loss: 0.1066, Validation Loss: 0.1483, Dropout p: 0.30, Patience: 0/50            
Epoch [0073/0300], Training Loss: 0.1271, Validation Loss: 0.1263, Dropout p: 0.30, Patience: 0/50            
Epoch [0074/0300], Training Loss: 0.0941, Validation Loss: 0.1523, Dropout p: 0.30, Patience: 0/50            
Epoch [0075/0300], Training Loss: 0.0893, Validation Loss: 0.1466, Dropout p: 0.30, Patience: 0/50            
Epoch [0076/0300], Training Loss: 0.1032, Validation Loss: 0.1501, Dropout p: 0.30, Patience: 0/50            
Epoch [0077/0300], Training Loss: 0.1004, Validation Loss: 0.1625, Dropout p: 0.30, Patience: 0/50            
Epoch [0078/0300], Training Loss: 0.0810, Validation Loss: 0.1652, Dropout p: 0.30, Patience: 0/50            
Epoch [0079/0300], Training Loss: 0.0945, Validation Loss: 0.1612, Dropout p: 0.30, Patience: 0/50            
Epoch [0080/0300], Training Loss: 0.1057, Validation Loss: 0.1496, Dropout p: 0.30, Patience: 0/50            
Epoch [0081/0300], Training Loss: 0.0691, Validation Loss: 0.1699, Dropout p: 0.30, Patience: 0/50            
Epoch [0082/0300], Training Loss: 0.1098, Validation Loss: 0.1573, Dropout p: 0.30, Patience: 0/50            
Epoch [0083/0300], Training Loss: 0.0758, Validation Loss: 0.1377, Dropout p: 0.30, Patience: 0/50            
Epoch [0084/0300], Training Loss: 0.0937, Validation Loss: 0.1449, Dropout p: 0.30, Patience: 0/50            
Epoch [0085/0300], Training Loss: 0.0967, Validation Loss: 0.1316, Dropout p: 0.30, Patience: 0/50            
Epoch [0086/0300], Training Loss: 0.0806, Validation Loss: 0.1435, Dropout p: 0.30, Patience: 0/50            
Epoch [0087/0300], Training Loss: 0.0681, Validation Loss: 0.1637, Dropout p: 0.30, Patience: 0/50            
Epoch [0088/0300], Training Loss: 0.0935, Validation Loss: 0.1457, Dropout p: 0.30, Patience: 0/50            
Epoch [0089/0300], Training Loss: 0.0852, Validation Loss: 0.1705, Dropout p: 0.30, Patience: 0/50            
Epoch [0090/0300], Training Loss: 0.0791, Validation Loss: 0.1403, Dropout p: 0.30, Patience: 0/50            
Epoch [0091/0300], Training Loss: 0.0893, Validation Loss: 0.1394, Dropout p: 0.30, Patience: 0/50            
Epoch [0092/0300], Training Loss: 0.0958, Validation Loss: 0.1607, Dropout p: 0.30, Patience: 0/50            
Epoch [0093/0300], Training Loss: 0.0747, Validation Loss: 0.1435, Dropout p: 0.30, Patience: 0/50            
Epoch [0094/0300], Training Loss: 0.0857, Validation Loss: 0.1283, Dropout p: 0.30, Patience: 0/50            
Epoch [0095/0300], Training Loss: 0.0891, Validation Loss: 0.1444, Dropout p: 0.30, Patience: 0/50            
Epoch [0096/0300], Training Loss: 0.0658, Validation Loss: 0.1343, Dropout p: 0.30, Patience: 0/50            
Epoch [0097/0300], Training Loss: 0.1035, Validation Loss: 0.1408, Dropout p: 0.30, Patience: 0/50            
Epoch [0098/0300], Training Loss: 0.0900, Validation Loss: 0.1576, Dropout p: 0.30, Patience: 0/50            
Epoch [0099/0300], Training Loss: 0.0714, Validation Loss: 0.1503, Dropout p: 0.30, Patience: 0/50            
Epoch [0100/0300], Training Loss: 0.0730, Validation Loss: 0.1500, Dropout p: 0.30, Patience: 0/50            
Epoch [0101/0300], Training Loss: 0.0732, Validation Loss: 0.1407, Dropout p: 0.30, Patience: 0/50            
Epoch [0102/0300], Training Loss: 0.0762, Validation Loss: 0.1569, Dropout p: 0.30, Patience: 0/50            
Epoch [0103/0300], Training Loss: 0.0773, Validation Loss: 0.1377, Dropout p: 0.30, Patience: 0/50            
Epoch [0104/0300], Training Loss: 0.0730, Validation Loss: 0.1464, Dropout p: 0.30, Patience: 0/50            
Epoch [0105/0300], Training Loss: 0.0714, Validation Loss: 0.1570, Dropout p: 0.30, Patience: 0/50            
Epoch [0106/0300], Training Loss: 0.0704, Validation Loss: 0.1399, Dropout p: 0.30, Patience: 0/50            
Epoch [0107/0300], Training Loss: 0.0687, Validation Loss: 0.1427, Dropout p: 0.30, Patience: 0/50            
Epoch [0108/0300], Training Loss: 0.0682, Validation Loss: 0.1536, Dropout p: 0.30, Patience: 0/50            
Epoch [0109/0300], Training Loss: 0.0710, Validation Loss: 0.1372, Dropout p: 0.30, Patience: 0/50            
Epoch [0110/0300], Training Loss: 0.0619, Validation Loss: 0.1339, Dropout p: 0.30, Patience: 0/50            
Epoch [0111/0300], Training Loss: 0.0845, Validation Loss: 0.1356, Dropout p: 0.30, Patience: 0/50            
Epoch [0112/0300], Training Loss: 0.0708, Validation Loss: 0.1338, Dropout p: 0.30, Patience: 0/50            
Epoch [0113/0300], Training Loss: 0.0736, Validation Loss: 0.1462, Dropout p: 0.30, Patience: 0/50            
Epoch [0114/0300], Training Loss: 0.0683, Validation Loss: 0.1385, Dropout p: 0.30, Patience: 0/50            
Epoch [0115/0300], Training Loss: 0.0614, Validation Loss: 0.1488, Dropout p: 0.30, Patience: 0/50            
Epoch [0116/0300], Training Loss: 0.0596, Validation Loss: 0.1398, Dropout p: 0.30, Patience: 0/50            
Epoch [0117/0300], Training Loss: 0.0611, Validation Loss: 0.1394, Dropout p: 0.30, Patience: 0/50            
Epoch [0118/0300], Training Loss: 0.0652, Validation Loss: 0.1335, Dropout p: 0.30, Patience: 0/50            
Epoch [0119/0300], Training Loss: 0.0563, Validation Loss: 0.1246, Dropout p: 0.30, Patience: 0/50            
Epoch [0120/0300], Training Loss: 0.0519, Validation Loss: 0.1306, Dropout p: 0.30, Patience: 0/50            
Epoch [0121/0300], Training Loss: 0.0630, Validation Loss: 0.1269, Dropout p: 0.30, Patience: 0/50            
Epoch [0122/0300], Training Loss: 0.0746, Validation Loss: 0.1433, Dropout p: 0.30, Patience: 0/50            
Epoch [0123/0300], Training Loss: 0.0646, Validation Loss: 0.1474, Dropout p: 0.30, Patience: 0/50            
Epoch [0124/0300], Training Loss: 0.0645, Validation Loss: 0.1402, Dropout p: 0.30, Patience: 0/50            
Epoch [0125/0300], Training Loss: 0.0603, Validation Loss: 0.1348, Dropout p: 0.30, Patience: 0/50            
Epoch [0126/0300], Training Loss: 0.0604, Validation Loss: 0.1304, Dropout p: 0.30, Patience: 0/50            
Epoch [0127/0300], Training Loss: 0.0827, Validation Loss: 0.1365, Dropout p: 0.30, Patience: 1/50            
Epoch [0128/0300], Training Loss: 0.0658, Validation Loss: 0.1238, Dropout p: 0.30, Patience: 0/50            
Epoch [0129/0300], Training Loss: 0.0560, Validation Loss: 0.1477, Dropout p: 0.30, Patience: 1/50            
Epoch [0130/0300], Training Loss: 0.0532, Validation Loss: 0.1309, Dropout p: 0.30, Patience: 2/50            
Epoch [0131/0300], Training Loss: 0.0570, Validation Loss: 0.1394, Dropout p: 0.30, Patience: 3/50            
Epoch [0132/0300], Training Loss: 0.0454, Validation Loss: 0.1409, Dropout p: 0.30, Patience: 4/50            
Epoch [0133/0300], Training Loss: 0.0640, Validation Loss: 0.1486, Dropout p: 0.30, Patience: 5/50            
Epoch [0134/0300], Training Loss: 0.0507, Validation Loss: 0.1338, Dropout p: 0.30, Patience: 6/50            
Epoch [0135/0300], Training Loss: 0.0654, Validation Loss: 0.1298, Dropout p: 0.30, Patience: 7/50            
Epoch [0136/0300], Training Loss: 0.0527, Validation Loss: 0.1370, Dropout p: 0.30, Patience: 8/50            
Epoch [0137/0300], Training Loss: 0.0574, Validation Loss: 0.1288, Dropout p: 0.30, Patience: 9/50            
Epoch [0138/0300], Training Loss: 0.0602, Validation Loss: 0.1323, Dropout p: 0.30, Patience: 10/50            
Epoch [0139/0300], Training Loss: 0.0567, Validation Loss: 0.1287, Dropout p: 0.30, Patience: 11/50            
Epoch [0140/0300], Training Loss: 0.0502, Validation Loss: 0.1310, Dropout p: 0.30, Patience: 12/50            
Epoch [0141/0300], Training Loss: 0.0599, Validation Loss: 0.1426, Dropout p: 0.30, Patience: 13/50            
Epoch [0142/0300], Training Loss: 0.0495, Validation Loss: 0.1297, Dropout p: 0.30, Patience: 14/50            
Epoch [0143/0300], Training Loss: 0.0488, Validation Loss: 0.1281, Dropout p: 0.30, Patience: 15/50            
Epoch [0144/0300], Training Loss: 0.0502, Validation Loss: 0.1227, Dropout p: 0.30, Patience: 0/50            
Epoch [0145/0300], Training Loss: 0.0487, Validation Loss: 0.1220, Dropout p: 0.30, Patience: 0/50            
Epoch [0146/0300], Training Loss: 0.0588, Validation Loss: 0.1279, Dropout p: 0.30, Patience: 1/50            
Epoch [0147/0300], Training Loss: 0.0566, Validation Loss: 0.1280, Dropout p: 0.30, Patience: 2/50            
Epoch [0148/0300], Training Loss: 0.0565, Validation Loss: 0.1337, Dropout p: 0.30, Patience: 3/50            
Epoch [0149/0300], Training Loss: 0.0586, Validation Loss: 0.1312, Dropout p: 0.30, Patience: 4/50            
Epoch [0150/0300], Training Loss: 0.0608, Validation Loss: 0.1288, Dropout p: 0.30, Patience: 5/50            
Epoch [0151/0300], Training Loss: 0.0500, Validation Loss: 0.1297, Dropout p: 0.30, Patience: 6/50            
Epoch [0152/0300], Training Loss: 0.0589, Validation Loss: 0.1322, Dropout p: 0.30, Patience: 7/50            
Epoch [0153/0300], Training Loss: 0.0467, Validation Loss: 0.1223, Dropout p: 0.30, Patience: 8/50            
Epoch [0154/0300], Training Loss: 0.0570, Validation Loss: 0.1310, Dropout p: 0.30, Patience: 9/50            
Epoch [0155/0300], Training Loss: 0.0544, Validation Loss: 0.1268, Dropout p: 0.30, Patience: 10/50            
Epoch [0156/0300], Training Loss: 0.0540, Validation Loss: 0.1289, Dropout p: 0.30, Patience: 11/50            
Epoch [0157/0300], Training Loss: 0.0571, Validation Loss: 0.1274, Dropout p: 0.30, Patience: 12/50            
Epoch [0158/0300], Training Loss: 0.0495, Validation Loss: 0.1299, Dropout p: 0.30, Patience: 13/50            
Epoch [0159/0300], Training Loss: 0.0565, Validation Loss: 0.1294, Dropout p: 0.30, Patience: 14/50            
Epoch [0160/0300], Training Loss: 0.0514, Validation Loss: 0.1351, Dropout p: 0.30, Patience: 15/50            
Epoch [0161/0300], Training Loss: 0.0550, Validation Loss: 0.1324, Dropout p: 0.30, Patience: 16/50            
Epoch [0162/0300], Training Loss: 0.0462, Validation Loss: 0.1295, Dropout p: 0.30, Patience: 17/50            
Epoch [0163/0300], Training Loss: 0.0500, Validation Loss: 0.1284, Dropout p: 0.30, Patience: 18/50            
Epoch [0164/0300], Training Loss: 0.0492, Validation Loss: 0.1279, Dropout p: 0.30, Patience: 19/50            
Epoch [0165/0300], Training Loss: 0.0491, Validation Loss: 0.1329, Dropout p: 0.30, Patience: 20/50            
Epoch [0166/0300], Training Loss: 0.0466, Validation Loss: 0.1299, Dropout p: 0.30, Patience: 21/50            
Epoch [0167/0300], Training Loss: 0.0500, Validation Loss: 0.1315, Dropout p: 0.30, Patience: 22/50            
Epoch [0168/0300], Training Loss: 0.0514, Validation Loss: 0.1273, Dropout p: 0.30, Patience: 23/50            
Epoch [0169/0300], Training Loss: 0.0570, Validation Loss: 0.1347, Dropout p: 0.30, Patience: 24/50            
Epoch [0170/0300], Training Loss: 0.0401, Validation Loss: 0.1324, Dropout p: 0.30, Patience: 25/50            
Epoch [0171/0300], Training Loss: 0.0502, Validation Loss: 0.1341, Dropout p: 0.30, Patience: 26/50            
Epoch [0172/0300], Training Loss: 0.0496, Validation Loss: 0.1314, Dropout p: 0.30, Patience: 27/50            
Epoch [0173/0300], Training Loss: 0.0455, Validation Loss: 0.1267, Dropout p: 0.30, Patience: 28/50            
Epoch [0174/0300], Training Loss: 0.0568, Validation Loss: 0.1262, Dropout p: 0.30, Patience: 29/50            
Epoch [0175/0300], Training Loss: 0.0506, Validation Loss: 0.1299, Dropout p: 0.30, Patience: 30/50            
Epoch [0176/0300], Training Loss: 0.0575, Validation Loss: 0.1309, Dropout p: 0.30, Patience: 31/50            
Epoch [0177/0300], Training Loss: 0.0507, Validation Loss: 0.1300, Dropout p: 0.30, Patience: 32/50            
Epoch [0178/0300], Training Loss: 0.0506, Validation Loss: 0.1302, Dropout p: 0.30, Patience: 33/50            
Epoch [0179/0300], Training Loss: 0.0414, Validation Loss: 0.1298, Dropout p: 0.30, Patience: 34/50            
Epoch [0180/0300], Training Loss: 0.0602, Validation Loss: 0.1312, Dropout p: 0.30, Patience: 35/50            
Epoch [0181/0300], Training Loss: 0.0651, Validation Loss: 0.1312, Dropout p: 0.30, Patience: 36/50            
Epoch [0182/0300], Training Loss: 0.0566, Validation Loss: 0.1298, Dropout p: 0.30, Patience: 37/50            
Epoch [0183/0300], Training Loss: 0.0690, Validation Loss: 0.1300, Dropout p: 0.30, Patience: 38/50            
Epoch [0184/0300], Training Loss: 0.0698, Validation Loss: 0.1303, Dropout p: 0.30, Patience: 39/50            
Epoch [0185/0300], Training Loss: 0.0499, Validation Loss: 0.1290, Dropout p: 0.30, Patience: 40/50            
Epoch [0186/0300], Training Loss: 0.0467, Validation Loss: 0.1328, Dropout p: 0.30, Patience: 41/50            
Epoch [0187/0300], Training Loss: 0.0560, Validation Loss: 0.1288, Dropout p: 0.30, Patience: 42/50            
Epoch [0188/0300], Training Loss: 0.0508, Validation Loss: 0.1290, Dropout p: 0.30, Patience: 43/50            
Epoch [0189/0300], Training Loss: 0.0620, Validation Loss: 0.1300, Dropout p: 0.30, Patience: 44/50            
Epoch [0190/0300], Training Loss: 0.0514, Validation Loss: 0.1302, Dropout p: 0.30, Patience: 45/50            
Epoch [0191/0300], Training Loss: 0.0529, Validation Loss: 0.1321, Dropout p: 0.30, Patience: 46/50            
Epoch [0192/0300], Training Loss: 0.0483, Validation Loss: 0.1302, Dropout p: 0.30, Patience: 47/50            
Epoch [0193/0300], Training Loss: 0.0567, Validation Loss: 0.1285, Dropout p: 0.30, Patience: 48/50            
Epoch [0194/0300], Training Loss: 0.0387, Validation Loss: 0.1316, Dropout p: 0.30, Patience: 49/50            
Epoch [0195/0300], Training Loss: 0.0542, Validation Loss: 0.1296, Dropout p: 0.30, Patience: 50/50            

Evaluating model on test set...
Predicted TBV: 264.13	Actual TBV: 274.21	Difference: 10.08
Predicted TBV: 309.52	Actual TBV: 297.34	Difference: 12.18
Predicted TBV: 204.86	Actual TBV: 193.54	Difference: 11.32
Predicted TBV: 277.95	Actual TBV: 288.02	Difference: 10.07
Predicted TBV: 245.69	Actual TBV: 217.43	Difference: 28.26
Predicted TBV: 253.48	Actual TBV: 244.76	Difference: 8.72
Predicted TBV: 222.98	Actual TBV: 227.86	Difference: 4.88
Predicted TBV: 211.48	Actual TBV: 216.03	Difference: 4.55
Predicted TBV: 311.37	Actual TBV: 287.25	Difference: 24.12
Predicted TBV: 285.89	Actual TBV: 254.41	Difference: 31.48
Predicted TBV: 181.95	Actual TBV: 177.02	Difference: 4.93
Predicted TBV: 310.03	Actual TBV: 318.79	Difference: 8.76
Predicted TBV: 225.71	Actual TBV: 220.38	Difference: 5.33
Predicted TBV: 317.55	Actual TBV: 301.98	Difference: 15.57
Predicted TBV: 169.24	Actual TBV: 174.04	Difference: 4.80
Predicted TBV: 225.60	Actual TBV: 241.29	Difference: 15.69
Predicted TBV: 211.62	Actual TBV: 214.35	Difference: 2.73
Predicted TBV: 198.66	Actual TBV: 208.83	Difference: 10.17
Predicted TBV: 199.10	Actual TBV: 181.75	Difference: 17.35
Predicted TBV: 237.44	Actual TBV: 238.17	Difference: 0.73
Predicted TBV: 170.44	Actual TBV: 152.74	Difference: 17.70
Predicted TBV: 130.33	Actual TBV: 118.78	Difference: 11.55
Predicted TBV: 366.73	Actual TBV: 330.32	Difference: 36.41
Predicted TBV: 213.77	Actual TBV: 219.78	Difference: 6.01
Predicted TBV: 194.85	Actual TBV: 189.84	Difference: 5.01
Predicted TBV: 111.11	Actual TBV: 103.13	Difference: 7.98
Predicted TBV: 210.64	Actual TBV: 207.10	Difference: 3.54
Predicted TBV: 255.57	Actual TBV: 256.54	Difference: 0.97
Predicted TBV: 246.85	Actual TBV: 236.15	Difference: 10.70
Predicted TBV: 252.36	Actual TBV: 219.26	Difference: 33.10
Predicted TBV: 265.12	Actual TBV: 269.49	Difference: 4.37
Predicted TBV: 201.15	Actual TBV: 199.59	Difference: 1.56
Predicted TBV: 179.82	Actual TBV: 180.37	Difference: 0.55
Predicted TBV: 201.29	Actual TBV: 209.36	Difference: 8.07
Predicted TBV: 190.79	Actual TBV: 182.50	Difference: 8.29
Predicted TBV: 199.10	Actual TBV: 208.69	Difference: 9.59
Predicted TBV: 195.86	Actual TBV: 205.68	Difference: 9.82
Predicted TBV: 257.47	Actual TBV: 274.21	Difference: 16.74
Predicted TBV: 173.93	Actual TBV: 173.22	Difference: 0.71
Predicted TBV: 171.37	Actual TBV: 178.10	Difference: 6.73

Evaluating model with 30 Bayesian runs...
Refused raster with error 9.51. TBV: 10.08.
Refused raster with error 9.50. TBV: 28.26.
Refused raster with error 9.35. TBV: 4.55.
Refused raster with error 16.89. TBV: 24.12.
Refused raster with error 10.60. TBV: 31.48.
Refused raster with error 9.57. TBV: 5.33.
Refused raster with error 12.09. TBV: 15.57.
Refused raster with error 12.56. TBV: 36.41.
Refused raster with error 9.33. TBV: 8.07.
Refused raster with error 11.90. TBV: 16.74.
Refused raster with error 10.07. TBV: 16.59.
Refused raster with error 9.74. TBV: 25.37.
Refused raster with error 10.19. TBV: 2.95.
Refused raster with error 13.35. TBV: 30.15.
Refused raster with error 16.62. TBV: 1.36.
Refused raster with error 11.36. TBV: 8.03.
Refused raster with error 10.07. TBV: 1.51.
Refused raster with error 11.54. TBV: 20.69.
Refused raster with error 9.89. TBV: 18.52.

- - - - - -
Total Raster Count: 74
Refused Raster Count: 19
- - - - - -

- - - - - -
Non-bayesian prediction:
Mean Absolute Error: 12.18 cc
Standard Deviation: 10.28 cc
Big error count (>30): 7
Big error mean: 35.72 cc
Big error std: 4.91 cc
- - - - - -

- - - - - -
Bayesian prediction:
Mean Absolute Error: 10.83 cc
Standard Deviation: 9.75 cc
Big error count (>30): 4
Big error mean: 38.01 cc
Big error std: 4.95 cc
- - - - - -

 - - - Fold 5/6 - - -

Train volume count: 321
Validation volume count: 49
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '29' '7' '41' '33' '76' '30' '87' '77' '79' '43' '47' '35' '16' '44'
 '26' '81' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.0924, Validation Loss: 0.4261, Dropout p: 0.30, Patience: 0/50            
Epoch [0002/0300], Training Loss: 0.9859, Validation Loss: 1.3259, Dropout p: 0.30, Patience: 0/50            
Epoch [0003/0300], Training Loss: 0.8729, Validation Loss: 0.4153, Dropout p: 0.30, Patience: 0/50            
Epoch [0004/0300], Training Loss: 0.8086, Validation Loss: 0.4900, Dropout p: 0.30, Patience: 0/50            
Epoch [0005/0300], Training Loss: 0.6424, Validation Loss: 0.1255, Dropout p: 0.30, Patience: 0/50            
Epoch [0006/0300], Training Loss: 0.6105, Validation Loss: 0.3119, Dropout p: 0.30, Patience: 0/50            
Epoch [0007/0300], Training Loss: 0.5050, Validation Loss: 0.1132, Dropout p: 0.30, Patience: 0/50            
Epoch [0008/0300], Training Loss: 0.4591, Validation Loss: 0.3143, Dropout p: 0.30, Patience: 0/50            
Epoch [0009/0300], Training Loss: 0.4835, Validation Loss: 0.0929, Dropout p: 0.30, Patience: 0/50            
Epoch [0010/0300], Training Loss: 0.4083, Validation Loss: 0.2817, Dropout p: 0.30, Patience: 0/50            
Epoch [0011/0300], Training Loss: 0.5107, Validation Loss: 0.2251, Dropout p: 0.30, Patience: 0/50            
Epoch [0012/0300], Training Loss: 0.4107, Validation Loss: 0.0896, Dropout p: 0.30, Patience: 0/50            
Epoch [0013/0300], Training Loss: 0.3676, Validation Loss: 0.1542, Dropout p: 0.30, Patience: 0/50            
Epoch [0014/0300], Training Loss: 0.3719, Validation Loss: 0.0638, Dropout p: 0.30, Patience: 0/50            
Epoch [0015/0300], Training Loss: 0.3807, Validation Loss: 0.4492, Dropout p: 0.30, Patience: 0/50            
Epoch [0016/0300], Training Loss: 0.3281, Validation Loss: 0.0895, Dropout p: 0.30, Patience: 0/50            
Epoch [0017/0300], Training Loss: 0.2916, Validation Loss: 0.0941, Dropout p: 0.30, Patience: 0/50            
Epoch [0018/0300], Training Loss: 0.3739, Validation Loss: 0.0576, Dropout p: 0.30, Patience: 0/50            
Epoch [0019/0300], Training Loss: 0.3757, Validation Loss: 0.0868, Dropout p: 0.30, Patience: 0/50            
Epoch [0020/0300], Training Loss: 0.3521, Validation Loss: 0.1253, Dropout p: 0.30, Patience: 0/50            
Epoch [0021/0300], Training Loss: 0.2664, Validation Loss: 0.1584, Dropout p: 0.30, Patience: 0/50            
Epoch [0022/0300], Training Loss: 0.3550, Validation Loss: 0.0985, Dropout p: 0.30, Patience: 0/50            
Epoch [0023/0300], Training Loss: 0.4898, Validation Loss: 0.0573, Dropout p: 0.30, Patience: 0/50            
Epoch [0024/0300], Training Loss: 0.2716, Validation Loss: 0.0557, Dropout p: 0.30, Patience: 0/50            
Epoch [0025/0300], Training Loss: 0.2762, Validation Loss: 0.1337, Dropout p: 0.30, Patience: 0/50            
Epoch [0026/0300], Training Loss: 0.2953, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 0/50            
Epoch [0027/0300], Training Loss: 0.3028, Validation Loss: 0.0609, Dropout p: 0.30, Patience: 0/50            
Epoch [0028/0300], Training Loss: 0.2292, Validation Loss: 0.0949, Dropout p: 0.30, Patience: 0/50            
Epoch [0029/0300], Training Loss: 0.2062, Validation Loss: 0.0661, Dropout p: 0.30, Patience: 0/50            
Epoch [0030/0300], Training Loss: 0.2093, Validation Loss: 0.0861, Dropout p: 0.30, Patience: 0/50            
Epoch [0031/0300], Training Loss: 0.2378, Validation Loss: 0.1003, Dropout p: 0.30, Patience: 0/50            
Epoch [0032/0300], Training Loss: 0.2050, Validation Loss: 0.0913, Dropout p: 0.30, Patience: 0/50            
Epoch [0033/0300], Training Loss: 0.2397, Validation Loss: 0.2423, Dropout p: 0.30, Patience: 0/50            
Epoch [0034/0300], Training Loss: 0.4048, Validation Loss: 0.0658, Dropout p: 0.30, Patience: 0/50            
Epoch [0035/0300], Training Loss: 0.1858, Validation Loss: 0.0723, Dropout p: 0.30, Patience: 0/50            
Epoch [0036/0300], Training Loss: 0.2103, Validation Loss: 0.0792, Dropout p: 0.30, Patience: 0/50            
Epoch [0037/0300], Training Loss: 0.1871, Validation Loss: 0.0458, Dropout p: 0.30, Patience: 0/50            
Epoch [0038/0300], Training Loss: 0.2140, Validation Loss: 0.0777, Dropout p: 0.30, Patience: 0/50            
Epoch [0039/0300], Training Loss: 0.1626, Validation Loss: 0.0618, Dropout p: 0.30, Patience: 0/50            
Epoch [0040/0300], Training Loss: 0.2177, Validation Loss: 0.0705, Dropout p: 0.30, Patience: 0/50            
Epoch [0041/0300], Training Loss: 0.2171, Validation Loss: 0.0891, Dropout p: 0.30, Patience: 0/50            
Epoch [0042/0300], Training Loss: 0.3520, Validation Loss: 0.0540, Dropout p: 0.30, Patience: 0/50            
Epoch [0043/0300], Training Loss: 0.2149, Validation Loss: 0.0748, Dropout p: 0.30, Patience: 0/50            
Epoch [0044/0300], Training Loss: 0.1779, Validation Loss: 0.0806, Dropout p: 0.30, Patience: 0/50            
Epoch [0045/0300], Training Loss: 0.2212, Validation Loss: 0.0541, Dropout p: 0.30, Patience: 0/50            
Epoch [0046/0300], Training Loss: 0.2311, Validation Loss: 0.0634, Dropout p: 0.30, Patience: 0/50            
Epoch [0047/0300], Training Loss: 0.2871, Validation Loss: 0.0653, Dropout p: 0.30, Patience: 0/50            
Epoch [0048/0300], Training Loss: 0.1563, Validation Loss: 0.0654, Dropout p: 0.30, Patience: 0/50            
Epoch [0049/0300], Training Loss: 0.3465, Validation Loss: 0.0662, Dropout p: 0.30, Patience: 0/50            
Epoch [0050/0300], Training Loss: 0.1568, Validation Loss: 0.0872, Dropout p: 0.30, Patience: 0/50            
Epoch [0051/0300], Training Loss: 0.1792, Validation Loss: 0.0726, Dropout p: 0.30, Patience: 0/50            
Epoch [0052/0300], Training Loss: 0.1507, Validation Loss: 0.0627, Dropout p: 0.30, Patience: 0/50            
Epoch [0053/0300], Training Loss: 0.1632, Validation Loss: 0.0771, Dropout p: 0.30, Patience: 0/50            
Epoch [0054/0300], Training Loss: 0.1335, Validation Loss: 0.0687, Dropout p: 0.30, Patience: 0/50            
Epoch [0055/0300], Training Loss: 0.1568, Validation Loss: 0.0733, Dropout p: 0.30, Patience: 0/50            
Epoch [0056/0300], Training Loss: 0.1637, Validation Loss: 0.0401, Dropout p: 0.30, Patience: 0/50            
Epoch [0057/0300], Training Loss: 0.1961, Validation Loss: 0.0989, Dropout p: 0.30, Patience: 0/50            
Epoch [0058/0300], Training Loss: 0.1701, Validation Loss: 0.1184, Dropout p: 0.30, Patience: 0/50            
Epoch [0059/0300], Training Loss: 0.1488, Validation Loss: 0.0629, Dropout p: 0.30, Patience: 0/50            
Epoch [0060/0300], Training Loss: 0.1304, Validation Loss: 0.0466, Dropout p: 0.30, Patience: 0/50            
Epoch [0061/0300], Training Loss: 0.1802, Validation Loss: 0.1231, Dropout p: 0.30, Patience: 0/50            
Epoch [0062/0300], Training Loss: 0.1760, Validation Loss: 0.0653, Dropout p: 0.30, Patience: 0/50            
Epoch [0063/0300], Training Loss: 0.1939, Validation Loss: 0.0890, Dropout p: 0.30, Patience: 0/50            
Epoch [0064/0300], Training Loss: 0.1436, Validation Loss: 0.0807, Dropout p: 0.30, Patience: 0/50            
Epoch [0065/0300], Training Loss: 0.1161, Validation Loss: 0.0657, Dropout p: 0.30, Patience: 0/50            
Epoch [0066/0300], Training Loss: 0.1246, Validation Loss: 0.0494, Dropout p: 0.30, Patience: 0/50            
Epoch [0067/0300], Training Loss: 0.3421, Validation Loss: 0.0629, Dropout p: 0.30, Patience: 0/50            
Epoch [0068/0300], Training Loss: 0.1235, Validation Loss: 0.0913, Dropout p: 0.30, Patience: 0/50            
Epoch [0069/0300], Training Loss: 0.1173, Validation Loss: 0.0555, Dropout p: 0.30, Patience: 0/50            
Epoch [0070/0300], Training Loss: 0.1388, Validation Loss: 0.0700, Dropout p: 0.30, Patience: 0/50            
Epoch [0071/0300], Training Loss: 0.1099, Validation Loss: 0.0610, Dropout p: 0.30, Patience: 0/50            
Epoch [0072/0300], Training Loss: 0.1609, Validation Loss: 0.0814, Dropout p: 0.30, Patience: 0/50            
Epoch [0073/0300], Training Loss: 0.1367, Validation Loss: 0.0906, Dropout p: 0.30, Patience: 0/50            
Epoch [0074/0300], Training Loss: 0.1475, Validation Loss: 0.0862, Dropout p: 0.30, Patience: 0/50            
Epoch [0075/0300], Training Loss: 0.1279, Validation Loss: 0.0902, Dropout p: 0.30, Patience: 0/50            
Epoch [0076/0300], Training Loss: 0.1146, Validation Loss: 0.0712, Dropout p: 0.30, Patience: 0/50            
Epoch [0077/0300], Training Loss: 0.1065, Validation Loss: 0.0857, Dropout p: 0.30, Patience: 0/50            
Epoch [0078/0300], Training Loss: 0.1749, Validation Loss: 0.1409, Dropout p: 0.30, Patience: 0/50            
Epoch [0079/0300], Training Loss: 0.1086, Validation Loss: 0.0779, Dropout p: 0.30, Patience: 0/50            
Epoch [0080/0300], Training Loss: 0.1153, Validation Loss: 0.0507, Dropout p: 0.30, Patience: 0/50            
Epoch [0081/0300], Training Loss: 0.0902, Validation Loss: 0.0767, Dropout p: 0.30, Patience: 0/50            
Epoch [0082/0300], Training Loss: 0.1113, Validation Loss: 0.0549, Dropout p: 0.30, Patience: 0/50            
Epoch [0083/0300], Training Loss: 0.0983, Validation Loss: 0.0968, Dropout p: 0.30, Patience: 0/50            
Epoch [0084/0300], Training Loss: 0.2387, Validation Loss: 0.0757, Dropout p: 0.30, Patience: 0/50            
Epoch [0085/0300], Training Loss: 0.0958, Validation Loss: 0.1121, Dropout p: 0.30, Patience: 0/50            
Epoch [0086/0300], Training Loss: 0.1047, Validation Loss: 0.0653, Dropout p: 0.30, Patience: 0/50            
Epoch [0087/0300], Training Loss: 0.0893, Validation Loss: 0.0472, Dropout p: 0.30, Patience: 0/50            
Epoch [0088/0300], Training Loss: 0.0885, Validation Loss: 0.0301, Dropout p: 0.30, Patience: 0/50            
Epoch [0089/0300], Training Loss: 0.0988, Validation Loss: 0.0744, Dropout p: 0.30, Patience: 0/50            
Epoch [0090/0300], Training Loss: 0.1148, Validation Loss: 0.1012, Dropout p: 0.30, Patience: 0/50            
Epoch [0091/0300], Training Loss: 0.0921, Validation Loss: 0.1026, Dropout p: 0.30, Patience: 0/50            
Epoch [0092/0300], Training Loss: 0.1117, Validation Loss: 0.0774, Dropout p: 0.30, Patience: 0/50            
Epoch [0093/0300], Training Loss: 0.0875, Validation Loss: 0.1282, Dropout p: 0.30, Patience: 0/50            
Epoch [0094/0300], Training Loss: 0.0770, Validation Loss: 0.0680, Dropout p: 0.30, Patience: 0/50            
Epoch [0095/0300], Training Loss: 0.0693, Validation Loss: 0.1034, Dropout p: 0.30, Patience: 0/50            
Epoch [0096/0300], Training Loss: 0.1006, Validation Loss: 0.0712, Dropout p: 0.30, Patience: 0/50            
Epoch [0097/0300], Training Loss: 0.0906, Validation Loss: 0.0658, Dropout p: 0.30, Patience: 0/50            
Epoch [0098/0300], Training Loss: 0.1345, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 0/50            
Epoch [0099/0300], Training Loss: 0.0748, Validation Loss: 0.1729, Dropout p: 0.30, Patience: 0/50            
Epoch [0100/0300], Training Loss: 0.0998, Validation Loss: 0.0940, Dropout p: 0.30, Patience: 0/50            
Epoch [0101/0300], Training Loss: 0.0763, Validation Loss: 0.0793, Dropout p: 0.30, Patience: 0/50            
Epoch [0102/0300], Training Loss: 0.0784, Validation Loss: 0.0687, Dropout p: 0.30, Patience: 0/50            
Epoch [0103/0300], Training Loss: 0.1171, Validation Loss: 0.1009, Dropout p: 0.30, Patience: 0/50            
Epoch [0104/0300], Training Loss: 0.1079, Validation Loss: 0.1186, Dropout p: 0.30, Patience: 0/50            
Epoch [0105/0300], Training Loss: 0.0836, Validation Loss: 0.0893, Dropout p: 0.30, Patience: 0/50            
Epoch [0106/0300], Training Loss: 0.0647, Validation Loss: 0.0781, Dropout p: 0.30, Patience: 0/50            
Epoch [0107/0300], Training Loss: 0.0934, Validation Loss: 0.1136, Dropout p: 0.30, Patience: 0/50            
Epoch [0108/0300], Training Loss: 0.0700, Validation Loss: 0.0656, Dropout p: 0.30, Patience: 0/50            
Epoch [0109/0300], Training Loss: 0.0646, Validation Loss: 0.0456, Dropout p: 0.30, Patience: 0/50            
Epoch [0110/0300], Training Loss: 0.0758, Validation Loss: 0.1251, Dropout p: 0.30, Patience: 0/50            
Epoch [0111/0300], Training Loss: 0.0708, Validation Loss: 0.0486, Dropout p: 0.30, Patience: 0/50            
Epoch [0112/0300], Training Loss: 0.2307, Validation Loss: 0.1250, Dropout p: 0.30, Patience: 0/50            
Epoch [0113/0300], Training Loss: 0.0734, Validation Loss: 0.1008, Dropout p: 0.30, Patience: 0/50            
Epoch [0114/0300], Training Loss: 0.1015, Validation Loss: 0.0564, Dropout p: 0.30, Patience: 0/50            
Epoch [0115/0300], Training Loss: 0.0821, Validation Loss: 0.0719, Dropout p: 0.30, Patience: 0/50            
Epoch [0116/0300], Training Loss: 0.0733, Validation Loss: 0.0765, Dropout p: 0.30, Patience: 0/50            
Epoch [0117/0300], Training Loss: 0.0915, Validation Loss: 0.0986, Dropout p: 0.30, Patience: 0/50            
Epoch [0118/0300], Training Loss: 0.1861, Validation Loss: 0.1136, Dropout p: 0.30, Patience: 0/50            
Epoch [0119/0300], Training Loss: 0.0842, Validation Loss: 0.1154, Dropout p: 0.30, Patience: 0/50            
Epoch [0120/0300], Training Loss: 0.0834, Validation Loss: 0.1060, Dropout p: 0.30, Patience: 0/50            
Epoch [0121/0300], Training Loss: 0.0693, Validation Loss: 0.1017, Dropout p: 0.30, Patience: 0/50            
Epoch [0122/0300], Training Loss: 0.0587, Validation Loss: 0.0680, Dropout p: 0.30, Patience: 0/50            
Epoch [0123/0300], Training Loss: 0.0637, Validation Loss: 0.0959, Dropout p: 0.30, Patience: 0/50            
Epoch [0124/0300], Training Loss: 0.0598, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 0/50            
Epoch [0125/0300], Training Loss: 0.0711, Validation Loss: 0.0899, Dropout p: 0.30, Patience: 0/50            
Epoch [0126/0300], Training Loss: 0.0599, Validation Loss: 0.0924, Dropout p: 0.30, Patience: 0/50            
Epoch [0127/0300], Training Loss: 0.0671, Validation Loss: 0.1049, Dropout p: 0.30, Patience: 1/50            
Epoch [0128/0300], Training Loss: 0.0579, Validation Loss: 0.0995, Dropout p: 0.30, Patience: 2/50            
Epoch [0129/0300], Training Loss: 0.0535, Validation Loss: 0.1114, Dropout p: 0.30, Patience: 3/50            
Epoch [0130/0300], Training Loss: 0.1540, Validation Loss: 0.0775, Dropout p: 0.30, Patience: 0/50            
Epoch [0131/0300], Training Loss: 0.0538, Validation Loss: 0.0880, Dropout p: 0.30, Patience: 1/50            
Epoch [0132/0300], Training Loss: 0.1693, Validation Loss: 0.1078, Dropout p: 0.30, Patience: 2/50            
Epoch [0133/0300], Training Loss: 0.0559, Validation Loss: 0.0882, Dropout p: 0.30, Patience: 3/50            
Epoch [0134/0300], Training Loss: 0.0423, Validation Loss: 0.1170, Dropout p: 0.30, Patience: 4/50            
Epoch [0135/0300], Training Loss: 0.0489, Validation Loss: 0.1066, Dropout p: 0.30, Patience: 5/50            
Epoch [0136/0300], Training Loss: 0.0580, Validation Loss: 0.0956, Dropout p: 0.30, Patience: 6/50            
Epoch [0137/0300], Training Loss: 0.0518, Validation Loss: 0.0841, Dropout p: 0.30, Patience: 7/50            
Epoch [0138/0300], Training Loss: 0.0418, Validation Loss: 0.1037, Dropout p: 0.30, Patience: 8/50            
Epoch [0139/0300], Training Loss: 0.0539, Validation Loss: 0.0764, Dropout p: 0.30, Patience: 0/50            
Epoch [0140/0300], Training Loss: 0.0431, Validation Loss: 0.0723, Dropout p: 0.30, Patience: 0/50            
Epoch [0141/0300], Training Loss: 0.0385, Validation Loss: 0.0974, Dropout p: 0.30, Patience: 1/50            
Epoch [0142/0300], Training Loss: 0.0527, Validation Loss: 0.0920, Dropout p: 0.30, Patience: 2/50            
Epoch [0143/0300], Training Loss: 0.0501, Validation Loss: 0.1132, Dropout p: 0.30, Patience: 3/50            
Epoch [0144/0300], Training Loss: 0.0601, Validation Loss: 0.0972, Dropout p: 0.30, Patience: 4/50            
Epoch [0145/0300], Training Loss: 0.0543, Validation Loss: 0.0784, Dropout p: 0.30, Patience: 5/50            
Epoch [0146/0300], Training Loss: 0.0472, Validation Loss: 0.1019, Dropout p: 0.30, Patience: 6/50            
Epoch [0147/0300], Training Loss: 0.0474, Validation Loss: 0.1143, Dropout p: 0.30, Patience: 7/50            
Epoch [0148/0300], Training Loss: 0.0642, Validation Loss: 0.0778, Dropout p: 0.30, Patience: 8/50            
Epoch [0149/0300], Training Loss: 0.0449, Validation Loss: 0.0982, Dropout p: 0.30, Patience: 9/50            
Epoch [0150/0300], Training Loss: 0.0536, Validation Loss: 0.1249, Dropout p: 0.30, Patience: 10/50            
Epoch [0151/0300], Training Loss: 0.0484, Validation Loss: 0.1086, Dropout p: 0.30, Patience: 11/50            
Epoch [0152/0300], Training Loss: 0.0454, Validation Loss: 0.1210, Dropout p: 0.30, Patience: 12/50            
Epoch [0153/0300], Training Loss: 0.0561, Validation Loss: 0.1022, Dropout p: 0.30, Patience: 13/50            
Epoch [0154/0300], Training Loss: 0.0381, Validation Loss: 0.0897, Dropout p: 0.30, Patience: 14/50            
Epoch [0155/0300], Training Loss: 0.0367, Validation Loss: 0.0945, Dropout p: 0.30, Patience: 15/50            
Epoch [0156/0300], Training Loss: 0.0336, Validation Loss: 0.0935, Dropout p: 0.30, Patience: 16/50            
Epoch [0157/0300], Training Loss: 0.0408, Validation Loss: 0.0935, Dropout p: 0.30, Patience: 17/50            
Epoch [0158/0300], Training Loss: 0.0476, Validation Loss: 0.0865, Dropout p: 0.30, Patience: 18/50            
Epoch [0159/0300], Training Loss: 0.1300, Validation Loss: 0.1185, Dropout p: 0.30, Patience: 19/50            
Epoch [0160/0300], Training Loss: 0.0463, Validation Loss: 0.0752, Dropout p: 0.30, Patience: 20/50            
Epoch [0161/0300], Training Loss: 0.0402, Validation Loss: 0.0972, Dropout p: 0.30, Patience: 21/50            
Epoch [0162/0300], Training Loss: 0.0436, Validation Loss: 0.0800, Dropout p: 0.30, Patience: 22/50            
Epoch [0163/0300], Training Loss: 0.0466, Validation Loss: 0.0989, Dropout p: 0.30, Patience: 23/50            
Epoch [0164/0300], Training Loss: 0.0428, Validation Loss: 0.0925, Dropout p: 0.30, Patience: 24/50            
Epoch [0165/0300], Training Loss: 0.0372, Validation Loss: 0.0886, Dropout p: 0.30, Patience: 25/50            
Epoch [0166/0300], Training Loss: 0.0393, Validation Loss: 0.0938, Dropout p: 0.30, Patience: 26/50            
Epoch [0167/0300], Training Loss: 0.0364, Validation Loss: 0.1010, Dropout p: 0.30, Patience: 27/50            
Epoch [0168/0300], Training Loss: 0.0418, Validation Loss: 0.0920, Dropout p: 0.30, Patience: 28/50            
Epoch [0169/0300], Training Loss: 0.0346, Validation Loss: 0.0931, Dropout p: 0.30, Patience: 29/50            
Epoch [0170/0300], Training Loss: 0.0436, Validation Loss: 0.0947, Dropout p: 0.30, Patience: 30/50            
Epoch [0171/0300], Training Loss: 0.0393, Validation Loss: 0.0898, Dropout p: 0.30, Patience: 31/50            
Epoch [0172/0300], Training Loss: 0.0327, Validation Loss: 0.0900, Dropout p: 0.30, Patience: 32/50            
Epoch [0173/0300], Training Loss: 0.0367, Validation Loss: 0.0920, Dropout p: 0.30, Patience: 33/50            
Epoch [0174/0300], Training Loss: 0.0785, Validation Loss: 0.1055, Dropout p: 0.30, Patience: 34/50            
Epoch [0175/0300], Training Loss: 0.0478, Validation Loss: 0.0844, Dropout p: 0.30, Patience: 35/50            
Epoch [0176/0300], Training Loss: 0.0402, Validation Loss: 0.0830, Dropout p: 0.30, Patience: 36/50            
Epoch [0177/0300], Training Loss: 0.0400, Validation Loss: 0.0823, Dropout p: 0.30, Patience: 37/50            
Epoch [0178/0300], Training Loss: 0.0324, Validation Loss: 0.0848, Dropout p: 0.30, Patience: 38/50            
Epoch [0179/0300], Training Loss: 0.0659, Validation Loss: 0.1040, Dropout p: 0.30, Patience: 39/50            
Epoch [0180/0300], Training Loss: 0.0328, Validation Loss: 0.0948, Dropout p: 0.30, Patience: 40/50            
Epoch [0181/0300], Training Loss: 0.0520, Validation Loss: 0.0995, Dropout p: 0.30, Patience: 41/50            
Epoch [0182/0300], Training Loss: 0.0391, Validation Loss: 0.0968, Dropout p: 0.30, Patience: 42/50            
Epoch [0183/0300], Training Loss: 0.0344, Validation Loss: 0.0934, Dropout p: 0.30, Patience: 43/50            
Epoch [0184/0300], Training Loss: 0.0476, Validation Loss: 0.0948, Dropout p: 0.30, Patience: 44/50            
Epoch [0185/0300], Training Loss: 0.0420, Validation Loss: 0.0846, Dropout p: 0.30, Patience: 45/50            
Epoch [0186/0300], Training Loss: 0.0389, Validation Loss: 0.0865, Dropout p: 0.30, Patience: 46/50            
Epoch [0187/0300], Training Loss: 0.0397, Validation Loss: 0.0877, Dropout p: 0.30, Patience: 47/50            
Epoch [0188/0300], Training Loss: 0.0320, Validation Loss: 0.0929, Dropout p: 0.30, Patience: 48/50            
Epoch [0189/0300], Training Loss: 0.0385, Validation Loss: 0.0842, Dropout p: 0.30, Patience: 49/50            
Epoch [0190/0300], Training Loss: 0.1319, Validation Loss: 0.1056, Dropout p: 0.30, Patience: 50/50            

Evaluating model on test set...
Predicted TBV: 216.29	Actual TBV: 222.61	Difference: 6.32
Predicted TBV: 428.62	Actual TBV: 449.16	Difference: 20.54
Predicted TBV: 206.80	Actual TBV: 197.64	Difference: 9.16
Predicted TBV: 182.43	Actual TBV: 162.77	Difference: 19.66
Predicted TBV: 213.18	Actual TBV: 211.20	Difference: 1.98
Predicted TBV: 214.62	Actual TBV: 217.47	Difference: 2.85
Predicted TBV: 172.39	Actual TBV: 138.39	Difference: 34.00
Predicted TBV: 275.76	Actual TBV: 270.78	Difference: 4.98
Predicted TBV: 269.04	Actual TBV: 281.42	Difference: 12.38
Predicted TBV: 170.43	Actual TBV: 152.51	Difference: 17.92
Predicted TBV: 183.82	Actual TBV: 180.57	Difference: 3.25
Predicted TBV: 205.15	Actual TBV: 190.48	Difference: 14.67
Predicted TBV: 215.78	Actual TBV: 229.87	Difference: 14.09
Predicted TBV: 222.19	Actual TBV: 201.10	Difference: 21.09
Predicted TBV: 201.85	Actual TBV: 186.07	Difference: 15.78
Predicted TBV: 249.67	Actual TBV: 243.00	Difference: 6.67
Predicted TBV: 241.02	Actual TBV: 257.03	Difference: 16.01
Predicted TBV: 165.47	Actual TBV: 166.79	Difference: 1.32
Predicted TBV: 199.98	Actual TBV: 205.72	Difference: 5.74
Predicted TBV: 202.39	Actual TBV: 206.72	Difference: 4.33
Predicted TBV: 245.58	Actual TBV: 231.37	Difference: 14.21
Predicted TBV: 239.57	Actual TBV: 221.39	Difference: 18.18
Predicted TBV: 193.45	Actual TBV: 180.80	Difference: 12.65
Predicted TBV: 176.22	Actual TBV: 151.91	Difference: 24.31
Predicted TBV: 237.87	Actual TBV: 253.94	Difference: 16.07
Predicted TBV: 160.70	Actual TBV: 137.38	Difference: 23.32
Predicted TBV: 246.64	Actual TBV: 252.89	Difference: 6.25
Predicted TBV: 211.20	Actual TBV: 206.11	Difference: 5.09
Predicted TBV: 255.85	Actual TBV: 277.75	Difference: 21.90
Predicted TBV: 211.78	Actual TBV: 200.34	Difference: 11.44
Predicted TBV: 199.69	Actual TBV: 195.75	Difference: 3.94
Predicted TBV: 179.50	Actual TBV: 182.91	Difference: 3.41
Predicted TBV: 298.70	Actual TBV: 303.83	Difference: 5.13
Predicted TBV: 194.84	Actual TBV: 184.64	Difference: 10.20
Predicted TBV: 149.81	Actual TBV: 160.92	Difference: 11.11
Predicted TBV: 298.78	Actual TBV: 297.35	Difference: 1.43
Predicted TBV: 171.36	Actual TBV: 180.60	Difference: 9.24
Predicted TBV: 262.24	Actual TBV: 262.10	Difference: 0.14
Predicted TBV: 189.65	Actual TBV: 161.64	Difference: 28.01
Predicted TBV: 235.07	Actual TBV: 249.58	Difference: 14.51

Evaluating model with 30 Bayesian runs...
Refused raster with error 12.56. TBV: 20.54.
Refused raster with error 12.47. TBV: 21.09.
Refused raster with error 11.14. TBV: 5.09.
Refused raster with error 10.98. TBV: 1.43.

- - - - - -
Total Raster Count: 49
Refused Raster Count: 4
- - - - - -

- - - - - -
Non-bayesian prediction:
Mean Absolute Error: 11.85 cc
Standard Deviation: 8.19 cc
Big error count (>30): 2
Big error mean: 32.07 cc
Big error std: 1.93 cc
- - - - - -

- - - - - -
Bayesian prediction:
Mean Absolute Error: 11.83 cc
Standard Deviation: 8.12 cc
Big error count (>30): 2
Big error mean: 32.07 cc
Big error std: 1.93 cc
- - - - - -

 - - - Fold 6/6 - - -

Train volume count: 341
Validation volume count: 29
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '29' '7' '41' '33' '76' '30' '87' '77' '79' '43' '47' '35' '16' '44'
 '26' '81' '46' '45' '50' '18' '32' '21' '85' '3' '86']

Epoch [0001/0300], Training Loss: 1.0659, Validation Loss: 0.5546, Dropout p: 0.30, Patience: 0/50            
Epoch [0002/0300], Training Loss: 1.0311, Validation Loss: 0.5140, Dropout p: 0.30, Patience: 0/50            
Epoch [0003/0300], Training Loss: 0.9154, Validation Loss: 0.8518, Dropout p: 0.30, Patience: 0/50            
Epoch [0004/0300], Training Loss: 0.6829, Validation Loss: 0.5796, Dropout p: 0.30, Patience: 0/50            
Epoch [0005/0300], Training Loss: 0.6194, Validation Loss: 0.3652, Dropout p: 0.30, Patience: 0/50            
Epoch [0006/0300], Training Loss: 0.5693, Validation Loss: 0.5087, Dropout p: 0.30, Patience: 0/50            
Epoch [0007/0300], Training Loss: 0.5101, Validation Loss: 0.4082, Dropout p: 0.30, Patience: 0/50            
Epoch [0008/0300], Training Loss: 0.4130, Validation Loss: 0.4786, Dropout p: 0.30, Patience: 0/50            
Epoch [0009/0300], Training Loss: 0.4073, Validation Loss: 0.5853, Dropout p: 0.30, Patience: 0/50            
Epoch [0010/0300], Training Loss: 0.4246, Validation Loss: 0.4113, Dropout p: 0.30, Patience: 0/50            
Epoch [0011/0300], Training Loss: 0.4643, Validation Loss: 0.6667, Dropout p: 0.30, Patience: 0/50            
Epoch [0012/0300], Training Loss: 0.3664, Validation Loss: 0.3864, Dropout p: 0.30, Patience: 0/50            
Epoch [0013/0300], Training Loss: 0.4042, Validation Loss: 0.3958, Dropout p: 0.30, Patience: 0/50            
Epoch [0014/0300], Training Loss: 0.3763, Validation Loss: 0.5883, Dropout p: 0.30, Patience: 0/50            
Epoch [0015/0300], Training Loss: 0.3182, Validation Loss: 0.5116, Dropout p: 0.30, Patience: 0/50            
Epoch [0016/0300], Training Loss: 0.3285, Validation Loss: 0.4695, Dropout p: 0.30, Patience: 0/50            
Epoch [0017/0300], Training Loss: 0.3284, Validation Loss: 0.4945, Dropout p: 0.30, Patience: 0/50            
Epoch [0018/0300], Training Loss: 0.3383, Validation Loss: 0.4442, Dropout p: 0.30, Patience: 0/50            
Epoch [0019/0300], Training Loss: 0.3884, Validation Loss: 0.4401, Dropout p: 0.30, Patience: 0/50            
Epoch [0020/0300], Training Loss: 0.2579, Validation Loss: 0.4735, Dropout p: 0.30, Patience: 0/50            
Epoch [0021/0300], Training Loss: 0.2463, Validation Loss: 0.4049, Dropout p: 0.30, Patience: 0/50            
Epoch [0022/0300], Training Loss: 0.2872, Validation Loss: 0.4909, Dropout p: 0.30, Patience: 0/50            
Epoch [0023/0300], Training Loss: 0.2883, Validation Loss: 0.4867, Dropout p: 0.30, Patience: 0/50            
Epoch [0024/0300], Training Loss: 0.2915, Validation Loss: 0.4728, Dropout p: 0.30, Patience: 0/50            
Epoch [0025/0300], Training Loss: 0.2305, Validation Loss: 0.4439, Dropout p: 0.30, Patience: 0/50            
Epoch [0026/0300], Training Loss: 0.2152, Validation Loss: 0.4490, Dropout p: 0.30, Patience: 0/50            
Epoch [0027/0300], Training Loss: 0.2505, Validation Loss: 0.3849, Dropout p: 0.30, Patience: 0/50            
Epoch [0028/0300], Training Loss: 0.2492, Validation Loss: 0.4320, Dropout p: 0.30, Patience: 0/50            
Epoch [0029/0300], Training Loss: 0.2285, Validation Loss: 0.4497, Dropout p: 0.30, Patience: 0/50            
Epoch [0030/0300], Training Loss: 0.2448, Validation Loss: 0.5137, Dropout p: 0.30, Patience: 0/50            
Epoch [0031/0300], Training Loss: 0.2086, Validation Loss: 0.3973, Dropout p: 0.30, Patience: 0/50            
Epoch [0032/0300], Training Loss: 0.2015, Validation Loss: 0.4293, Dropout p: 0.30, Patience: 0/50            
Epoch [0033/0300], Training Loss: 0.2126, Validation Loss: 0.5623, Dropout p: 0.30, Patience: 0/50            
Epoch [0034/0300], Training Loss: 0.2383, Validation Loss: 0.5251, Dropout p: 0.30, Patience: 0/50            
Epoch [0035/0300], Training Loss: 0.1575, Validation Loss: 0.4299, Dropout p: 0.30, Patience: 0/50            
Epoch [0036/0300], Training Loss: 0.1860, Validation Loss: 0.3881, Dropout p: 0.30, Patience: 0/50            
Epoch [0037/0300], Training Loss: 0.1698, Validation Loss: 0.4610, Dropout p: 0.30, Patience: 0/50            
Epoch [0038/0300], Training Loss: 0.2050, Validation Loss: 0.4188, Dropout p: 0.30, Patience: 0/50            
Epoch [0039/0300], Training Loss: 0.1873, Validation Loss: 0.3623, Dropout p: 0.30, Patience: 0/50            
Epoch [0040/0300], Training Loss: 0.1792, Validation Loss: 0.3948, Dropout p: 0.30, Patience: 0/50            
Epoch [0041/0300], Training Loss: 0.1780, Validation Loss: 0.3681, Dropout p: 0.30, Patience: 0/50            
Epoch [0042/0300], Training Loss: 0.1808, Validation Loss: 0.4224, Dropout p: 0.30, Patience: 0/50            
Epoch [0043/0300], Training Loss: 0.1568, Validation Loss: 0.4111, Dropout p: 0.30, Patience: 0/50            
Epoch [0044/0300], Training Loss: 0.1517, Validation Loss: 0.3335, Dropout p: 0.30, Patience: 0/50            
Epoch [0045/0300], Training Loss: 0.1580, Validation Loss: 0.3714, Dropout p: 0.30, Patience: 0/50            
Epoch [0046/0300], Training Loss: 0.1521, Validation Loss: 0.2392, Dropout p: 0.30, Patience: 0/50            
Epoch [0047/0300], Training Loss: 0.1575, Validation Loss: 0.2793, Dropout p: 0.30, Patience: 0/50            
Epoch [0048/0300], Training Loss: 0.1701, Validation Loss: 0.2904, Dropout p: 0.30, Patience: 0/50            
Epoch [0049/0300], Training Loss: 0.1489, Validation Loss: 0.4365, Dropout p: 0.30, Patience: 0/50            
Epoch [0050/0300], Training Loss: 0.1392, Validation Loss: 0.4187, Dropout p: 0.30, Patience: 0/50            
Epoch [0051/0300], Training Loss: 0.1348, Validation Loss: 0.3971, Dropout p: 0.30, Patience: 0/50            
Epoch [0052/0300], Training Loss: 0.1462, Validation Loss: 0.4394, Dropout p: 0.30, Patience: 0/50            
Epoch [0053/0300], Training Loss: 0.1430, Validation Loss: 0.4642, Dropout p: 0.30, Patience: 0/50            
Epoch [0054/0300], Training Loss: 0.1342, Validation Loss: 0.2882, Dropout p: 0.30, Patience: 0/50            
Epoch [0055/0300], Training Loss: 0.1616, Validation Loss: 0.3629, Dropout p: 0.30, Patience: 0/50            
Epoch [0056/0300], Training Loss: 0.1322, Validation Loss: 0.4918, Dropout p: 0.30, Patience: 0/50            
Epoch [0057/0300], Training Loss: 0.1392, Validation Loss: 0.4479, Dropout p: 0.30, Patience: 0/50            
Epoch [0058/0300], Training Loss: 0.1320, Validation Loss: 0.4082, Dropout p: 0.30, Patience: 0/50            
Epoch [0059/0300], Training Loss: 0.1432, Validation Loss: 0.3818, Dropout p: 0.30, Patience: 0/50            
Epoch [0060/0300], Training Loss: 0.1449, Validation Loss: 0.5206, Dropout p: 0.30, Patience: 0/50            
Epoch [0061/0300], Training Loss: 0.1058, Validation Loss: 0.4390, Dropout p: 0.30, Patience: 0/50            
Epoch [0062/0300], Training Loss: 0.1087, Validation Loss: 0.4143, Dropout p: 0.30, Patience: 0/50            
Epoch [0063/0300], Training Loss: 0.1431, Validation Loss: 0.3890, Dropout p: 0.30, Patience: 0/50            
Epoch [0064/0300], Training Loss: 0.1099, Validation Loss: 0.4566, Dropout p: 0.30, Patience: 0/50            
Epoch [0065/0300], Training Loss: 0.1251, Validation Loss: 0.3686, Dropout p: 0.30, Patience: 0/50            
Epoch [0066/0300], Training Loss: 0.1427, Validation Loss: 0.3242, Dropout p: 0.30, Patience: 0/50            
Epoch [0067/0300], Training Loss: 0.1136, Validation Loss: 0.4202, Dropout p: 0.30, Patience: 0/50            
Epoch [0068/0300], Training Loss: 0.1215, Validation Loss: 0.4451, Dropout p: 0.30, Patience: 0/50            
Epoch [0069/0300], Training Loss: 0.1135, Validation Loss: 0.4732, Dropout p: 0.30, Patience: 0/50            
Epoch [0070/0300], Training Loss: 0.1107, Validation Loss: 0.4478, Dropout p: 0.30, Patience: 0/50            
Epoch [0071/0300], Training Loss: 0.1055, Validation Loss: 0.4107, Dropout p: 0.30, Patience: 0/50            
Epoch [0072/0300], Training Loss: 0.1072, Validation Loss: 0.4193, Dropout p: 0.30, Patience: 0/50            
Epoch [0073/0300], Training Loss: 0.1024, Validation Loss: 0.4010, Dropout p: 0.30, Patience: 0/50            
Epoch [0074/0300], Training Loss: 0.1201, Validation Loss: 0.4382, Dropout p: 0.30, Patience: 0/50            
Epoch [0075/0300], Training Loss: 0.1103, Validation Loss: 0.3168, Dropout p: 0.30, Patience: 0/50            
Epoch [0076/0300], Training Loss: 0.0856, Validation Loss: 0.3238, Dropout p: 0.30, Patience: 0/50            
Epoch [0077/0300], Training Loss: 0.1062, Validation Loss: 0.3546, Dropout p: 0.30, Patience: 0/50            
Epoch [0078/0300], Training Loss: 0.1160, Validation Loss: 0.3323, Dropout p: 0.30, Patience: 0/50            
Epoch [0079/0300], Training Loss: 0.1383, Validation Loss: 0.3832, Dropout p: 0.30, Patience: 0/50            
Epoch [0080/0300], Training Loss: 0.1044, Validation Loss: 0.4225, Dropout p: 0.30, Patience: 0/50            
Epoch [0081/0300], Training Loss: 0.1153, Validation Loss: 0.4428, Dropout p: 0.30, Patience: 0/50            
Epoch [0082/0300], Training Loss: 0.1140, Validation Loss: 0.3863, Dropout p: 0.30, Patience: 0/50            
Epoch [0083/0300], Training Loss: 0.1287, Validation Loss: 0.4490, Dropout p: 0.30, Patience: 0/50            
Epoch [0084/0300], Training Loss: 0.0918, Validation Loss: 0.4231, Dropout p: 0.30, Patience: 0/50            
Epoch [0085/0300], Training Loss: 0.0740, Validation Loss: 0.3899, Dropout p: 0.30, Patience: 0/50            
Epoch [0086/0300], Training Loss: 0.0788, Validation Loss: 0.4327, Dropout p: 0.30, Patience: 0/50            
Epoch [0087/0300], Training Loss: 0.0873, Validation Loss: 0.3515, Dropout p: 0.30, Patience: 0/50            
Epoch [0088/0300], Training Loss: 0.0922, Validation Loss: 0.4282, Dropout p: 0.30, Patience: 0/50            
Epoch [0089/0300], Training Loss: 0.0782, Validation Loss: 0.4894, Dropout p: 0.30, Patience: 0/50            
Epoch [0090/0300], Training Loss: 0.0851, Validation Loss: 0.4453, Dropout p: 0.30, Patience: 0/50            
Epoch [0091/0300], Training Loss: 0.0912, Validation Loss: 0.4205, Dropout p: 0.30, Patience: 0/50            
Epoch [0092/0300], Training Loss: 0.0784, Validation Loss: 0.3798, Dropout p: 0.30, Patience: 0/50            
Epoch [0093/0300], Training Loss: 0.0729, Validation Loss: 0.4123, Dropout p: 0.30, Patience: 0/50            
Epoch [0094/0300], Training Loss: 0.0987, Validation Loss: 0.4125, Dropout p: 0.30, Patience: 0/50            
Epoch [0095/0300], Training Loss: 0.0849, Validation Loss: 0.4066, Dropout p: 0.30, Patience: 0/50            
Epoch [0096/0300], Training Loss: 0.0752, Validation Loss: 0.3758, Dropout p: 0.30, Patience: 0/50            
Epoch [0097/0300], Training Loss: 0.0714, Validation Loss: 0.4052, Dropout p: 0.30, Patience: 0/50            
Epoch [0098/0300], Training Loss: 0.0746, Validation Loss: 0.3902, Dropout p: 0.30, Patience: 0/50            
Epoch [0099/0300], Training Loss: 0.0758, Validation Loss: 0.3922, Dropout p: 0.30, Patience: 0/50            
Epoch [0100/0300], Training Loss: 0.0909, Validation Loss: 0.4690, Dropout p: 0.30, Patience: 0/50            
Epoch [0101/0300], Training Loss: 0.0807, Validation Loss: 0.3460, Dropout p: 0.30, Patience: 0/50            
Epoch [0102/0300], Training Loss: 0.0943, Validation Loss: 0.3136, Dropout p: 0.30, Patience: 0/50            
Epoch [0103/0300], Training Loss: 0.0737, Validation Loss: 0.3679, Dropout p: 0.30, Patience: 0/50            
Epoch [0104/0300], Training Loss: 0.0661, Validation Loss: 0.3134, Dropout p: 0.30, Patience: 0/50            
Epoch [0105/0300], Training Loss: 0.0774, Validation Loss: 0.3937, Dropout p: 0.30, Patience: 0/50            
Epoch [0106/0300], Training Loss: 0.0865, Validation Loss: 0.4443, Dropout p: 0.30, Patience: 0/50            
Epoch [0107/0300], Training Loss: 0.0707, Validation Loss: 0.3953, Dropout p: 0.30, Patience: 0/50            
Epoch [0108/0300], Training Loss: 0.0728, Validation Loss: 0.4202, Dropout p: 0.30, Patience: 0/50            
Epoch [0109/0300], Training Loss: 0.0830, Validation Loss: 0.4134, Dropout p: 0.30, Patience: 0/50            
Epoch [0110/0300], Training Loss: 0.0664, Validation Loss: 0.3704, Dropout p: 0.30, Patience: 0/50            
Epoch [0111/0300], Training Loss: 0.0708, Validation Loss: 0.3993, Dropout p: 0.30, Patience: 0/50            
Epoch [0112/0300], Training Loss: 0.0641, Validation Loss: 0.4830, Dropout p: 0.30, Patience: 0/50            
Epoch [0113/0300], Training Loss: 0.0808, Validation Loss: 0.4162, Dropout p: 0.30, Patience: 0/50            
Epoch [0114/0300], Training Loss: 0.0683, Validation Loss: 0.4792, Dropout p: 0.30, Patience: 0/50            
Epoch [0115/0300], Training Loss: 0.0640, Validation Loss: 0.5237, Dropout p: 0.30, Patience: 0/50            
Epoch [0116/0300], Training Loss: 0.0750, Validation Loss: 0.4179, Dropout p: 0.30, Patience: 0/50            
Epoch [0117/0300], Training Loss: 0.0719, Validation Loss: 0.4404, Dropout p: 0.30, Patience: 0/50            
Epoch [0118/0300], Training Loss: 0.0805, Validation Loss: 0.4694, Dropout p: 0.30, Patience: 0/50            
Epoch [0119/0300], Training Loss: 0.0776, Validation Loss: 0.3560, Dropout p: 0.30, Patience: 0/50            
Epoch [0120/0300], Training Loss: 0.0679, Validation Loss: 0.3933, Dropout p: 0.30, Patience: 0/50            
Epoch [0121/0300], Training Loss: 0.0900, Validation Loss: 0.4248, Dropout p: 0.30, Patience: 0/50            
Epoch [0122/0300], Training Loss: 0.0656, Validation Loss: 0.3125, Dropout p: 0.30, Patience: 0/50            
Epoch [0123/0300], Training Loss: 0.0718, Validation Loss: 0.4080, Dropout p: 0.30, Patience: 0/50            
Epoch [0124/0300], Training Loss: 0.0511, Validation Loss: 0.4280, Dropout p: 0.30, Patience: 0/50            
Epoch [0125/0300], Training Loss: 0.0646, Validation Loss: 0.4128, Dropout p: 0.30, Patience: 0/50            
Epoch [0126/0300], Training Loss: 0.0671, Validation Loss: 0.4431, Dropout p: 0.30, Patience: 0/50            
Epoch [0127/0300], Training Loss: 0.0690, Validation Loss: 0.4512, Dropout p: 0.30, Patience: 1/50            
Epoch [0128/0300], Training Loss: 0.0742, Validation Loss: 0.4670, Dropout p: 0.30, Patience: 2/50            
Epoch [0129/0300], Training Loss: 0.0689, Validation Loss: 0.4399, Dropout p: 0.30, Patience: 0/50            
Epoch [0130/0300], Training Loss: 0.0484, Validation Loss: 0.4214, Dropout p: 0.30, Patience: 0/50            
Epoch [0131/0300], Training Loss: 0.0610, Validation Loss: 0.4282, Dropout p: 0.30, Patience: 1/50            
Epoch [0132/0300], Training Loss: 0.0668, Validation Loss: 0.4605, Dropout p: 0.30, Patience: 2/50            
Epoch [0133/0300], Training Loss: 0.0554, Validation Loss: 0.4407, Dropout p: 0.30, Patience: 3/50            
Epoch [0134/0300], Training Loss: 0.0605, Validation Loss: 0.5273, Dropout p: 0.30, Patience: 4/50            
Epoch [0135/0300], Training Loss: 0.0528, Validation Loss: 0.3976, Dropout p: 0.30, Patience: 0/50            
Epoch [0136/0300], Training Loss: 0.0691, Validation Loss: 0.4786, Dropout p: 0.30, Patience: 1/50            
Epoch [0137/0300], Training Loss: 0.0629, Validation Loss: 0.4511, Dropout p: 0.30, Patience: 2/50            
Epoch [0138/0300], Training Loss: 0.0608, Validation Loss: 0.4401, Dropout p: 0.30, Patience: 3/50            
Epoch [0139/0300], Training Loss: 0.0553, Validation Loss: 0.4328, Dropout p: 0.30, Patience: 4/50            
Epoch [0140/0300], Training Loss: 0.0494, Validation Loss: 0.4308, Dropout p: 0.30, Patience: 5/50            
Epoch [0141/0300], Training Loss: 0.0552, Validation Loss: 0.4224, Dropout p: 0.30, Patience: 6/50            
Epoch [0142/0300], Training Loss: 0.0517, Validation Loss: 0.4383, Dropout p: 0.30, Patience: 7/50            
Epoch [0143/0300], Training Loss: 0.0498, Validation Loss: 0.4494, Dropout p: 0.30, Patience: 8/50            
Epoch [0144/0300], Training Loss: 0.0450, Validation Loss: 0.4260, Dropout p: 0.30, Patience: 9/50            
Epoch [0145/0300], Training Loss: 0.0493, Validation Loss: 0.4259, Dropout p: 0.30, Patience: 10/50            
Epoch [0146/0300], Training Loss: 0.0517, Validation Loss: 0.4160, Dropout p: 0.30, Patience: 11/50            
Epoch [0147/0300], Training Loss: 0.0450, Validation Loss: 0.4068, Dropout p: 0.30, Patience: 12/50            
Epoch [0148/0300], Training Loss: 0.0474, Validation Loss: 0.3931, Dropout p: 0.30, Patience: 0/50            
Epoch [0149/0300], Training Loss: 0.0387, Validation Loss: 0.3525, Dropout p: 0.30, Patience: 0/50            
Epoch [0150/0300], Training Loss: 0.0474, Validation Loss: 0.4088, Dropout p: 0.30, Patience: 1/50            
Epoch [0151/0300], Training Loss: 0.0420, Validation Loss: 0.3595, Dropout p: 0.30, Patience: 2/50            
Epoch [0152/0300], Training Loss: 0.0617, Validation Loss: 0.3507, Dropout p: 0.30, Patience: 0/50            
Epoch [0153/0300], Training Loss: 0.0378, Validation Loss: 0.4122, Dropout p: 0.30, Patience: 1/50            
Epoch [0154/0300], Training Loss: 0.0445, Validation Loss: 0.4488, Dropout p: 0.30, Patience: 2/50            
Epoch [0155/0300], Training Loss: 0.0372, Validation Loss: 0.4662, Dropout p: 0.30, Patience: 3/50            
Epoch [0156/0300], Training Loss: 0.0411, Validation Loss: 0.4676, Dropout p: 0.30, Patience: 4/50            
Epoch [0157/0300], Training Loss: 0.0469, Validation Loss: 0.4373, Dropout p: 0.30, Patience: 5/50            
Epoch [0158/0300], Training Loss: 0.0397, Validation Loss: 0.4592, Dropout p: 0.30, Patience: 6/50            
Epoch [0159/0300], Training Loss: 0.0342, Validation Loss: 0.4418, Dropout p: 0.30, Patience: 7/50            
Epoch [0160/0300], Training Loss: 0.0510, Validation Loss: 0.4320, Dropout p: 0.30, Patience: 8/50            
Epoch [0161/0300], Training Loss: 0.0376, Validation Loss: 0.4214, Dropout p: 0.30, Patience: 9/50            
Epoch [0162/0300], Training Loss: 0.0353, Validation Loss: 0.4502, Dropout p: 0.30, Patience: 10/50            
Epoch [0163/0300], Training Loss: 0.0341, Validation Loss: 0.4306, Dropout p: 0.30, Patience: 11/50            
Epoch [0164/0300], Training Loss: 0.0425, Validation Loss: 0.4014, Dropout p: 0.30, Patience: 12/50            
Epoch [0165/0300], Training Loss: 0.0402, Validation Loss: 0.3714, Dropout p: 0.30, Patience: 13/50            
Epoch [0166/0300], Training Loss: 0.0522, Validation Loss: 0.4228, Dropout p: 0.30, Patience: 14/50            
Epoch [0167/0300], Training Loss: 0.0353, Validation Loss: 0.3986, Dropout p: 0.30, Patience: 15/50            
Epoch [0168/0300], Training Loss: 0.0345, Validation Loss: 0.3933, Dropout p: 0.30, Patience: 16/50            
Epoch [0169/0300], Training Loss: 0.0377, Validation Loss: 0.3785, Dropout p: 0.30, Patience: 17/50            
Epoch [0170/0300], Training Loss: 0.0400, Validation Loss: 0.3558, Dropout p: 0.30, Patience: 18/50            
Epoch [0171/0300], Training Loss: 0.0379, Validation Loss: 0.3770, Dropout p: 0.30, Patience: 19/50            
Epoch [0172/0300], Training Loss: 0.0323, Validation Loss: 0.3675, Dropout p: 0.30, Patience: 20/50            
Epoch [0173/0300], Training Loss: 0.0396, Validation Loss: 0.2990, Dropout p: 0.30, Patience: 0/50            
Epoch [0174/0300], Training Loss: 0.0353, Validation Loss: 0.3680, Dropout p: 0.30, Patience: 1/50            
Epoch [0175/0300], Training Loss: 0.0380, Validation Loss: 0.3793, Dropout p: 0.30, Patience: 2/50            
Epoch [0176/0300], Training Loss: 0.0365, Validation Loss: 0.3793, Dropout p: 0.30, Patience: 3/50            
Epoch [0177/0300], Training Loss: 0.0372, Validation Loss: 0.4040, Dropout p: 0.30, Patience: 4/50            
Epoch [0178/0300], Training Loss: 0.0459, Validation Loss: 0.4242, Dropout p: 0.30, Patience: 5/50            
Epoch [0179/0300], Training Loss: 0.0398, Validation Loss: 0.4072, Dropout p: 0.30, Patience: 6/50            
Epoch [0180/0300], Training Loss: 0.0394, Validation Loss: 0.3848, Dropout p: 0.30, Patience: 7/50            
Epoch [0181/0300], Training Loss: 0.0372, Validation Loss: 0.4179, Dropout p: 0.30, Patience: 8/50            
Epoch [0182/0300], Training Loss: 0.0381, Validation Loss: 0.4185, Dropout p: 0.30, Patience: 9/50            
Epoch [0183/0300], Training Loss: 0.0385, Validation Loss: 0.4007, Dropout p: 0.30, Patience: 10/50            
Epoch [0184/0300], Training Loss: 0.0401, Validation Loss: 0.4045, Dropout p: 0.30, Patience: 11/50            
Epoch [0185/0300], Training Loss: 0.0338, Validation Loss: 0.3969, Dropout p: 0.30, Patience: 12/50            
Epoch [0186/0300], Training Loss: 0.0349, Validation Loss: 0.3872, Dropout p: 0.30, Patience: 13/50            
Epoch [0187/0300], Training Loss: 0.0394, Validation Loss: 0.4002, Dropout p: 0.30, Patience: 14/50            
Epoch [0188/0300], Training Loss: 0.0297, Validation Loss: 0.3977, Dropout p: 0.30, Patience: 15/50            
Epoch [0189/0300], Training Loss: 0.0285, Validation Loss: 0.3891, Dropout p: 0.30, Patience: 16/50            
Epoch [0190/0300], Training Loss: 0.0344, Validation Loss: 0.3778, Dropout p: 0.30, Patience: 17/50            
Epoch [0191/0300], Training Loss: 0.0343, Validation Loss: 0.3908, Dropout p: 0.30, Patience: 18/50            
Epoch [0192/0300], Training Loss: 0.0382, Validation Loss: 0.4007, Dropout p: 0.30, Patience: 19/50            
Epoch [0193/0300], Training Loss: 0.0393, Validation Loss: 0.3938, Dropout p: 0.30, Patience: 20/50            
Epoch [0194/0300], Training Loss: 0.0367, Validation Loss: 0.4083, Dropout p: 0.30, Patience: 21/50            
Epoch [0195/0300], Training Loss: 0.0319, Validation Loss: 0.4007, Dropout p: 0.30, Patience: 22/50            
Epoch [0196/0300], Training Loss: 0.0328, Validation Loss: 0.3999, Dropout p: 0.30, Patience: 23/50            
Epoch [0197/0300], Training Loss: 0.0307, Validation Loss: 0.3930, Dropout p: 0.30, Patience: 24/50            
Epoch [0198/0300], Training Loss: 0.0378, Validation Loss: 0.3846, Dropout p: 0.30, Patience: 25/50            
Epoch [0199/0300], Training Loss: 0.0319, Validation Loss: 0.3767, Dropout p: 0.30, Patience: 26/50            
Epoch [0200/0300], Training Loss: 0.0318, Validation Loss: 0.3577, Dropout p: 0.30, Patience: 27/50            
Epoch [0201/0300], Training Loss: 0.0271, Validation Loss: 0.3675, Dropout p: 0.30, Patience: 28/50            
Epoch [0202/0300], Training Loss: 0.0333, Validation Loss: 0.3803, Dropout p: 0.30, Patience: 29/50            
Epoch [0203/0300], Training Loss: 0.0339, Validation Loss: 0.3621, Dropout p: 0.30, Patience: 30/50            
Epoch [0204/0300], Training Loss: 0.0336, Validation Loss: 0.3746, Dropout p: 0.30, Patience: 31/50            
Epoch [0205/0300], Training Loss: 0.0382, Validation Loss: 0.3791, Dropout p: 0.30, Patience: 32/50            
Epoch [0206/0300], Training Loss: 0.0371, Validation Loss: 0.3900, Dropout p: 0.30, Patience: 33/50            
Epoch [0207/0300], Training Loss: 0.0347, Validation Loss: 0.3636, Dropout p: 0.30, Patience: 34/50            
Epoch [0208/0300], Training Loss: 0.0330, Validation Loss: 0.3888, Dropout p: 0.30, Patience: 35/50            
Epoch [0209/0300], Training Loss: 0.0315, Validation Loss: 0.3959, Dropout p: 0.30, Patience: 36/50            
Epoch [0210/0300], Training Loss: 0.0298, Validation Loss: 0.3860, Dropout p: 0.30, Patience: 37/50            
Epoch [0211/0300], Training Loss: 0.0450, Validation Loss: 0.3914, Dropout p: 0.30, Patience: 38/50            
Epoch [0212/0300], Training Loss: 0.0286, Validation Loss: 0.3736, Dropout p: 0.30, Patience: 39/50            
Epoch [0213/0300], Training Loss: 0.0282, Validation Loss: 0.3786, Dropout p: 0.30, Patience: 40/50            
Epoch [0214/0300], Training Loss: 0.0306, Validation Loss: 0.3662, Dropout p: 0.30, Patience: 41/50            
Epoch [0215/0300], Training Loss: 0.0365, Validation Loss: 0.3773, Dropout p: 0.30, Patience: 42/50            
Epoch [0216/0300], Training Loss: 0.0297, Validation Loss: 0.3789, Dropout p: 0.30, Patience: 43/50            
Epoch [0217/0300], Training Loss: 0.0328, Validation Loss: 0.3793, Dropout p: 0.30, Patience: 44/50            
Epoch [0218/0300], Training Loss: 0.0360, Validation Loss: 0.3949, Dropout p: 0.30, Patience: 45/50            
Epoch [0219/0300], Training Loss: 0.0299, Validation Loss: 0.3862, Dropout p: 0.30, Patience: 46/50            
Epoch [0220/0300], Training Loss: 0.0267, Validation Loss: 0.3785, Dropout p: 0.30, Patience: 47/50            
Epoch [0221/0300], Training Loss: 0.0377, Validation Loss: 0.3838, Dropout p: 0.30, Patience: 48/50            
Epoch [0222/0300], Training Loss: 0.0279, Validation Loss: 0.3657, Dropout p: 0.30, Patience: 49/50            
Epoch [0223/0300], Training Loss: 0.0253, Validation Loss: 0.3774, Dropout p: 0.30, Patience: 50/50            

Evaluating model on test set...
Predicted TBV: 183.98	Actual TBV: 199.14	Difference: 15.16
Predicted TBV: 125.82	Actual TBV: 138.90	Difference: 13.08
Predicted TBV: 252.70	Actual TBV: 237.24	Difference: 15.46
Predicted TBV: 135.82	Actual TBV: 122.49	Difference: 13.33
Predicted TBV: 190.67	Actual TBV: 187.45	Difference: 3.22
Predicted TBV: 112.95	Actual TBV: 101.02	Difference: 11.93
Predicted TBV: 123.60	Actual TBV: 111.96	Difference: 11.64
Predicted TBV: 211.72	Actual TBV: 220.32	Difference: 8.60
Predicted TBV: 84.80	Actual TBV: 144.29	Difference: 59.49
Predicted TBV: 210.61	Actual TBV: 199.69	Difference: 10.92
Predicted TBV: 264.71	Actual TBV: 246.98	Difference: 17.73
Predicted TBV: 264.52	Actual TBV: 266.85	Difference: 2.33
Predicted TBV: 213.96	Actual TBV: 190.90	Difference: 23.06
Predicted TBV: 203.05	Actual TBV: 197.25	Difference: 5.80
Predicted TBV: 262.46	Actual TBV: 236.37	Difference: 26.09
Predicted TBV: 218.56	Actual TBV: 224.11	Difference: 5.55
Predicted TBV: 170.62	Actual TBV: 185.05	Difference: 14.43
Predicted TBV: 216.17	Actual TBV: 204.55	Difference: 11.62
Predicted TBV: 173.80	Actual TBV: 159.53	Difference: 14.27
Predicted TBV: 215.86	Actual TBV: 198.08	Difference: 17.78
Predicted TBV: 229.61	Actual TBV: 211.79	Difference: 17.82
Predicted TBV: 212.59	Actual TBV: 210.79	Difference: 1.80
Predicted TBV: 213.39	Actual TBV: 188.56	Difference: 24.83
Predicted TBV: 226.07	Actual TBV: 228.15	Difference: 2.08
Predicted TBV: 198.89	Actual TBV: 171.22	Difference: 27.67
Predicted TBV: 297.68	Actual TBV: 282.68	Difference: 15.00
Predicted TBV: 196.70	Actual TBV: 190.81	Difference: 5.89
Predicted TBV: 254.02	Actual TBV: 227.74	Difference: 26.28
Predicted TBV: 217.05	Actual TBV: 222.40	Difference: 5.35

Evaluating model with 30 Bayesian runs...
Refused raster with error 15.32. TBV: 59.49.
Refused raster with error 9.53. TBV: 24.83.
Refused raster with error 11.51. TBV: 15.00.
Refused raster with error 10.47. TBV: 26.28.

- - - - - -
Total Raster Count: 29
Refused Raster Count: 4
- - - - - -

- - - - - -
Non-bayesian prediction:
Mean Absolute Error: 14.77 cc
Standard Deviation: 11.24 cc
Big error count (>30): 1
Big error mean: 59.49 cc
Big error std: 0.00 cc
- - - - - -

- - - - - -
Bayesian prediction:
Mean Absolute Error: 12.11 cc
Standard Deviation: 7.07 cc
Big error count (>30): 0
- - - - - -

Best fold: 5 with average TBV error: 11.85 cc
Validated on: ['46' '45' '50' '18' '32' '21' '85' '3']
