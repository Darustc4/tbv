 - - - Fold 1/6 - - -

Train volume count: 294
Validation volume count: 76
Validation volumes: ['24' '28' '15' '78' '82' '25' '40' '31' '29' '7' '41' '33' '76' '30' '87'
 '77' '79' '43' '47' '35' '16' '44' '26' '81' '46' '45' '50' '18' '32'
 '21' '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.1942, Validation Loss: 0.5188, Dropout p: 0.30, Patience: 0/75            
Epoch [0002/0300], Training Loss: 1.1827, Validation Loss: 0.3808, Dropout p: 0.30, Patience: 0/75            
Epoch [0003/0300], Training Loss: 1.0259, Validation Loss: 0.5586, Dropout p: 0.30, Patience: 0/75            
Epoch [0004/0300], Training Loss: 0.9845, Validation Loss: 0.7081, Dropout p: 0.30, Patience: 0/75            
Epoch [0005/0300], Training Loss: 0.8701, Validation Loss: 0.5650, Dropout p: 0.30, Patience: 0/75            
Epoch [0006/0300], Training Loss: 0.8183, Validation Loss: 4.0208, Dropout p: 0.30, Patience: 0/75            
Epoch [0007/0300], Training Loss: 0.7645, Validation Loss: 3.1340, Dropout p: 0.30, Patience: 0/75            
Epoch [0008/0300], Training Loss: 0.6777, Validation Loss: 5.1393, Dropout p: 0.30, Patience: 0/75            
Epoch [0009/0300], Training Loss: 0.5451, Validation Loss: 8.5427, Dropout p: 0.30, Patience: 0/75            
Epoch [0010/0300], Training Loss: 0.6592, Validation Loss: 9.5212, Dropout p: 0.30, Patience: 0/75            
Epoch [0011/0300], Training Loss: 0.6331, Validation Loss: 8.0895, Dropout p: 0.30, Patience: 0/75            
Epoch [0012/0300], Training Loss: 0.6394, Validation Loss: 7.7263, Dropout p: 0.30, Patience: 0/75            
Epoch [0013/0300], Training Loss: 0.4896, Validation Loss: 13.2373, Dropout p: 0.30, Patience: 0/75            
Epoch [0014/0300], Training Loss: 0.4501, Validation Loss: 3.4877, Dropout p: 0.30, Patience: 0/75            
Epoch [0015/0300], Training Loss: 0.5389, Validation Loss: 1.6616, Dropout p: 0.30, Patience: 0/75            
Epoch [0016/0300], Training Loss: 0.5075, Validation Loss: 1.8432, Dropout p: 0.30, Patience: 0/75            
Epoch [0017/0300], Training Loss: 0.4700, Validation Loss: 3.0277, Dropout p: 0.30, Patience: 0/75            
Epoch [0018/0300], Training Loss: 0.3885, Validation Loss: 3.3710, Dropout p: 0.30, Patience: 0/75            
Epoch [0019/0300], Training Loss: 0.4465, Validation Loss: 4.8630, Dropout p: 0.30, Patience: 0/75            
Epoch [0020/0300], Training Loss: 0.3976, Validation Loss: 3.0529, Dropout p: 0.30, Patience: 0/75            
Epoch [0021/0300], Training Loss: 0.4241, Validation Loss: 1.1363, Dropout p: 0.30, Patience: 0/75            
Epoch [0022/0300], Training Loss: 0.3676, Validation Loss: 1.1981, Dropout p: 0.30, Patience: 0/75            
Epoch [0023/0300], Training Loss: 0.3555, Validation Loss: 2.8512, Dropout p: 0.30, Patience: 0/75            
Epoch [0024/0300], Training Loss: 0.3732, Validation Loss: 1.8843, Dropout p: 0.30, Patience: 0/75            
Epoch [0025/0300], Training Loss: 0.3393, Validation Loss: 0.9295, Dropout p: 0.30, Patience: 0/75            
Epoch [0026/0300], Training Loss: 0.2672, Validation Loss: 1.5877, Dropout p: 0.30, Patience: 0/75            
Epoch [0027/0300], Training Loss: 0.3626, Validation Loss: 1.7366, Dropout p: 0.30, Patience: 0/75            
Epoch [0028/0300], Training Loss: 0.2968, Validation Loss: 0.8704, Dropout p: 0.30, Patience: 0/75            
Epoch [0029/0300], Training Loss: 0.3662, Validation Loss: 0.7714, Dropout p: 0.30, Patience: 0/75            
Epoch [0030/0300], Training Loss: 0.4020, Validation Loss: 0.8765, Dropout p: 0.30, Patience: 0/75            
Epoch [0031/0300], Training Loss: 0.2838, Validation Loss: 0.5758, Dropout p: 0.30, Patience: 0/75            
Epoch [0032/0300], Training Loss: 0.3075, Validation Loss: 0.5042, Dropout p: 0.30, Patience: 0/75            
Epoch [0033/0300], Training Loss: 0.3231, Validation Loss: 0.8420, Dropout p: 0.30, Patience: 0/75            
Epoch [0034/0300], Training Loss: 0.3240, Validation Loss: 0.9608, Dropout p: 0.30, Patience: 0/75            
Epoch [0035/0300], Training Loss: 0.3230, Validation Loss: 0.4539, Dropout p: 0.30, Patience: 0/75            
Epoch [0036/0300], Training Loss: 0.2859, Validation Loss: 0.7341, Dropout p: 0.30, Patience: 0/75            
Epoch [0037/0300], Training Loss: 0.3459, Validation Loss: 0.4788, Dropout p: 0.30, Patience: 0/75            
Epoch [0038/0300], Training Loss: 0.2810, Validation Loss: 0.4594, Dropout p: 0.30, Patience: 0/75            
Epoch [0039/0300], Training Loss: 0.2793, Validation Loss: 0.4771, Dropout p: 0.30, Patience: 0/75            
Epoch [0040/0300], Training Loss: 0.2775, Validation Loss: 0.5067, Dropout p: 0.30, Patience: 0/75            
Epoch [0041/0300], Training Loss: 0.2584, Validation Loss: 0.4532, Dropout p: 0.30, Patience: 0/75            
Epoch [0042/0300], Training Loss: 0.2219, Validation Loss: 0.3608, Dropout p: 0.30, Patience: 0/75            
Epoch [0043/0300], Training Loss: 0.2718, Validation Loss: 0.4582, Dropout p: 0.30, Patience: 0/75            
Epoch [0044/0300], Training Loss: 0.2878, Validation Loss: 0.2909, Dropout p: 0.30, Patience: 0/75            
Epoch [0045/0300], Training Loss: 0.2765, Validation Loss: 0.2750, Dropout p: 0.30, Patience: 0/75            
Epoch [0046/0300], Training Loss: 0.2221, Validation Loss: 0.4685, Dropout p: 0.30, Patience: 0/75            
Epoch [0047/0300], Training Loss: 0.1924, Validation Loss: 0.4571, Dropout p: 0.30, Patience: 0/75            
Epoch [0048/0300], Training Loss: 0.2078, Validation Loss: 0.3498, Dropout p: 0.30, Patience: 0/75            
Epoch [0049/0300], Training Loss: 0.2019, Validation Loss: 0.3270, Dropout p: 0.30, Patience: 0/75            
Epoch [0050/0300], Training Loss: 0.2159, Validation Loss: 0.2454, Dropout p: 0.30, Patience: 0/75            
Epoch [0051/0300], Training Loss: 0.2310, Validation Loss: 0.3417, Dropout p: 0.30, Patience: 0/75            
Epoch [0052/0300], Training Loss: 0.2301, Validation Loss: 0.2507, Dropout p: 0.30, Patience: 0/75            
Epoch [0053/0300], Training Loss: 0.2191, Validation Loss: 0.3212, Dropout p: 0.30, Patience: 0/75            
Epoch [0054/0300], Training Loss: 0.2188, Validation Loss: 0.3760, Dropout p: 0.30, Patience: 0/75            
Epoch [0055/0300], Training Loss: 0.2750, Validation Loss: 0.2408, Dropout p: 0.30, Patience: 0/75            
Epoch [0056/0300], Training Loss: 0.1790, Validation Loss: 0.2671, Dropout p: 0.30, Patience: 0/75            
Epoch [0057/0300], Training Loss: 0.2006, Validation Loss: 0.3287, Dropout p: 0.30, Patience: 0/75            
Epoch [0058/0300], Training Loss: 0.1746, Validation Loss: 0.2614, Dropout p: 0.30, Patience: 0/75            
Epoch [0059/0300], Training Loss: 0.1933, Validation Loss: 0.2817, Dropout p: 0.30, Patience: 0/75            
Epoch [0060/0300], Training Loss: 0.2159, Validation Loss: 0.2395, Dropout p: 0.30, Patience: 0/75            
Epoch [0061/0300], Training Loss: 0.2088, Validation Loss: 0.2345, Dropout p: 0.30, Patience: 0/75            
Epoch [0062/0300], Training Loss: 0.2086, Validation Loss: 0.2594, Dropout p: 0.30, Patience: 0/75            
Epoch [0063/0300], Training Loss: 0.1784, Validation Loss: 0.1605, Dropout p: 0.30, Patience: 0/75            
Epoch [0064/0300], Training Loss: 0.2518, Validation Loss: 0.1646, Dropout p: 0.30, Patience: 0/75            
Epoch [0065/0300], Training Loss: 0.2018, Validation Loss: 0.2658, Dropout p: 0.30, Patience: 0/75            
Epoch [0066/0300], Training Loss: 0.2176, Validation Loss: 0.1848, Dropout p: 0.30, Patience: 0/75            
Epoch [0067/0300], Training Loss: 0.1625, Validation Loss: 0.2509, Dropout p: 0.30, Patience: 0/75            
Epoch [0068/0300], Training Loss: 0.1977, Validation Loss: 0.2040, Dropout p: 0.30, Patience: 0/75            
Epoch [0069/0300], Training Loss: 0.2409, Validation Loss: 0.2223, Dropout p: 0.30, Patience: 0/75            
Epoch [0070/0300], Training Loss: 0.1881, Validation Loss: 0.2255, Dropout p: 0.30, Patience: 0/75            
Epoch [0071/0300], Training Loss: 0.1704, Validation Loss: 0.2024, Dropout p: 0.30, Patience: 0/75            
Epoch [0072/0300], Training Loss: 0.1709, Validation Loss: 0.2061, Dropout p: 0.30, Patience: 0/75            
Epoch [0073/0300], Training Loss: 0.1897, Validation Loss: 0.2014, Dropout p: 0.30, Patience: 0/75            
Epoch [0074/0300], Training Loss: 0.1891, Validation Loss: 0.2132, Dropout p: 0.30, Patience: 0/75            
Epoch [0075/0300], Training Loss: 0.1946, Validation Loss: 0.1835, Dropout p: 0.30, Patience: 0/75            
Epoch [0076/0300], Training Loss: 0.2373, Validation Loss: 0.2146, Dropout p: 0.30, Patience: 0/75            
Epoch [0077/0300], Training Loss: 0.1362, Validation Loss: 0.1956, Dropout p: 0.30, Patience: 0/75            
Epoch [0078/0300], Training Loss: 0.1672, Validation Loss: 0.2273, Dropout p: 0.30, Patience: 0/75            
Epoch [0079/0300], Training Loss: 0.1510, Validation Loss: 0.2014, Dropout p: 0.30, Patience: 0/75            
Epoch [0080/0300], Training Loss: 0.2228, Validation Loss: 0.1804, Dropout p: 0.30, Patience: 0/75            
Epoch [0081/0300], Training Loss: 0.1371, Validation Loss: 0.1520, Dropout p: 0.30, Patience: 0/75            
Epoch [0082/0300], Training Loss: 0.1569, Validation Loss: 0.1838, Dropout p: 0.30, Patience: 0/75            
Epoch [0083/0300], Training Loss: 0.1703, Validation Loss: 0.1860, Dropout p: 0.30, Patience: 0/75            
Epoch [0084/0300], Training Loss: 0.2096, Validation Loss: 0.1901, Dropout p: 0.30, Patience: 0/75            
Epoch [0085/0300], Training Loss: 0.1521, Validation Loss: 0.1688, Dropout p: 0.30, Patience: 0/75            
Epoch [0086/0300], Training Loss: 0.2197, Validation Loss: 0.1323, Dropout p: 0.30, Patience: 0/75            
Epoch [0087/0300], Training Loss: 0.1700, Validation Loss: 0.1857, Dropout p: 0.30, Patience: 0/75            
Epoch [0088/0300], Training Loss: 0.1434, Validation Loss: 0.1557, Dropout p: 0.30, Patience: 0/75            
Epoch [0089/0300], Training Loss: 0.1425, Validation Loss: 0.2148, Dropout p: 0.30, Patience: 0/75            
Epoch [0090/0300], Training Loss: 0.1501, Validation Loss: 0.1955, Dropout p: 0.30, Patience: 0/75            
Epoch [0091/0300], Training Loss: 0.1438, Validation Loss: 0.1625, Dropout p: 0.30, Patience: 0/75            
Epoch [0092/0300], Training Loss: 0.1407, Validation Loss: 0.1689, Dropout p: 0.30, Patience: 0/75            
Epoch [0093/0300], Training Loss: 0.1173, Validation Loss: 0.1606, Dropout p: 0.30, Patience: 0/75            
Epoch [0094/0300], Training Loss: 0.1254, Validation Loss: 0.1629, Dropout p: 0.30, Patience: 0/75            
Epoch [0095/0300], Training Loss: 0.1512, Validation Loss: 0.1638, Dropout p: 0.30, Patience: 0/75            
Epoch [0096/0300], Training Loss: 0.1491, Validation Loss: 0.1643, Dropout p: 0.30, Patience: 0/75            
Epoch [0097/0300], Training Loss: 0.1638, Validation Loss: 0.1690, Dropout p: 0.30, Patience: 0/75            
Epoch [0098/0300], Training Loss: 0.1506, Validation Loss: 0.1891, Dropout p: 0.30, Patience: 0/75            
Epoch [0099/0300], Training Loss: 0.1628, Validation Loss: 0.1779, Dropout p: 0.30, Patience: 0/75            
Epoch [0100/0300], Training Loss: 0.1380, Validation Loss: 0.1688, Dropout p: 0.30, Patience: 0/75            
Epoch [0101/0300], Training Loss: 0.1537, Validation Loss: 0.1779, Dropout p: 0.30, Patience: 0/75            
Epoch [0102/0300], Training Loss: 0.1281, Validation Loss: 0.1757, Dropout p: 0.30, Patience: 0/75            
Epoch [0103/0300], Training Loss: 0.1478, Validation Loss: 0.1476, Dropout p: 0.30, Patience: 0/75            
Epoch [0104/0300], Training Loss: 0.1303, Validation Loss: 0.1559, Dropout p: 0.30, Patience: 1/75            
Epoch [0105/0300], Training Loss: 0.1049, Validation Loss: 0.1772, Dropout p: 0.30, Patience: 2/75            
Epoch [0106/0300], Training Loss: 0.1274, Validation Loss: 0.1647, Dropout p: 0.30, Patience: 3/75            
Epoch [0107/0300], Training Loss: 0.1379, Validation Loss: 0.1745, Dropout p: 0.30, Patience: 4/75            
Epoch [0108/0300], Training Loss: 0.1344, Validation Loss: 0.1624, Dropout p: 0.30, Patience: 5/75            
Epoch [0109/0300], Training Loss: 0.1267, Validation Loss: 0.1517, Dropout p: 0.30, Patience: 6/75            
Epoch [0110/0300], Training Loss: 0.1330, Validation Loss: 0.1725, Dropout p: 0.30, Patience: 7/75            
Epoch [0111/0300], Training Loss: 0.1242, Validation Loss: 0.1571, Dropout p: 0.30, Patience: 8/75            
Epoch [0112/0300], Training Loss: 0.1211, Validation Loss: 0.1675, Dropout p: 0.30, Patience: 9/75            
Epoch [0113/0300], Training Loss: 0.1116, Validation Loss: 0.1501, Dropout p: 0.30, Patience: 10/75            
Epoch [0114/0300], Training Loss: 0.1445, Validation Loss: 0.1506, Dropout p: 0.30, Patience: 11/75            
Epoch [0115/0300], Training Loss: 0.1485, Validation Loss: 0.1471, Dropout p: 0.30, Patience: 0/75            
Epoch [0116/0300], Training Loss: 0.1347, Validation Loss: 0.1562, Dropout p: 0.30, Patience: 1/75            
Epoch [0117/0300], Training Loss: 0.1188, Validation Loss: 0.1555, Dropout p: 0.30, Patience: 2/75            
Epoch [0118/0300], Training Loss: 0.1167, Validation Loss: 0.1801, Dropout p: 0.30, Patience: 3/75            
Epoch [0119/0300], Training Loss: 0.1021, Validation Loss: 0.1642, Dropout p: 0.30, Patience: 4/75            
Epoch [0120/0300], Training Loss: 0.1198, Validation Loss: 0.1462, Dropout p: 0.30, Patience: 0/75            
Epoch [0121/0300], Training Loss: 0.1249, Validation Loss: 0.1640, Dropout p: 0.30, Patience: 1/75            
Epoch [0122/0300], Training Loss: 0.1125, Validation Loss: 0.1595, Dropout p: 0.30, Patience: 2/75            
Epoch [0123/0300], Training Loss: 0.0956, Validation Loss: 0.1552, Dropout p: 0.30, Patience: 3/75            
Epoch [0124/0300], Training Loss: 0.1049, Validation Loss: 0.1430, Dropout p: 0.30, Patience: 0/75            
Epoch [0125/0300], Training Loss: 0.1112, Validation Loss: 0.1513, Dropout p: 0.30, Patience: 1/75            
Epoch [0126/0300], Training Loss: 0.1511, Validation Loss: 0.1569, Dropout p: 0.30, Patience: 2/75            
Epoch [0127/0300], Training Loss: 0.1242, Validation Loss: 0.1473, Dropout p: 0.30, Patience: 3/75            
Epoch [0128/0300], Training Loss: 0.1407, Validation Loss: 0.1547, Dropout p: 0.30, Patience: 4/75            
Epoch [0129/0300], Training Loss: 0.1251, Validation Loss: 0.1412, Dropout p: 0.30, Patience: 0/75            
Epoch [0130/0300], Training Loss: 0.1232, Validation Loss: 0.1396, Dropout p: 0.30, Patience: 0/75            
Epoch [0131/0300], Training Loss: 0.1533, Validation Loss: 0.1632, Dropout p: 0.30, Patience: 1/75            
Epoch [0132/0300], Training Loss: 0.1087, Validation Loss: 0.1522, Dropout p: 0.30, Patience: 2/75            
Epoch [0133/0300], Training Loss: 0.1243, Validation Loss: 0.1504, Dropout p: 0.30, Patience: 3/75            
Epoch [0134/0300], Training Loss: 0.1244, Validation Loss: 0.1475, Dropout p: 0.30, Patience: 4/75            
Epoch [0135/0300], Training Loss: 0.1249, Validation Loss: 0.1568, Dropout p: 0.30, Patience: 5/75            
Epoch [0136/0300], Training Loss: 0.1209, Validation Loss: 0.1569, Dropout p: 0.30, Patience: 6/75            
Epoch [0137/0300], Training Loss: 0.1561, Validation Loss: 0.1688, Dropout p: 0.30, Patience: 7/75            
Epoch [0138/0300], Training Loss: 0.1353, Validation Loss: 0.1814, Dropout p: 0.30, Patience: 8/75            
Epoch [0139/0300], Training Loss: 0.1246, Validation Loss: 0.1624, Dropout p: 0.30, Patience: 9/75            
Epoch [0140/0300], Training Loss: 0.1246, Validation Loss: 0.1595, Dropout p: 0.30, Patience: 10/75            
Epoch [0141/0300], Training Loss: 0.1301, Validation Loss: 0.1559, Dropout p: 0.30, Patience: 11/75            
Epoch [0142/0300], Training Loss: 0.1381, Validation Loss: 0.1615, Dropout p: 0.30, Patience: 12/75            
Epoch [0143/0300], Training Loss: 0.1131, Validation Loss: 0.1478, Dropout p: 0.30, Patience: 13/75            
Epoch [0144/0300], Training Loss: 0.1122, Validation Loss: 0.1584, Dropout p: 0.30, Patience: 14/75            
Epoch [0145/0300], Training Loss: 0.1497, Validation Loss: 0.1533, Dropout p: 0.30, Patience: 15/75            
Epoch [0146/0300], Training Loss: 0.1453, Validation Loss: 0.1505, Dropout p: 0.30, Patience: 16/75            
Epoch [0147/0300], Training Loss: 0.1021, Validation Loss: 0.1513, Dropout p: 0.30, Patience: 17/75            
Epoch [0148/0300], Training Loss: 0.1299, Validation Loss: 0.1614, Dropout p: 0.30, Patience: 18/75            
Epoch [0149/0300], Training Loss: 0.1382, Validation Loss: 0.1437, Dropout p: 0.30, Patience: 19/75            
Epoch [0150/0300], Training Loss: 0.1061, Validation Loss: 0.1497, Dropout p: 0.30, Patience: 20/75            
Epoch [0151/0300], Training Loss: 0.1124, Validation Loss: 0.1384, Dropout p: 0.30, Patience: 0/75            
Epoch [0152/0300], Training Loss: 0.1357, Validation Loss: 0.1658, Dropout p: 0.30, Patience: 1/75            
Epoch [0153/0300], Training Loss: 0.1385, Validation Loss: 0.1556, Dropout p: 0.30, Patience: 2/75            
Epoch [0154/0300], Training Loss: 0.1025, Validation Loss: 0.1588, Dropout p: 0.30, Patience: 3/75            
Epoch [0155/0300], Training Loss: 0.1556, Validation Loss: 0.1556, Dropout p: 0.30, Patience: 4/75            
Epoch [0156/0300], Training Loss: 0.1274, Validation Loss: 0.1573, Dropout p: 0.30, Patience: 5/75            
Epoch [0157/0300], Training Loss: 0.1172, Validation Loss: 0.1498, Dropout p: 0.30, Patience: 6/75            
Epoch [0158/0300], Training Loss: 0.1439, Validation Loss: 0.1501, Dropout p: 0.30, Patience: 7/75            
Epoch [0159/0300], Training Loss: 0.1483, Validation Loss: 0.1679, Dropout p: 0.30, Patience: 8/75            
Epoch [0160/0300], Training Loss: 0.1125, Validation Loss: 0.1496, Dropout p: 0.30, Patience: 9/75            
Epoch [0161/0300], Training Loss: 0.1175, Validation Loss: 0.1584, Dropout p: 0.30, Patience: 10/75            
Epoch [0162/0300], Training Loss: 0.1067, Validation Loss: 0.1570, Dropout p: 0.30, Patience: 11/75            
Epoch [0163/0300], Training Loss: 0.1313, Validation Loss: 0.1595, Dropout p: 0.30, Patience: 12/75            
Epoch [0164/0300], Training Loss: 0.1148, Validation Loss: 0.1442, Dropout p: 0.30, Patience: 13/75            
Epoch [0165/0300], Training Loss: 0.1254, Validation Loss: 0.1430, Dropout p: 0.30, Patience: 14/75            
Epoch [0166/0300], Training Loss: 0.1131, Validation Loss: 0.1538, Dropout p: 0.30, Patience: 15/75            
Epoch [0167/0300], Training Loss: 0.1084, Validation Loss: 0.1384, Dropout p: 0.30, Patience: 16/75            
Epoch [0168/0300], Training Loss: 0.1329, Validation Loss: 0.1529, Dropout p: 0.30, Patience: 17/75            
Epoch [0169/0300], Training Loss: 0.1236, Validation Loss: 0.1640, Dropout p: 0.30, Patience: 18/75            
Epoch [0170/0300], Training Loss: 0.1392, Validation Loss: 0.1461, Dropout p: 0.30, Patience: 19/75            
Epoch [0171/0300], Training Loss: 0.1376, Validation Loss: 0.1621, Dropout p: 0.30, Patience: 20/75            
Epoch [0172/0300], Training Loss: 0.0900, Validation Loss: 0.1566, Dropout p: 0.30, Patience: 21/75            
Epoch [0173/0300], Training Loss: 0.1267, Validation Loss: 0.1468, Dropout p: 0.30, Patience: 22/75            
Epoch [0174/0300], Training Loss: 0.1401, Validation Loss: 0.1608, Dropout p: 0.30, Patience: 23/75            
Epoch [0175/0300], Training Loss: 0.0902, Validation Loss: 0.1640, Dropout p: 0.30, Patience: 24/75            
Epoch [0176/0300], Training Loss: 0.1038, Validation Loss: 0.1357, Dropout p: 0.30, Patience: 0/75            
Epoch [0177/0300], Training Loss: 0.1400, Validation Loss: 0.1427, Dropout p: 0.30, Patience: 1/75            
Epoch [0178/0300], Training Loss: 0.1200, Validation Loss: 0.1486, Dropout p: 0.30, Patience: 2/75            
Epoch [0179/0300], Training Loss: 0.1233, Validation Loss: 0.1499, Dropout p: 0.30, Patience: 3/75            
Epoch [0180/0300], Training Loss: 0.1336, Validation Loss: 0.1624, Dropout p: 0.30, Patience: 4/75            
Epoch [0181/0300], Training Loss: 0.1068, Validation Loss: 0.1499, Dropout p: 0.30, Patience: 5/75            
Epoch [0182/0300], Training Loss: 0.1111, Validation Loss: 0.1579, Dropout p: 0.30, Patience: 6/75            
Epoch [0183/0300], Training Loss: 0.1015, Validation Loss: 0.1539, Dropout p: 0.30, Patience: 7/75            
Epoch [0184/0300], Training Loss: 0.1084, Validation Loss: 0.1614, Dropout p: 0.30, Patience: 8/75            
Epoch [0185/0300], Training Loss: 0.1267, Validation Loss: 0.1559, Dropout p: 0.30, Patience: 9/75            
Epoch [0186/0300], Training Loss: 0.1130, Validation Loss: 0.1465, Dropout p: 0.30, Patience: 10/75            
Epoch [0187/0300], Training Loss: 0.1306, Validation Loss: 0.1597, Dropout p: 0.30, Patience: 11/75            
Epoch [0188/0300], Training Loss: 0.1118, Validation Loss: 0.1471, Dropout p: 0.30, Patience: 12/75            
Epoch [0189/0300], Training Loss: 0.1341, Validation Loss: 0.1480, Dropout p: 0.30, Patience: 13/75            
Epoch [0190/0300], Training Loss: 0.1473, Validation Loss: 0.1525, Dropout p: 0.30, Patience: 14/75            
Epoch [0191/0300], Training Loss: 0.1415, Validation Loss: 0.1461, Dropout p: 0.30, Patience: 15/75            
Epoch [0192/0300], Training Loss: 0.1136, Validation Loss: 0.1477, Dropout p: 0.30, Patience: 16/75            
Epoch [0193/0300], Training Loss: 0.1430, Validation Loss: 0.1603, Dropout p: 0.30, Patience: 17/75            
Epoch [0194/0300], Training Loss: 0.1152, Validation Loss: 0.1594, Dropout p: 0.30, Patience: 18/75            
Epoch [0195/0300], Training Loss: 0.1117, Validation Loss: 0.1510, Dropout p: 0.30, Patience: 19/75            
Epoch [0196/0300], Training Loss: 0.1319, Validation Loss: 0.1424, Dropout p: 0.30, Patience: 20/75            
Epoch [0197/0300], Training Loss: 0.1135, Validation Loss: 0.1476, Dropout p: 0.30, Patience: 21/75            
Epoch [0198/0300], Training Loss: 0.0949, Validation Loss: 0.1574, Dropout p: 0.30, Patience: 22/75            
Epoch [0199/0300], Training Loss: 0.1174, Validation Loss: 0.1454, Dropout p: 0.30, Patience: 23/75            
Epoch [0200/0300], Training Loss: 0.1085, Validation Loss: 0.1512, Dropout p: 0.30, Patience: 24/75            
Epoch [0201/0300], Training Loss: 0.1368, Validation Loss: 0.1549, Dropout p: 0.30, Patience: 25/75            
Epoch [0202/0300], Training Loss: 0.1102, Validation Loss: 0.1647, Dropout p: 0.30, Patience: 26/75            
Epoch [0203/0300], Training Loss: 0.1168, Validation Loss: 0.1539, Dropout p: 0.30, Patience: 27/75            
Epoch [0204/0300], Training Loss: 0.1172, Validation Loss: 0.1517, Dropout p: 0.30, Patience: 28/75            
Epoch [0205/0300], Training Loss: 0.1400, Validation Loss: 0.1539, Dropout p: 0.30, Patience: 29/75            
Epoch [0206/0300], Training Loss: 0.1100, Validation Loss: 0.1668, Dropout p: 0.30, Patience: 30/75            
Epoch [0207/0300], Training Loss: 0.1275, Validation Loss: 0.1471, Dropout p: 0.30, Patience: 31/75            
Epoch [0208/0300], Training Loss: 0.1277, Validation Loss: 0.1542, Dropout p: 0.30, Patience: 32/75            
Epoch [0209/0300], Training Loss: 0.1612, Validation Loss: 0.1480, Dropout p: 0.30, Patience: 33/75            
Epoch [0210/0300], Training Loss: 0.1228, Validation Loss: 0.1433, Dropout p: 0.30, Patience: 34/75            
Epoch [0211/0300], Training Loss: 0.1233, Validation Loss: 0.1471, Dropout p: 0.30, Patience: 35/75            
Epoch [0212/0300], Training Loss: 0.1570, Validation Loss: 0.1514, Dropout p: 0.30, Patience: 36/75            
Epoch [0213/0300], Training Loss: 0.0884, Validation Loss: 0.1532, Dropout p: 0.30, Patience: 37/75            
Epoch [0214/0300], Training Loss: 0.1345, Validation Loss: 0.1428, Dropout p: 0.30, Patience: 38/75            
Epoch [0215/0300], Training Loss: 0.1156, Validation Loss: 0.1628, Dropout p: 0.30, Patience: 39/75            
Epoch [0216/0300], Training Loss: 0.1209, Validation Loss: 0.1642, Dropout p: 0.30, Patience: 40/75            
Epoch [0217/0300], Training Loss: 0.1373, Validation Loss: 0.1540, Dropout p: 0.30, Patience: 41/75            
Epoch [0218/0300], Training Loss: 0.1298, Validation Loss: 0.1500, Dropout p: 0.30, Patience: 42/75            
Epoch [0219/0300], Training Loss: 0.1066, Validation Loss: 0.1548, Dropout p: 0.30, Patience: 43/75            
Epoch [0220/0300], Training Loss: 0.1296, Validation Loss: 0.1505, Dropout p: 0.30, Patience: 44/75            
Epoch [0221/0300], Training Loss: 0.1044, Validation Loss: 0.1572, Dropout p: 0.30, Patience: 45/75            
Epoch [0222/0300], Training Loss: 0.1213, Validation Loss: 0.1684, Dropout p: 0.30, Patience: 46/75            
Epoch [0223/0300], Training Loss: 0.1128, Validation Loss: 0.1732, Dropout p: 0.30, Patience: 47/75            
Epoch [0224/0300], Training Loss: 0.1362, Validation Loss: 0.1643, Dropout p: 0.30, Patience: 48/75            
Epoch [0225/0300], Training Loss: 0.1110, Validation Loss: 0.1701, Dropout p: 0.30, Patience: 49/75            
Epoch [0226/0300], Training Loss: 0.1157, Validation Loss: 0.1600, Dropout p: 0.30, Patience: 50/75            
Epoch [0227/0300], Training Loss: 0.0989, Validation Loss: 0.1526, Dropout p: 0.30, Patience: 51/75            
Epoch [0228/0300], Training Loss: 0.1277, Validation Loss: 0.1458, Dropout p: 0.30, Patience: 52/75            
Epoch [0229/0300], Training Loss: 0.1142, Validation Loss: 0.1514, Dropout p: 0.30, Patience: 53/75            
Epoch [0230/0300], Training Loss: 0.1417, Validation Loss: 0.1627, Dropout p: 0.30, Patience: 54/75            
Epoch [0231/0300], Training Loss: 0.1530, Validation Loss: 0.1573, Dropout p: 0.30, Patience: 55/75            
Epoch [0232/0300], Training Loss: 0.1202, Validation Loss: 0.1551, Dropout p: 0.30, Patience: 56/75            
Epoch [0233/0300], Training Loss: 0.1138, Validation Loss: 0.1504, Dropout p: 0.30, Patience: 57/75            
Epoch [0234/0300], Training Loss: 0.1285, Validation Loss: 0.1483, Dropout p: 0.30, Patience: 58/75            
Epoch [0235/0300], Training Loss: 0.1055, Validation Loss: 0.1503, Dropout p: 0.30, Patience: 59/75            
Epoch [0236/0300], Training Loss: 0.0937, Validation Loss: 0.1562, Dropout p: 0.30, Patience: 60/75            
Epoch [0237/0300], Training Loss: 0.1174, Validation Loss: 0.1446, Dropout p: 0.30, Patience: 61/75            
Epoch [0238/0300], Training Loss: 0.1184, Validation Loss: 0.1416, Dropout p: 0.30, Patience: 62/75            
Epoch [0239/0300], Training Loss: 0.0947, Validation Loss: 0.1515, Dropout p: 0.30, Patience: 63/75            
Epoch [0240/0300], Training Loss: 0.1005, Validation Loss: 0.1447, Dropout p: 0.30, Patience: 64/75            
Epoch [0241/0300], Training Loss: 0.1046, Validation Loss: 0.1499, Dropout p: 0.30, Patience: 65/75            
Epoch [0242/0300], Training Loss: 0.1236, Validation Loss: 0.1559, Dropout p: 0.30, Patience: 66/75            
Epoch [0243/0300], Training Loss: 0.1682, Validation Loss: 0.1536, Dropout p: 0.30, Patience: 67/75            
Epoch [0244/0300], Training Loss: 0.1416, Validation Loss: 0.1511, Dropout p: 0.30, Patience: 68/75            
Epoch [0245/0300], Training Loss: 0.1299, Validation Loss: 0.1464, Dropout p: 0.30, Patience: 69/75            
Epoch [0246/0300], Training Loss: 0.1214, Validation Loss: 0.1383, Dropout p: 0.30, Patience: 70/75            
Epoch [0247/0300], Training Loss: 0.1406, Validation Loss: 0.1757, Dropout p: 0.30, Patience: 71/75            
Epoch [0248/0300], Training Loss: 0.1034, Validation Loss: 0.1477, Dropout p: 0.30, Patience: 72/75            
Epoch [0249/0300], Training Loss: 0.1425, Validation Loss: 0.1666, Dropout p: 0.30, Patience: 73/75            
Epoch [0250/0300], Training Loss: 0.1195, Validation Loss: 0.1498, Dropout p: 0.30, Patience: 74/75            
Epoch [0251/0300], Training Loss: 0.1217, Validation Loss: 0.1439, Dropout p: 0.30, Patience: 75/75            

Evaluating model on test set...
Predicted TBV: 164.29	Actual TBV: 171.57	Difference: 7.28
Predicted TBV: 173.94	Actual TBV: 201.02	Difference: 27.08
Predicted TBV: 209.97	Actual TBV: 216.91	Difference: 6.94
Predicted TBV: 269.65	Actual TBV: 255.22	Difference: 14.43
Predicted TBV: 75.75	Actual TBV: 73.72	Difference: 2.03
Predicted TBV: 242.18	Actual TBV: 229.56	Difference: 12.62
Predicted TBV: 222.34	Actual TBV: 297.97	Difference: 75.63
Predicted TBV: 217.85	Actual TBV: 246.01	Difference: 28.16
Predicted TBV: 188.75	Actual TBV: 173.80	Difference: 14.95
Predicted TBV: 142.11	Actual TBV: 177.28	Difference: 35.17
Predicted TBV: 154.74	Actual TBV: 175.59	Difference: 20.85
Predicted TBV: 220.52	Actual TBV: 223.44	Difference: 2.92
Predicted TBV: 230.80	Actual TBV: 248.90	Difference: 18.10
Predicted TBV: 190.35	Actual TBV: 191.24	Difference: 0.89
Predicted TBV: 170.24	Actual TBV: 164.27	Difference: 5.97
Predicted TBV: 126.66	Actual TBV: 121.69	Difference: 4.97
Predicted TBV: 252.80	Actual TBV: 261.03	Difference: 8.23
Predicted TBV: 172.17	Actual TBV: 176.77	Difference: 4.60
Predicted TBV: 164.07	Actual TBV: 171.73	Difference: 7.66
Predicted TBV: 317.10	Actual TBV: 278.87	Difference: 38.23
Predicted TBV: 219.94	Actual TBV: 218.52	Difference: 1.42
Predicted TBV: 210.35	Actual TBV: 193.38	Difference: 16.97
Predicted TBV: 96.06	Actual TBV: 76.97	Difference: 19.09
Predicted TBV: 340.39	Actual TBV: 347.91	Difference: 7.52
Predicted TBV: 266.50	Actual TBV: 261.97	Difference: 4.53
Predicted TBV: 226.11	Actual TBV: 227.14	Difference: 1.03
Predicted TBV: 191.16	Actual TBV: 195.02	Difference: 3.86
Predicted TBV: 144.17	Actual TBV: 177.13	Difference: 32.96
Predicted TBV: 236.33	Actual TBV: 169.96	Difference: 66.37
Predicted TBV: 172.92	Actual TBV: 181.50	Difference: 8.58
Predicted TBV: 170.44	Actual TBV: 177.62	Difference: 7.18
Predicted TBV: 204.63	Actual TBV: 178.51	Difference: 26.12
Predicted TBV: 222.55	Actual TBV: 218.43	Difference: 4.12
Predicted TBV: 226.76	Actual TBV: 229.76	Difference: 3.00
Predicted TBV: 212.34	Actual TBV: 248.58	Difference: 36.24
Predicted TBV: 228.23	Actual TBV: 239.22	Difference: 10.99
Predicted TBV: 179.44	Actual TBV: 187.35	Difference: 7.91
Predicted TBV: 263.37	Actual TBV: 268.50	Difference: 5.13
Predicted TBV: 332.30	Actual TBV: 326.10	Difference: 6.20
Predicted TBV: 211.47	Actual TBV: 232.66	Difference: 21.19

Evaluating model with 30 Bayesian runs...

Evaluating model with transforms and dropout...
Refused raster with error 11.06. TBV: 20.85.
Refused raster with error 10.73. TBV: 8.23.
Refused raster with error 13.28. TBV: 38.23.
Refused raster with error 11.22. TBV: 7.52.
Refused raster with error 14.04. TBV: 66.37.
Refused raster with error 10.62. TBV: 36.24.
Refused raster with error 10.46. TBV: 10.99.
Refused raster with error 16.99. TBV: 6.20.
Refused raster with error 15.52. TBV: 21.19.
Refused raster with error 11.88. TBV: 10.76.
Refused raster with error 12.44. TBV: 3.38.
Refused raster with error 13.53. TBV: 16.49.
Refused raster with error 12.09. TBV: 9.13.
Refused raster with error 10.39. TBV: 13.55.
Refused raster with error 11.81. TBV: 33.71.
Refused raster with error 14.89. TBV: 6.04.

Evaluating model with transforms only...
Refused raster with error 6.02. TBV: 75.63.
Refused raster with error 6.07. TBV: 18.10.
Refused raster with error 5.82. TBV: 8.23.
Refused raster with error 10.05. TBV: 38.23.
Refused raster with error 5.63. TBV: 3.86.
Refused raster with error 6.28. TBV: 36.24.
Refused raster with error 5.96. TBV: 10.99.
Refused raster with error 9.19. TBV: 6.20.
Refused raster with error 5.68. TBV: 21.19.
Refused raster with error 8.57. TBV: 10.76.
Refused raster with error 5.93. TBV: 34.42.
Refused raster with error 7.27. TBV: 3.38.
Refused raster with error 8.03. TBV: 16.49.
Refused raster with error 5.69. TBV: 13.22.
Refused raster with error 5.68. TBV: 30.38.
Refused raster with error 5.70. TBV: 9.13.
Refused raster with error 5.97. TBV: 13.55.
Refused raster with error 9.56. TBV: 2.63.
Refused raster with error 6.49. TBV: 21.51.
Refused raster with error 7.29. TBV: 3.24.
Refused raster with error 5.89. TBV: 6.04.
Refused raster with error 6.08. TBV: 17.36.

Evaluating model with dropout only...
Refused raster with error 10.12. TBV: 20.85.
Refused raster with error 11.06. TBV: 38.23.
Refused raster with error 12.00. TBV: 66.37.
Refused raster with error 17.96. TBV: 6.20.
Refused raster with error 10.49. TBV: 10.76.
Refused raster with error 11.60. TBV: 3.38.
Refused raster with error 9.57. TBV: 16.49.
Refused raster with error 11.57. TBV: 9.13.
Refused raster with error 9.79. TBV: 13.55.
Refused raster with error 13.47. TBV: 33.71.
Refused raster with error 9.14. TBV: 3.24.
Refused raster with error 10.86. TBV: 6.04.

- - - - - -
Non-bayesian prediction:
Total Raster Count: 76
Mean Absolute Error: 14.85 cc
Standard Deviation: 13.61 cc
Big error count (>30): 11
Big error mean: 40.42 cc
Big error std: 14.73 cc
- - - - - -

Bayesian prediction Dropout + Transforms Bayes:
Refused Raster Count: 16
Refused Raster Percentage: 21.05%
Refusal threshold: 10.35
Mean Absolute Error: 13.66 cc
Standard Deviation: 12.55 cc
Big error count (>30): 7
Big error mean: 38.59 cc
Big error std: 15.23 cc
- - - - - -

Bayesian prediction Transforms Bayes:
Refused Raster Count: 22
Refused Raster Percentage: 28.95%
Refusal threshold: 5.625
Mean Absolute Error: 13.48 cc
Standard Deviation: 11.97 cc
Big error count (>30): 6
Big error mean: 38.29 cc
Big error std: 12.66 cc
- - - - - -

Bayesian prediction Dropout Bayes:
Refused Raster Count: 12
Refused Raster Percentage: 15.79%
Refusal threshold: 9.0
Mean Absolute Error: 14.07 cc
Standard Deviation: 12.48 cc
Big error count (>30): 8
Big error mean: 38.29 cc
Big error std: 14.27 cc
- - - - - -

 - - - Fold 2/6 - - -

Train volume count: 288
Validation volume count: 82
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '29' '7' '41' '33' '76' '30' '87'
 '77' '79' '43' '47' '35' '16' '44' '26' '81' '46' '45' '50' '18' '32'
 '21' '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.0259, Validation Loss: 1.4850, Dropout p: 0.30, Patience: 0/75            
Epoch [0002/0300], Training Loss: 0.8822, Validation Loss: 1.5720, Dropout p: 0.30, Patience: 0/75            
Epoch [0003/0300], Training Loss: 0.8632, Validation Loss: 1.1808, Dropout p: 0.30, Patience: 0/75            
Epoch [0004/0300], Training Loss: 0.7416, Validation Loss: 0.9779, Dropout p: 0.30, Patience: 0/75            
Epoch [0005/0300], Training Loss: 0.7150, Validation Loss: 0.6584, Dropout p: 0.30, Patience: 0/75            
Epoch [0006/0300], Training Loss: 0.6402, Validation Loss: 0.4126, Dropout p: 0.30, Patience: 0/75            
Epoch [0007/0300], Training Loss: 0.4983, Validation Loss: 0.6378, Dropout p: 0.30, Patience: 0/75            
Epoch [0008/0300], Training Loss: 0.5149, Validation Loss: 0.5696, Dropout p: 0.30, Patience: 0/75            
Epoch [0009/0300], Training Loss: 0.4079, Validation Loss: 0.2124, Dropout p: 0.30, Patience: 0/75            
Epoch [0010/0300], Training Loss: 0.4557, Validation Loss: 0.2029, Dropout p: 0.30, Patience: 0/75            
Epoch [0011/0300], Training Loss: 0.4141, Validation Loss: 0.1428, Dropout p: 0.30, Patience: 0/75            
Epoch [0012/0300], Training Loss: 0.3139, Validation Loss: 0.1961, Dropout p: 0.30, Patience: 0/75            
Epoch [0013/0300], Training Loss: 0.4471, Validation Loss: 0.2403, Dropout p: 0.30, Patience: 0/75            
Epoch [0014/0300], Training Loss: 0.3484, Validation Loss: 0.1130, Dropout p: 0.30, Patience: 0/75            
Epoch [0015/0300], Training Loss: 0.3256, Validation Loss: 0.1733, Dropout p: 0.30, Patience: 0/75            
Epoch [0016/0300], Training Loss: 0.3450, Validation Loss: 0.1339, Dropout p: 0.30, Patience: 0/75            
Epoch [0017/0300], Training Loss: 0.3110, Validation Loss: 0.1117, Dropout p: 0.30, Patience: 0/75            
Epoch [0018/0300], Training Loss: 0.3251, Validation Loss: 0.1760, Dropout p: 0.30, Patience: 0/75            
Epoch [0019/0300], Training Loss: 0.3686, Validation Loss: 0.2234, Dropout p: 0.30, Patience: 0/75            
Epoch [0020/0300], Training Loss: 0.2930, Validation Loss: 0.1546, Dropout p: 0.30, Patience: 0/75            
Epoch [0021/0300], Training Loss: 0.3021, Validation Loss: 0.1728, Dropout p: 0.30, Patience: 0/75            
Epoch [0022/0300], Training Loss: 0.2714, Validation Loss: 0.3549, Dropout p: 0.30, Patience: 0/75            
Epoch [0023/0300], Training Loss: 0.2768, Validation Loss: 0.1913, Dropout p: 0.30, Patience: 0/75            
Epoch [0024/0300], Training Loss: 0.2997, Validation Loss: 0.1141, Dropout p: 0.30, Patience: 0/75            
Epoch [0025/0300], Training Loss: 0.2390, Validation Loss: 0.1036, Dropout p: 0.30, Patience: 0/75            
Epoch [0026/0300], Training Loss: 0.2760, Validation Loss: 0.1609, Dropout p: 0.30, Patience: 0/75            
Epoch [0027/0300], Training Loss: 0.3176, Validation Loss: 0.1074, Dropout p: 0.30, Patience: 0/75            
Epoch [0028/0300], Training Loss: 0.3325, Validation Loss: 0.0883, Dropout p: 0.30, Patience: 0/75            
Epoch [0029/0300], Training Loss: 0.2632, Validation Loss: 0.3477, Dropout p: 0.30, Patience: 0/75            
Epoch [0030/0300], Training Loss: 0.2396, Validation Loss: 0.1213, Dropout p: 0.30, Patience: 0/75            
Epoch [0031/0300], Training Loss: 0.2219, Validation Loss: 0.1960, Dropout p: 0.30, Patience: 0/75            
Epoch [0032/0300], Training Loss: 0.2510, Validation Loss: 0.0993, Dropout p: 0.30, Patience: 0/75            
Epoch [0033/0300], Training Loss: 0.2877, Validation Loss: 0.1023, Dropout p: 0.30, Patience: 0/75            
Epoch [0034/0300], Training Loss: 0.2332, Validation Loss: 0.1300, Dropout p: 0.30, Patience: 0/75            
Epoch [0035/0300], Training Loss: 0.1835, Validation Loss: 0.0924, Dropout p: 0.30, Patience: 0/75            
Epoch [0036/0300], Training Loss: 0.1685, Validation Loss: 0.4255, Dropout p: 0.30, Patience: 0/75            
Epoch [0037/0300], Training Loss: 0.2164, Validation Loss: 0.0927, Dropout p: 0.30, Patience: 0/75            
Epoch [0038/0300], Training Loss: 0.2184, Validation Loss: 0.1079, Dropout p: 0.30, Patience: 0/75            
Epoch [0039/0300], Training Loss: 0.1843, Validation Loss: 0.0858, Dropout p: 0.30, Patience: 0/75            
Epoch [0040/0300], Training Loss: 0.2292, Validation Loss: 0.2889, Dropout p: 0.30, Patience: 0/75            
Epoch [0041/0300], Training Loss: 0.1798, Validation Loss: 0.1312, Dropout p: 0.30, Patience: 0/75            
Epoch [0042/0300], Training Loss: 0.1951, Validation Loss: 0.0842, Dropout p: 0.30, Patience: 0/75            
Epoch [0043/0300], Training Loss: 0.1950, Validation Loss: 0.0878, Dropout p: 0.30, Patience: 0/75            
Epoch [0044/0300], Training Loss: 0.1680, Validation Loss: 0.1066, Dropout p: 0.30, Patience: 0/75            
Epoch [0045/0300], Training Loss: 0.1858, Validation Loss: 0.0882, Dropout p: 0.30, Patience: 0/75            
Epoch [0046/0300], Training Loss: 0.1684, Validation Loss: 0.1544, Dropout p: 0.30, Patience: 0/75            
Epoch [0047/0300], Training Loss: 0.1368, Validation Loss: 0.0824, Dropout p: 0.30, Patience: 0/75            
Epoch [0048/0300], Training Loss: 0.1609, Validation Loss: 0.0735, Dropout p: 0.30, Patience: 0/75            
Epoch [0049/0300], Training Loss: 0.1719, Validation Loss: 0.0778, Dropout p: 0.30, Patience: 0/75            
Epoch [0050/0300], Training Loss: 0.1504, Validation Loss: 0.0665, Dropout p: 0.30, Patience: 0/75            
Epoch [0051/0300], Training Loss: 0.1626, Validation Loss: 0.0907, Dropout p: 0.30, Patience: 0/75            
Epoch [0052/0300], Training Loss: 0.1518, Validation Loss: 0.2003, Dropout p: 0.30, Patience: 0/75            
Epoch [0053/0300], Training Loss: 0.1882, Validation Loss: 0.1297, Dropout p: 0.30, Patience: 0/75            
Epoch [0054/0300], Training Loss: 0.1755, Validation Loss: 0.0887, Dropout p: 0.30, Patience: 0/75            
Epoch [0055/0300], Training Loss: 0.1326, Validation Loss: 0.1044, Dropout p: 0.30, Patience: 0/75            
Epoch [0056/0300], Training Loss: 0.1631, Validation Loss: 0.0704, Dropout p: 0.30, Patience: 0/75            
Epoch [0057/0300], Training Loss: 0.1396, Validation Loss: 0.1858, Dropout p: 0.30, Patience: 0/75            
Epoch [0058/0300], Training Loss: 0.1752, Validation Loss: 0.0851, Dropout p: 0.30, Patience: 0/75            
Epoch [0059/0300], Training Loss: 0.1752, Validation Loss: 0.1211, Dropout p: 0.30, Patience: 0/75            
Epoch [0060/0300], Training Loss: 0.1561, Validation Loss: 0.0900, Dropout p: 0.30, Patience: 0/75            
Epoch [0061/0300], Training Loss: 0.1615, Validation Loss: 0.1096, Dropout p: 0.30, Patience: 0/75            
Epoch [0062/0300], Training Loss: 0.1554, Validation Loss: 0.2351, Dropout p: 0.30, Patience: 0/75            
Epoch [0063/0300], Training Loss: 0.1856, Validation Loss: 0.1018, Dropout p: 0.30, Patience: 0/75            
Epoch [0064/0300], Training Loss: 0.1349, Validation Loss: 0.1938, Dropout p: 0.30, Patience: 0/75            
Epoch [0065/0300], Training Loss: 0.1423, Validation Loss: 0.1749, Dropout p: 0.30, Patience: 0/75            
Epoch [0066/0300], Training Loss: 0.1346, Validation Loss: 0.0857, Dropout p: 0.30, Patience: 0/75            
Epoch [0067/0300], Training Loss: 0.1376, Validation Loss: 0.0614, Dropout p: 0.30, Patience: 0/75            
Epoch [0068/0300], Training Loss: 0.1314, Validation Loss: 0.1538, Dropout p: 0.30, Patience: 0/75            
Epoch [0069/0300], Training Loss: 0.1351, Validation Loss: 0.0819, Dropout p: 0.30, Patience: 0/75            
Epoch [0070/0300], Training Loss: 0.1312, Validation Loss: 0.1013, Dropout p: 0.30, Patience: 0/75            
Epoch [0071/0300], Training Loss: 0.1092, Validation Loss: 0.0822, Dropout p: 0.30, Patience: 0/75            
Epoch [0072/0300], Training Loss: 0.1230, Validation Loss: 0.0949, Dropout p: 0.30, Patience: 0/75            
Epoch [0073/0300], Training Loss: 0.1098, Validation Loss: 0.0609, Dropout p: 0.30, Patience: 0/75            
Epoch [0074/0300], Training Loss: 0.1360, Validation Loss: 0.0655, Dropout p: 0.30, Patience: 0/75            
Epoch [0075/0300], Training Loss: 0.0975, Validation Loss: 0.1199, Dropout p: 0.30, Patience: 0/75            
Epoch [0076/0300], Training Loss: 0.0915, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 0/75            
Epoch [0077/0300], Training Loss: 0.1033, Validation Loss: 0.0688, Dropout p: 0.30, Patience: 0/75            
Epoch [0078/0300], Training Loss: 0.1059, Validation Loss: 0.0703, Dropout p: 0.30, Patience: 0/75            
Epoch [0079/0300], Training Loss: 0.0892, Validation Loss: 0.0696, Dropout p: 0.30, Patience: 0/75            
Epoch [0080/0300], Training Loss: 0.1117, Validation Loss: 0.0999, Dropout p: 0.30, Patience: 0/75            
Epoch [0081/0300], Training Loss: 0.1207, Validation Loss: 0.0657, Dropout p: 0.30, Patience: 0/75            
Epoch [0082/0300], Training Loss: 0.0899, Validation Loss: 0.0760, Dropout p: 0.30, Patience: 0/75            
Epoch [0083/0300], Training Loss: 0.1059, Validation Loss: 0.1009, Dropout p: 0.30, Patience: 0/75            
Epoch [0084/0300], Training Loss: 0.1266, Validation Loss: 0.0752, Dropout p: 0.30, Patience: 0/75            
Epoch [0085/0300], Training Loss: 0.0872, Validation Loss: 0.0742, Dropout p: 0.30, Patience: 0/75            
Epoch [0086/0300], Training Loss: 0.1257, Validation Loss: 0.0645, Dropout p: 0.30, Patience: 0/75            
Epoch [0087/0300], Training Loss: 0.0813, Validation Loss: 0.0873, Dropout p: 0.30, Patience: 0/75            
Epoch [0088/0300], Training Loss: 0.0848, Validation Loss: 0.0912, Dropout p: 0.30, Patience: 0/75            
Epoch [0089/0300], Training Loss: 0.1011, Validation Loss: 0.0913, Dropout p: 0.30, Patience: 0/75            
Epoch [0090/0300], Training Loss: 0.0892, Validation Loss: 0.0756, Dropout p: 0.30, Patience: 0/75            
Epoch [0091/0300], Training Loss: 0.0707, Validation Loss: 0.0712, Dropout p: 0.30, Patience: 0/75            
Epoch [0092/0300], Training Loss: 0.1020, Validation Loss: 0.0755, Dropout p: 0.30, Patience: 0/75            
Epoch [0093/0300], Training Loss: 0.0779, Validation Loss: 0.0882, Dropout p: 0.30, Patience: 0/75            
Epoch [0094/0300], Training Loss: 0.0931, Validation Loss: 0.0879, Dropout p: 0.30, Patience: 0/75            
Epoch [0095/0300], Training Loss: 0.0709, Validation Loss: 0.0784, Dropout p: 0.30, Patience: 0/75            
Epoch [0096/0300], Training Loss: 0.0807, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 0/75            
Epoch [0097/0300], Training Loss: 0.1032, Validation Loss: 0.0885, Dropout p: 0.30, Patience: 0/75            
Epoch [0098/0300], Training Loss: 0.0710, Validation Loss: 0.0808, Dropout p: 0.30, Patience: 0/75            
Epoch [0099/0300], Training Loss: 0.0795, Validation Loss: 0.0917, Dropout p: 0.30, Patience: 0/75            
Epoch [0100/0300], Training Loss: 0.0897, Validation Loss: 0.0688, Dropout p: 0.30, Patience: 0/75            
Epoch [0101/0300], Training Loss: 0.0726, Validation Loss: 0.0902, Dropout p: 0.30, Patience: 0/75            
Epoch [0102/0300], Training Loss: 0.0757, Validation Loss: 0.0727, Dropout p: 0.30, Patience: 0/75            
Epoch [0103/0300], Training Loss: 0.0607, Validation Loss: 0.0680, Dropout p: 0.30, Patience: 0/75            
Epoch [0104/0300], Training Loss: 0.0676, Validation Loss: 0.0993, Dropout p: 0.30, Patience: 1/75            
Epoch [0105/0300], Training Loss: 0.0809, Validation Loss: 0.0730, Dropout p: 0.30, Patience: 2/75            
Epoch [0106/0300], Training Loss: 0.0613, Validation Loss: 0.0737, Dropout p: 0.30, Patience: 3/75            
Epoch [0107/0300], Training Loss: 0.0775, Validation Loss: 0.0826, Dropout p: 0.30, Patience: 4/75            
Epoch [0108/0300], Training Loss: 0.0701, Validation Loss: 0.0681, Dropout p: 0.30, Patience: 5/75            
Epoch [0109/0300], Training Loss: 0.0696, Validation Loss: 0.0965, Dropout p: 0.30, Patience: 6/75            
Epoch [0110/0300], Training Loss: 0.0819, Validation Loss: 0.0878, Dropout p: 0.30, Patience: 7/75            
Epoch [0111/0300], Training Loss: 0.0952, Validation Loss: 0.0776, Dropout p: 0.30, Patience: 8/75            
Epoch [0112/0300], Training Loss: 0.0630, Validation Loss: 0.0748, Dropout p: 0.30, Patience: 9/75            
Epoch [0113/0300], Training Loss: 0.0672, Validation Loss: 0.0633, Dropout p: 0.30, Patience: 0/75            
Epoch [0114/0300], Training Loss: 0.0619, Validation Loss: 0.0745, Dropout p: 0.30, Patience: 1/75            
Epoch [0115/0300], Training Loss: 0.0696, Validation Loss: 0.0778, Dropout p: 0.30, Patience: 2/75            
Epoch [0116/0300], Training Loss: 0.0605, Validation Loss: 0.0737, Dropout p: 0.30, Patience: 3/75            
Epoch [0117/0300], Training Loss: 0.0736, Validation Loss: 0.0694, Dropout p: 0.30, Patience: 4/75            
Epoch [0118/0300], Training Loss: 0.0690, Validation Loss: 0.0721, Dropout p: 0.30, Patience: 5/75            
Epoch [0119/0300], Training Loss: 0.0708, Validation Loss: 0.0798, Dropout p: 0.30, Patience: 6/75            
Epoch [0120/0300], Training Loss: 0.0656, Validation Loss: 0.0720, Dropout p: 0.30, Patience: 7/75            
Epoch [0121/0300], Training Loss: 0.0664, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 8/75            
Epoch [0122/0300], Training Loss: 0.0752, Validation Loss: 0.0817, Dropout p: 0.30, Patience: 9/75            
Epoch [0123/0300], Training Loss: 0.0612, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 10/75            
Epoch [0124/0300], Training Loss: 0.0665, Validation Loss: 0.0750, Dropout p: 0.30, Patience: 11/75            
Epoch [0125/0300], Training Loss: 0.0627, Validation Loss: 0.0748, Dropout p: 0.30, Patience: 12/75            
Epoch [0126/0300], Training Loss: 0.0684, Validation Loss: 0.0822, Dropout p: 0.30, Patience: 13/75            
Epoch [0127/0300], Training Loss: 0.0847, Validation Loss: 0.0813, Dropout p: 0.30, Patience: 14/75            
Epoch [0128/0300], Training Loss: 0.0599, Validation Loss: 0.0856, Dropout p: 0.30, Patience: 15/75            
Epoch [0129/0300], Training Loss: 0.0586, Validation Loss: 0.0777, Dropout p: 0.30, Patience: 16/75            
Epoch [0130/0300], Training Loss: 0.0632, Validation Loss: 0.0701, Dropout p: 0.30, Patience: 17/75            
Epoch [0131/0300], Training Loss: 0.0651, Validation Loss: 0.0754, Dropout p: 0.30, Patience: 18/75            
Epoch [0132/0300], Training Loss: 0.0732, Validation Loss: 0.0767, Dropout p: 0.30, Patience: 19/75            
Epoch [0133/0300], Training Loss: 0.0692, Validation Loss: 0.0730, Dropout p: 0.30, Patience: 20/75            
Epoch [0134/0300], Training Loss: 0.0705, Validation Loss: 0.0704, Dropout p: 0.30, Patience: 21/75            
Epoch [0135/0300], Training Loss: 0.0575, Validation Loss: 0.0744, Dropout p: 0.30, Patience: 22/75            
Epoch [0136/0300], Training Loss: 0.0731, Validation Loss: 0.0738, Dropout p: 0.30, Patience: 23/75            
Epoch [0137/0300], Training Loss: 0.0641, Validation Loss: 0.0684, Dropout p: 0.30, Patience: 24/75            
Epoch [0138/0300], Training Loss: 0.0848, Validation Loss: 0.0693, Dropout p: 0.30, Patience: 25/75            
Epoch [0139/0300], Training Loss: 0.0594, Validation Loss: 0.0738, Dropout p: 0.30, Patience: 26/75            
Epoch [0140/0300], Training Loss: 0.0652, Validation Loss: 0.0769, Dropout p: 0.30, Patience: 27/75            
Epoch [0141/0300], Training Loss: 0.0594, Validation Loss: 0.0762, Dropout p: 0.30, Patience: 28/75            
Epoch [0142/0300], Training Loss: 0.0511, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 29/75            
Epoch [0143/0300], Training Loss: 0.0625, Validation Loss: 0.0730, Dropout p: 0.30, Patience: 30/75            
Epoch [0144/0300], Training Loss: 0.0509, Validation Loss: 0.0693, Dropout p: 0.30, Patience: 31/75            
Epoch [0145/0300], Training Loss: 0.0537, Validation Loss: 0.0696, Dropout p: 0.30, Patience: 32/75            
Epoch [0146/0300], Training Loss: 0.0645, Validation Loss: 0.0749, Dropout p: 0.30, Patience: 33/75            
Epoch [0147/0300], Training Loss: 0.0773, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 34/75            
Epoch [0148/0300], Training Loss: 0.0680, Validation Loss: 0.0733, Dropout p: 0.30, Patience: 35/75            
Epoch [0149/0300], Training Loss: 0.0623, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 36/75            
Epoch [0150/0300], Training Loss: 0.0774, Validation Loss: 0.0681, Dropout p: 0.30, Patience: 37/75            
Epoch [0151/0300], Training Loss: 0.0597, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 38/75            
Epoch [0152/0300], Training Loss: 0.0519, Validation Loss: 0.0696, Dropout p: 0.30, Patience: 39/75            
Epoch [0153/0300], Training Loss: 0.0560, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 40/75            
Epoch [0154/0300], Training Loss: 0.0547, Validation Loss: 0.0714, Dropout p: 0.30, Patience: 41/75            
Epoch [0155/0300], Training Loss: 0.0678, Validation Loss: 0.0746, Dropout p: 0.30, Patience: 42/75            
Epoch [0156/0300], Training Loss: 0.0518, Validation Loss: 0.0726, Dropout p: 0.30, Patience: 43/75            
Epoch [0157/0300], Training Loss: 0.0619, Validation Loss: 0.0750, Dropout p: 0.30, Patience: 44/75            
Epoch [0158/0300], Training Loss: 0.0690, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 45/75            
Epoch [0159/0300], Training Loss: 0.0620, Validation Loss: 0.0730, Dropout p: 0.30, Patience: 46/75            
Epoch [0160/0300], Training Loss: 0.0451, Validation Loss: 0.0727, Dropout p: 0.30, Patience: 47/75            
Epoch [0161/0300], Training Loss: 0.0608, Validation Loss: 0.0767, Dropout p: 0.30, Patience: 48/75            
Epoch [0162/0300], Training Loss: 0.0567, Validation Loss: 0.0747, Dropout p: 0.30, Patience: 49/75            
Epoch [0163/0300], Training Loss: 0.0820, Validation Loss: 0.0756, Dropout p: 0.30, Patience: 50/75            
Epoch [0164/0300], Training Loss: 0.0591, Validation Loss: 0.0705, Dropout p: 0.30, Patience: 51/75            
Epoch [0165/0300], Training Loss: 0.0601, Validation Loss: 0.0763, Dropout p: 0.30, Patience: 52/75            
Epoch [0166/0300], Training Loss: 0.0580, Validation Loss: 0.0737, Dropout p: 0.30, Patience: 53/75            
Epoch [0167/0300], Training Loss: 0.0671, Validation Loss: 0.0735, Dropout p: 0.30, Patience: 54/75            
Epoch [0168/0300], Training Loss: 0.0696, Validation Loss: 0.0745, Dropout p: 0.30, Patience: 55/75            
Epoch [0169/0300], Training Loss: 0.0647, Validation Loss: 0.0708, Dropout p: 0.30, Patience: 56/75            
Epoch [0170/0300], Training Loss: 0.0571, Validation Loss: 0.0719, Dropout p: 0.30, Patience: 57/75            
Epoch [0171/0300], Training Loss: 0.0608, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 58/75            
Epoch [0172/0300], Training Loss: 0.0501, Validation Loss: 0.0711, Dropout p: 0.30, Patience: 59/75            
Epoch [0173/0300], Training Loss: 0.0693, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 60/75            
Epoch [0174/0300], Training Loss: 0.0884, Validation Loss: 0.0687, Dropout p: 0.30, Patience: 61/75            
Epoch [0175/0300], Training Loss: 0.0537, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 62/75            
Epoch [0176/0300], Training Loss: 0.0732, Validation Loss: 0.0728, Dropout p: 0.30, Patience: 63/75            
Epoch [0177/0300], Training Loss: 0.0653, Validation Loss: 0.0695, Dropout p: 0.30, Patience: 64/75            
Epoch [0178/0300], Training Loss: 0.0605, Validation Loss: 0.0689, Dropout p: 0.30, Patience: 65/75            
Epoch [0179/0300], Training Loss: 0.0592, Validation Loss: 0.0762, Dropout p: 0.30, Patience: 66/75            
Epoch [0180/0300], Training Loss: 0.0603, Validation Loss: 0.0711, Dropout p: 0.30, Patience: 67/75            
Epoch [0181/0300], Training Loss: 0.0582, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 68/75            
Epoch [0182/0300], Training Loss: 0.0650, Validation Loss: 0.0710, Dropout p: 0.30, Patience: 69/75            
Epoch [0183/0300], Training Loss: 0.0542, Validation Loss: 0.0731, Dropout p: 0.30, Patience: 70/75            
Epoch [0184/0300], Training Loss: 0.0725, Validation Loss: 0.0709, Dropout p: 0.30, Patience: 71/75            
Epoch [0185/0300], Training Loss: 0.0725, Validation Loss: 0.0700, Dropout p: 0.30, Patience: 72/75            
Epoch [0186/0300], Training Loss: 0.0495, Validation Loss: 0.0695, Dropout p: 0.30, Patience: 73/75            
Epoch [0187/0300], Training Loss: 0.0476, Validation Loss: 0.0738, Dropout p: 0.30, Patience: 74/75            
Epoch [0188/0300], Training Loss: 0.0560, Validation Loss: 0.0705, Dropout p: 0.30, Patience: 75/75            

Evaluating model on test set...
Predicted TBV: 259.62	Actual TBV: 278.68	Difference: 19.06
Predicted TBV: 185.66	Actual TBV: 185.38	Difference: 0.28
Predicted TBV: 202.05	Actual TBV: 232.09	Difference: 30.04
Predicted TBV: 252.79	Actual TBV: 255.49	Difference: 2.70
Predicted TBV: 272.10	Actual TBV: 270.60	Difference: 1.50
Predicted TBV: 425.91	Actual TBV: 436.77	Difference: 10.86
Predicted TBV: 108.99	Actual TBV: 103.80	Difference: 5.19
Predicted TBV: 262.23	Actual TBV: 261.07	Difference: 1.16
Predicted TBV: 326.93	Actual TBV: 326.98	Difference: 0.05
Predicted TBV: 323.55	Actual TBV: 310.82	Difference: 12.73
Predicted TBV: 221.52	Actual TBV: 203.67	Difference: 17.85
Predicted TBV: 214.21	Actual TBV: 203.15	Difference: 11.06
Predicted TBV: 186.47	Actual TBV: 173.72	Difference: 12.75
Predicted TBV: 227.43	Actual TBV: 199.16	Difference: 28.27
Predicted TBV: 187.63	Actual TBV: 185.07	Difference: 2.56
Predicted TBV: 174.58	Actual TBV: 167.47	Difference: 7.11
Predicted TBV: 171.54	Actual TBV: 169.57	Difference: 1.97
Predicted TBV: 131.04	Actual TBV: 128.96	Difference: 2.08
Predicted TBV: 441.89	Actual TBV: 389.83	Difference: 52.06
Predicted TBV: 141.98	Actual TBV: 159.59	Difference: 17.61
Predicted TBV: 315.12	Actual TBV: 346.51	Difference: 31.39
Predicted TBV: 195.57	Actual TBV: 185.31	Difference: 10.26
Predicted TBV: 362.26	Actual TBV: 381.77	Difference: 19.51
Predicted TBV: 196.79	Actual TBV: 185.04	Difference: 11.75
Predicted TBV: 248.55	Actual TBV: 246.80	Difference: 1.75
Predicted TBV: 175.88	Actual TBV: 174.98	Difference: 0.90
Predicted TBV: 202.93	Actual TBV: 209.68	Difference: 6.75
Predicted TBV: 240.30	Actual TBV: 236.49	Difference: 3.81
Predicted TBV: 299.45	Actual TBV: 282.55	Difference: 16.90
Predicted TBV: 198.22	Actual TBV: 220.23	Difference: 22.01
Predicted TBV: 178.47	Actual TBV: 170.82	Difference: 7.65
Predicted TBV: 228.18	Actual TBV: 228.85	Difference: 0.67
Predicted TBV: 175.79	Actual TBV: 179.03	Difference: 3.24
Predicted TBV: 265.95	Actual TBV: 281.36	Difference: 15.41
Predicted TBV: 210.90	Actual TBV: 203.92	Difference: 6.98
Predicted TBV: 223.37	Actual TBV: 203.03	Difference: 20.34
Predicted TBV: 356.40	Actual TBV: 337.52	Difference: 18.88
Predicted TBV: 183.80	Actual TBV: 180.88	Difference: 2.92
Predicted TBV: 195.30	Actual TBV: 179.91	Difference: 15.39
Predicted TBV: 203.23	Actual TBV: 184.50	Difference: 18.73

Evaluating model with 30 Bayesian runs...

Evaluating model with transforms and dropout...
Refused raster with error 12.84. TBV: 2.70.
Refused raster with error 11.64. TBV: 1.50.
Refused raster with error 10.68. TBV: 10.86.
Refused raster with error 17.16. TBV: 12.73.
Refused raster with error 10.70. TBV: 2.08.
Refused raster with error 12.02. TBV: 52.06.
Refused raster with error 10.51. TBV: 6.75.
Refused raster with error 18.68. TBV: 18.88.
Refused raster with error 12.11. TBV: 18.73.
Refused raster with error 11.03. TBV: 7.79.
Refused raster with error 11.65. TBV: 15.48.
Refused raster with error 10.39. TBV: 17.71.
Refused raster with error 13.10. TBV: 10.60.
Refused raster with error 16.16. TBV: 7.08.
Refused raster with error 13.32. TBV: 31.93.
Refused raster with error 16.96. TBV: 21.09.

Evaluating model with transforms only...
Refused raster with error 5.65. TBV: 1.50.
Refused raster with error 7.82. TBV: 52.06.
Refused raster with error 5.72. TBV: 6.75.
Refused raster with error 5.98. TBV: 22.01.
Refused raster with error 7.34. TBV: 17.71.
Refused raster with error 9.72. TBV: 0.36.
Refused raster with error 5.94. TBV: 31.93.
Refused raster with error 6.04. TBV: 12.33.
Refused raster with error 7.00. TBV: 21.09.

Evaluating model with dropout only...
Refused raster with error 11.37. TBV: 2.70.
Refused raster with error 10.64. TBV: 1.50.
Refused raster with error 12.03. TBV: 10.86.
Refused raster with error 13.93. TBV: 12.73.
Refused raster with error 12.19. TBV: 52.06.
Refused raster with error 12.81. TBV: 18.88.
Refused raster with error 9.83. TBV: 8.22.
Refused raster with error 12.64. TBV: 15.48.
Refused raster with error 14.14. TBV: 17.71.
Refused raster with error 14.34. TBV: 7.08.
Refused raster with error 10.64. TBV: 31.93.
Refused raster with error 10.97. TBV: 1.85.
Refused raster with error 15.13. TBV: 21.09.
Refused raster with error 9.28. TBV: 17.42.
Refused raster with error 9.97. TBV: 11.72.

- - - - - -
Non-bayesian prediction:
Total Raster Count: 82
Mean Absolute Error: 11.66 cc
Standard Deviation: 9.44 cc
Big error count (>30): 4
Big error mean: 36.36 cc
Big error std: 9.09 cc
- - - - - -

Bayesian prediction Dropout + Transforms Bayes:
Refused Raster Count: 16
Refused Raster Percentage: 19.51%
Refusal threshold: 10.35
Mean Absolute Error: 10.88 cc
Standard Deviation: 8.39 cc
Big error count (>30): 2
Big error mean: 30.72 cc
Big error std: 0.68 cc
- - - - - -

Bayesian prediction Transforms Bayes:
Refused Raster Count: 9
Refused Raster Percentage: 10.98%
Refusal threshold: 5.625
Mean Absolute Error: 10.82 cc
Standard Deviation: 8.05 cc
Big error count (>30): 2
Big error mean: 30.72 cc
Big error std: 0.68 cc
- - - - - -

Bayesian prediction Dropout Bayes:
Refused Raster Count: 15
Refused Raster Percentage: 18.29%
Refusal threshold: 9.0
Mean Absolute Error: 10.82 cc
Standard Deviation: 8.36 cc
Big error count (>30): 2
Big error mean: 30.72 cc
Big error std: 0.68 cc
- - - - - -

 - - - Fold 3/6 - - -

Train volume count: 312
Validation volume count: 58
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '79' '43' '47' '35' '16' '44' '26' '81' '46' '45' '50' '18' '32'
 '21' '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 0.8891, Validation Loss: 2.0028, Dropout p: 0.30, Patience: 0/75            
Epoch [0002/0300], Training Loss: 0.8452, Validation Loss: 2.3043, Dropout p: 0.30, Patience: 0/75            
Epoch [0003/0300], Training Loss: 0.7988, Validation Loss: 1.1949, Dropout p: 0.30, Patience: 0/75            
Epoch [0004/0300], Training Loss: 0.6770, Validation Loss: 1.2151, Dropout p: 0.30, Patience: 0/75            
Epoch [0005/0300], Training Loss: 0.5878, Validation Loss: 0.6152, Dropout p: 0.30, Patience: 0/75            
Epoch [0006/0300], Training Loss: 0.5331, Validation Loss: 0.4816, Dropout p: 0.30, Patience: 0/75            
Epoch [0007/0300], Training Loss: 0.4387, Validation Loss: 0.3237, Dropout p: 0.30, Patience: 0/75            
Epoch [0008/0300], Training Loss: 0.4419, Validation Loss: 0.2058, Dropout p: 0.30, Patience: 0/75            
Epoch [0009/0300], Training Loss: 0.4508, Validation Loss: 0.3425, Dropout p: 0.30, Patience: 0/75            
Epoch [0010/0300], Training Loss: 0.3967, Validation Loss: 0.1530, Dropout p: 0.30, Patience: 0/75            
Epoch [0011/0300], Training Loss: 0.3320, Validation Loss: 0.1323, Dropout p: 0.30, Patience: 0/75            
Epoch [0012/0300], Training Loss: 0.3267, Validation Loss: 0.1339, Dropout p: 0.30, Patience: 0/75            
Epoch [0013/0300], Training Loss: 0.3667, Validation Loss: 0.2003, Dropout p: 0.30, Patience: 0/75            
Epoch [0014/0300], Training Loss: 0.3575, Validation Loss: 0.1117, Dropout p: 0.30, Patience: 0/75            
Epoch [0015/0300], Training Loss: 0.3349, Validation Loss: 0.0946, Dropout p: 0.30, Patience: 0/75            
Epoch [0016/0300], Training Loss: 0.2886, Validation Loss: 0.1202, Dropout p: 0.30, Patience: 0/75            
Epoch [0017/0300], Training Loss: 0.2541, Validation Loss: 0.2166, Dropout p: 0.30, Patience: 0/75            
Epoch [0018/0300], Training Loss: 0.2701, Validation Loss: 0.1140, Dropout p: 0.30, Patience: 0/75            
Epoch [0019/0300], Training Loss: 0.2742, Validation Loss: 0.2240, Dropout p: 0.30, Patience: 0/75            
Epoch [0020/0300], Training Loss: 0.2624, Validation Loss: 0.2021, Dropout p: 0.30, Patience: 0/75            
Epoch [0021/0300], Training Loss: 0.3039, Validation Loss: 0.1563, Dropout p: 0.30, Patience: 0/75            
Epoch [0022/0300], Training Loss: 0.2261, Validation Loss: 0.1370, Dropout p: 0.30, Patience: 0/75            
Epoch [0023/0300], Training Loss: 0.2866, Validation Loss: 0.2005, Dropout p: 0.30, Patience: 0/75            
Epoch [0024/0300], Training Loss: 0.2123, Validation Loss: 0.0645, Dropout p: 0.30, Patience: 0/75            
Epoch [0025/0300], Training Loss: 0.2231, Validation Loss: 0.1392, Dropout p: 0.30, Patience: 0/75            
Epoch [0026/0300], Training Loss: 0.2610, Validation Loss: 0.1050, Dropout p: 0.30, Patience: 0/75            
Epoch [0027/0300], Training Loss: 0.2005, Validation Loss: 0.0749, Dropout p: 0.30, Patience: 0/75            
Epoch [0028/0300], Training Loss: 0.1920, Validation Loss: 0.1473, Dropout p: 0.30, Patience: 0/75            
Epoch [0029/0300], Training Loss: 0.2318, Validation Loss: 0.1722, Dropout p: 0.30, Patience: 0/75            
Epoch [0030/0300], Training Loss: 0.2728, Validation Loss: 0.1379, Dropout p: 0.30, Patience: 0/75            
Epoch [0031/0300], Training Loss: 0.2039, Validation Loss: 0.0652, Dropout p: 0.30, Patience: 0/75            
Epoch [0032/0300], Training Loss: 0.1959, Validation Loss: 0.1007, Dropout p: 0.30, Patience: 0/75            
Epoch [0033/0300], Training Loss: 0.2353, Validation Loss: 0.1430, Dropout p: 0.30, Patience: 0/75            
Epoch [0034/0300], Training Loss: 0.1940, Validation Loss: 0.1617, Dropout p: 0.30, Patience: 0/75            
Epoch [0035/0300], Training Loss: 0.1913, Validation Loss: 0.0818, Dropout p: 0.30, Patience: 0/75            
Epoch [0036/0300], Training Loss: 0.2370, Validation Loss: 0.0977, Dropout p: 0.30, Patience: 0/75            
Epoch [0037/0300], Training Loss: 0.1873, Validation Loss: 0.1116, Dropout p: 0.30, Patience: 0/75            
Epoch [0038/0300], Training Loss: 0.1865, Validation Loss: 0.1377, Dropout p: 0.30, Patience: 0/75            
Epoch [0039/0300], Training Loss: 0.2113, Validation Loss: 0.1310, Dropout p: 0.30, Patience: 0/75            
Epoch [0040/0300], Training Loss: 0.1475, Validation Loss: 0.0733, Dropout p: 0.30, Patience: 0/75            
Epoch [0041/0300], Training Loss: 0.1727, Validation Loss: 0.0967, Dropout p: 0.30, Patience: 0/75            
Epoch [0042/0300], Training Loss: 0.1692, Validation Loss: 0.1039, Dropout p: 0.30, Patience: 0/75            
Epoch [0043/0300], Training Loss: 0.1767, Validation Loss: 0.0802, Dropout p: 0.30, Patience: 0/75            
Epoch [0044/0300], Training Loss: 0.1520, Validation Loss: 0.1040, Dropout p: 0.30, Patience: 0/75            
Epoch [0045/0300], Training Loss: 0.1193, Validation Loss: 0.0938, Dropout p: 0.30, Patience: 0/75            
Epoch [0046/0300], Training Loss: 0.1854, Validation Loss: 0.1754, Dropout p: 0.30, Patience: 0/75            
Epoch [0047/0300], Training Loss: 0.1588, Validation Loss: 0.0848, Dropout p: 0.30, Patience: 0/75            
Epoch [0048/0300], Training Loss: 0.1700, Validation Loss: 0.1072, Dropout p: 0.30, Patience: 0/75            
Epoch [0049/0300], Training Loss: 0.1571, Validation Loss: 0.1700, Dropout p: 0.30, Patience: 0/75            
Epoch [0050/0300], Training Loss: 0.1408, Validation Loss: 0.1024, Dropout p: 0.30, Patience: 0/75            
Epoch [0051/0300], Training Loss: 0.1878, Validation Loss: 0.0903, Dropout p: 0.30, Patience: 0/75            
Epoch [0052/0300], Training Loss: 0.1336, Validation Loss: 0.0779, Dropout p: 0.30, Patience: 0/75            
Epoch [0053/0300], Training Loss: 0.1518, Validation Loss: 0.1129, Dropout p: 0.30, Patience: 0/75            
Epoch [0054/0300], Training Loss: 0.1877, Validation Loss: 0.0713, Dropout p: 0.30, Patience: 0/75            
Epoch [0055/0300], Training Loss: 0.1319, Validation Loss: 0.0926, Dropout p: 0.30, Patience: 0/75            
Epoch [0056/0300], Training Loss: 0.1272, Validation Loss: 0.0608, Dropout p: 0.30, Patience: 0/75            
Epoch [0057/0300], Training Loss: 0.1387, Validation Loss: 0.1233, Dropout p: 0.30, Patience: 0/75            
Epoch [0058/0300], Training Loss: 0.1124, Validation Loss: 0.0724, Dropout p: 0.30, Patience: 0/75            
Epoch [0059/0300], Training Loss: 0.1158, Validation Loss: 0.1256, Dropout p: 0.30, Patience: 0/75            
Epoch [0060/0300], Training Loss: 0.1472, Validation Loss: 0.0818, Dropout p: 0.30, Patience: 0/75            
Epoch [0061/0300], Training Loss: 0.1205, Validation Loss: 0.0938, Dropout p: 0.30, Patience: 0/75            
Epoch [0062/0300], Training Loss: 0.0948, Validation Loss: 0.0774, Dropout p: 0.30, Patience: 0/75            
Epoch [0063/0300], Training Loss: 0.1120, Validation Loss: 0.0689, Dropout p: 0.30, Patience: 0/75            
Epoch [0064/0300], Training Loss: 0.1103, Validation Loss: 0.0982, Dropout p: 0.30, Patience: 0/75            
Epoch [0065/0300], Training Loss: 0.1163, Validation Loss: 0.1019, Dropout p: 0.30, Patience: 0/75            
Epoch [0066/0300], Training Loss: 0.1006, Validation Loss: 0.1344, Dropout p: 0.30, Patience: 0/75            
Epoch [0067/0300], Training Loss: 0.1115, Validation Loss: 0.0797, Dropout p: 0.30, Patience: 0/75            
Epoch [0068/0300], Training Loss: 0.1146, Validation Loss: 0.0909, Dropout p: 0.30, Patience: 0/75            
Epoch [0069/0300], Training Loss: 0.0984, Validation Loss: 0.0832, Dropout p: 0.30, Patience: 0/75            
Epoch [0070/0300], Training Loss: 0.1064, Validation Loss: 0.0565, Dropout p: 0.30, Patience: 0/75            
Epoch [0071/0300], Training Loss: 0.0833, Validation Loss: 0.0879, Dropout p: 0.30, Patience: 0/75            
Epoch [0072/0300], Training Loss: 0.0937, Validation Loss: 0.1008, Dropout p: 0.30, Patience: 0/75            
Epoch [0073/0300], Training Loss: 0.0961, Validation Loss: 0.0886, Dropout p: 0.30, Patience: 0/75            
Epoch [0074/0300], Training Loss: 0.0813, Validation Loss: 0.0839, Dropout p: 0.30, Patience: 0/75            
Epoch [0075/0300], Training Loss: 0.1061, Validation Loss: 0.1301, Dropout p: 0.30, Patience: 0/75            
Epoch [0076/0300], Training Loss: 0.1173, Validation Loss: 0.0956, Dropout p: 0.30, Patience: 0/75            
Epoch [0077/0300], Training Loss: 0.0648, Validation Loss: 0.0926, Dropout p: 0.30, Patience: 0/75            
Epoch [0078/0300], Training Loss: 0.0780, Validation Loss: 0.1215, Dropout p: 0.30, Patience: 0/75            
Epoch [0079/0300], Training Loss: 0.0898, Validation Loss: 0.0589, Dropout p: 0.30, Patience: 0/75            
Epoch [0080/0300], Training Loss: 0.0848, Validation Loss: 0.0802, Dropout p: 0.30, Patience: 0/75            
Epoch [0081/0300], Training Loss: 0.1204, Validation Loss: 0.0694, Dropout p: 0.30, Patience: 0/75            
Epoch [0082/0300], Training Loss: 0.1132, Validation Loss: 0.1214, Dropout p: 0.30, Patience: 0/75            
Epoch [0083/0300], Training Loss: 0.1099, Validation Loss: 0.0961, Dropout p: 0.30, Patience: 0/75            
Epoch [0084/0300], Training Loss: 0.0780, Validation Loss: 0.0971, Dropout p: 0.30, Patience: 0/75            
Epoch [0085/0300], Training Loss: 0.0841, Validation Loss: 0.0719, Dropout p: 0.30, Patience: 0/75            
Epoch [0086/0300], Training Loss: 0.0859, Validation Loss: 0.0852, Dropout p: 0.30, Patience: 0/75            
Epoch [0087/0300], Training Loss: 0.0795, Validation Loss: 0.1201, Dropout p: 0.30, Patience: 0/75            
Epoch [0088/0300], Training Loss: 0.0741, Validation Loss: 0.0548, Dropout p: 0.30, Patience: 0/75            
Epoch [0089/0300], Training Loss: 0.0985, Validation Loss: 0.0842, Dropout p: 0.30, Patience: 0/75            
Epoch [0090/0300], Training Loss: 0.0933, Validation Loss: 0.0880, Dropout p: 0.30, Patience: 0/75            
Epoch [0091/0300], Training Loss: 0.0959, Validation Loss: 0.0841, Dropout p: 0.30, Patience: 0/75            
Epoch [0092/0300], Training Loss: 0.0759, Validation Loss: 0.0645, Dropout p: 0.30, Patience: 0/75            
Epoch [0093/0300], Training Loss: 0.0670, Validation Loss: 0.0691, Dropout p: 0.30, Patience: 0/75            
Epoch [0094/0300], Training Loss: 0.0790, Validation Loss: 0.0589, Dropout p: 0.30, Patience: 0/75            
Epoch [0095/0300], Training Loss: 0.0688, Validation Loss: 0.0761, Dropout p: 0.30, Patience: 0/75            
Epoch [0096/0300], Training Loss: 0.0727, Validation Loss: 0.0744, Dropout p: 0.30, Patience: 0/75            
Epoch [0097/0300], Training Loss: 0.0714, Validation Loss: 0.0707, Dropout p: 0.30, Patience: 0/75            
Epoch [0098/0300], Training Loss: 0.0685, Validation Loss: 0.0778, Dropout p: 0.30, Patience: 0/75            
Epoch [0099/0300], Training Loss: 0.0725, Validation Loss: 0.0896, Dropout p: 0.30, Patience: 0/75            
Epoch [0100/0300], Training Loss: 0.1123, Validation Loss: 0.0982, Dropout p: 0.30, Patience: 0/75            
Epoch [0101/0300], Training Loss: 0.0650, Validation Loss: 0.0840, Dropout p: 0.30, Patience: 0/75            
Epoch [0102/0300], Training Loss: 0.0735, Validation Loss: 0.0867, Dropout p: 0.30, Patience: 1/75            
Epoch [0103/0300], Training Loss: 0.0878, Validation Loss: 0.0667, Dropout p: 0.30, Patience: 0/75            
Epoch [0104/0300], Training Loss: 0.0699, Validation Loss: 0.0812, Dropout p: 0.30, Patience: 1/75            
Epoch [0105/0300], Training Loss: 0.0600, Validation Loss: 0.0820, Dropout p: 0.30, Patience: 2/75            
Epoch [0106/0300], Training Loss: 0.0620, Validation Loss: 0.0721, Dropout p: 0.30, Patience: 3/75            
Epoch [0107/0300], Training Loss: 0.0757, Validation Loss: 0.0730, Dropout p: 0.30, Patience: 4/75            
Epoch [0108/0300], Training Loss: 0.0684, Validation Loss: 0.0737, Dropout p: 0.30, Patience: 5/75            
Epoch [0109/0300], Training Loss: 0.0600, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 6/75            
Epoch [0110/0300], Training Loss: 0.0712, Validation Loss: 0.0777, Dropout p: 0.30, Patience: 7/75            
Epoch [0111/0300], Training Loss: 0.0606, Validation Loss: 0.0741, Dropout p: 0.30, Patience: 8/75            
Epoch [0112/0300], Training Loss: 0.0891, Validation Loss: 0.0616, Dropout p: 0.30, Patience: 0/75            
Epoch [0113/0300], Training Loss: 0.0686, Validation Loss: 0.0666, Dropout p: 0.30, Patience: 1/75            
Epoch [0114/0300], Training Loss: 0.0645, Validation Loss: 0.0690, Dropout p: 0.30, Patience: 2/75            
Epoch [0115/0300], Training Loss: 0.0554, Validation Loss: 0.0689, Dropout p: 0.30, Patience: 3/75            
Epoch [0116/0300], Training Loss: 0.0700, Validation Loss: 0.0773, Dropout p: 0.30, Patience: 4/75            
Epoch [0117/0300], Training Loss: 0.0769, Validation Loss: 0.0729, Dropout p: 0.30, Patience: 5/75            
Epoch [0118/0300], Training Loss: 0.0708, Validation Loss: 0.0653, Dropout p: 0.30, Patience: 6/75            
Epoch [0119/0300], Training Loss: 0.0903, Validation Loss: 0.0804, Dropout p: 0.30, Patience: 7/75            
Epoch [0120/0300], Training Loss: 0.0617, Validation Loss: 0.0724, Dropout p: 0.30, Patience: 8/75            
Epoch [0121/0300], Training Loss: 0.0710, Validation Loss: 0.0725, Dropout p: 0.30, Patience: 9/75            
Epoch [0122/0300], Training Loss: 0.0734, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 10/75            
Epoch [0123/0300], Training Loss: 0.0786, Validation Loss: 0.0807, Dropout p: 0.30, Patience: 11/75            
Epoch [0124/0300], Training Loss: 0.0710, Validation Loss: 0.0708, Dropout p: 0.30, Patience: 12/75            
Epoch [0125/0300], Training Loss: 0.0491, Validation Loss: 0.0685, Dropout p: 0.30, Patience: 13/75            
Epoch [0126/0300], Training Loss: 0.0589, Validation Loss: 0.0757, Dropout p: 0.30, Patience: 14/75            
Epoch [0127/0300], Training Loss: 0.0635, Validation Loss: 0.0632, Dropout p: 0.30, Patience: 15/75            
Epoch [0128/0300], Training Loss: 0.0583, Validation Loss: 0.0712, Dropout p: 0.30, Patience: 16/75            
Epoch [0129/0300], Training Loss: 0.0674, Validation Loss: 0.0740, Dropout p: 0.30, Patience: 17/75            
Epoch [0130/0300], Training Loss: 0.0646, Validation Loss: 0.0699, Dropout p: 0.30, Patience: 18/75            
Epoch [0131/0300], Training Loss: 0.0527, Validation Loss: 0.0787, Dropout p: 0.30, Patience: 19/75            
Epoch [0132/0300], Training Loss: 0.0552, Validation Loss: 0.0645, Dropout p: 0.30, Patience: 20/75            
Epoch [0133/0300], Training Loss: 0.0622, Validation Loss: 0.0668, Dropout p: 0.30, Patience: 21/75            
Epoch [0134/0300], Training Loss: 0.0584, Validation Loss: 0.0731, Dropout p: 0.30, Patience: 22/75            
Epoch [0135/0300], Training Loss: 0.0635, Validation Loss: 0.0767, Dropout p: 0.30, Patience: 23/75            
Epoch [0136/0300], Training Loss: 0.0747, Validation Loss: 0.0739, Dropout p: 0.30, Patience: 24/75            
Epoch [0137/0300], Training Loss: 0.0749, Validation Loss: 0.0688, Dropout p: 0.30, Patience: 25/75            
Epoch [0138/0300], Training Loss: 0.0517, Validation Loss: 0.0672, Dropout p: 0.30, Patience: 26/75            
Epoch [0139/0300], Training Loss: 0.0431, Validation Loss: 0.0852, Dropout p: 0.30, Patience: 27/75            
Epoch [0140/0300], Training Loss: 0.0506, Validation Loss: 0.0723, Dropout p: 0.30, Patience: 28/75            
Epoch [0141/0300], Training Loss: 0.0547, Validation Loss: 0.0714, Dropout p: 0.30, Patience: 29/75            
Epoch [0142/0300], Training Loss: 0.0651, Validation Loss: 0.0736, Dropout p: 0.30, Patience: 30/75            
Epoch [0143/0300], Training Loss: 0.0647, Validation Loss: 0.0702, Dropout p: 0.30, Patience: 31/75            
Epoch [0144/0300], Training Loss: 0.0653, Validation Loss: 0.0623, Dropout p: 0.30, Patience: 32/75            
Epoch [0145/0300], Training Loss: 0.0721, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 33/75            
Epoch [0146/0300], Training Loss: 0.0625, Validation Loss: 0.0786, Dropout p: 0.30, Patience: 34/75            
Epoch [0147/0300], Training Loss: 0.0705, Validation Loss: 0.0797, Dropout p: 0.30, Patience: 35/75            
Epoch [0148/0300], Training Loss: 0.0637, Validation Loss: 0.0756, Dropout p: 0.30, Patience: 36/75            
Epoch [0149/0300], Training Loss: 0.0543, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 37/75            
Epoch [0150/0300], Training Loss: 0.0547, Validation Loss: 0.0788, Dropout p: 0.30, Patience: 38/75            
Epoch [0151/0300], Training Loss: 0.0653, Validation Loss: 0.0774, Dropout p: 0.30, Patience: 39/75            
Epoch [0152/0300], Training Loss: 0.0623, Validation Loss: 0.0744, Dropout p: 0.30, Patience: 40/75            
Epoch [0153/0300], Training Loss: 0.0610, Validation Loss: 0.0646, Dropout p: 0.30, Patience: 41/75            
Epoch [0154/0300], Training Loss: 0.0483, Validation Loss: 0.0710, Dropout p: 0.30, Patience: 42/75            
Epoch [0155/0300], Training Loss: 0.0686, Validation Loss: 0.0821, Dropout p: 0.30, Patience: 43/75            
Epoch [0156/0300], Training Loss: 0.0575, Validation Loss: 0.0783, Dropout p: 0.30, Patience: 44/75            
Epoch [0157/0300], Training Loss: 0.0590, Validation Loss: 0.0742, Dropout p: 0.30, Patience: 45/75            
Epoch [0158/0300], Training Loss: 0.0823, Validation Loss: 0.0666, Dropout p: 0.30, Patience: 46/75            
Epoch [0159/0300], Training Loss: 0.0701, Validation Loss: 0.0671, Dropout p: 0.30, Patience: 47/75            
Epoch [0160/0300], Training Loss: 0.0646, Validation Loss: 0.0766, Dropout p: 0.30, Patience: 48/75            
Epoch [0161/0300], Training Loss: 0.0810, Validation Loss: 0.0676, Dropout p: 0.30, Patience: 49/75            
Epoch [0162/0300], Training Loss: 0.0607, Validation Loss: 0.0824, Dropout p: 0.30, Patience: 50/75            
Epoch [0163/0300], Training Loss: 0.0492, Validation Loss: 0.0796, Dropout p: 0.30, Patience: 51/75            
Epoch [0164/0300], Training Loss: 0.0541, Validation Loss: 0.0675, Dropout p: 0.30, Patience: 52/75            
Epoch [0165/0300], Training Loss: 0.0799, Validation Loss: 0.0831, Dropout p: 0.30, Patience: 53/75            
Epoch [0166/0300], Training Loss: 0.0717, Validation Loss: 0.0744, Dropout p: 0.30, Patience: 54/75            
Epoch [0167/0300], Training Loss: 0.0990, Validation Loss: 0.0703, Dropout p: 0.30, Patience: 55/75            
Epoch [0168/0300], Training Loss: 0.0513, Validation Loss: 0.0708, Dropout p: 0.30, Patience: 56/75            
Epoch [0169/0300], Training Loss: 0.0651, Validation Loss: 0.0705, Dropout p: 0.30, Patience: 57/75            
Epoch [0170/0300], Training Loss: 0.0563, Validation Loss: 0.0718, Dropout p: 0.30, Patience: 58/75            
Epoch [0171/0300], Training Loss: 0.0721, Validation Loss: 0.0704, Dropout p: 0.30, Patience: 59/75            
Epoch [0172/0300], Training Loss: 0.0608, Validation Loss: 0.0684, Dropout p: 0.30, Patience: 60/75            
Epoch [0173/0300], Training Loss: 0.0626, Validation Loss: 0.0703, Dropout p: 0.30, Patience: 61/75            
Epoch [0174/0300], Training Loss: 0.0752, Validation Loss: 0.0806, Dropout p: 0.30, Patience: 62/75            
Epoch [0175/0300], Training Loss: 0.0578, Validation Loss: 0.0717, Dropout p: 0.30, Patience: 63/75            
Epoch [0176/0300], Training Loss: 0.0486, Validation Loss: 0.0679, Dropout p: 0.30, Patience: 64/75            
Epoch [0177/0300], Training Loss: 0.0601, Validation Loss: 0.0741, Dropout p: 0.30, Patience: 65/75            
Epoch [0178/0300], Training Loss: 0.0871, Validation Loss: 0.0729, Dropout p: 0.30, Patience: 66/75            
Epoch [0179/0300], Training Loss: 0.0675, Validation Loss: 0.0695, Dropout p: 0.30, Patience: 67/75            
Epoch [0180/0300], Training Loss: 0.0834, Validation Loss: 0.0674, Dropout p: 0.30, Patience: 68/75            
Epoch [0181/0300], Training Loss: 0.0586, Validation Loss: 0.0776, Dropout p: 0.30, Patience: 69/75            
Epoch [0182/0300], Training Loss: 0.0665, Validation Loss: 0.0696, Dropout p: 0.30, Patience: 70/75            
Epoch [0183/0300], Training Loss: 0.0601, Validation Loss: 0.0758, Dropout p: 0.30, Patience: 71/75            
Epoch [0184/0300], Training Loss: 0.0606, Validation Loss: 0.0715, Dropout p: 0.30, Patience: 72/75            
Epoch [0185/0300], Training Loss: 0.0502, Validation Loss: 0.0695, Dropout p: 0.30, Patience: 73/75            
Epoch [0186/0300], Training Loss: 0.0700, Validation Loss: 0.0728, Dropout p: 0.30, Patience: 74/75            
Epoch [0187/0300], Training Loss: 0.0607, Validation Loss: 0.0819, Dropout p: 0.30, Patience: 75/75            

Evaluating model on test set...
Predicted TBV: 212.26	Actual TBV: 191.67	Difference: 20.59
Predicted TBV: 122.31	Actual TBV: 114.85	Difference: 7.46
Predicted TBV: 201.10	Actual TBV: 186.51	Difference: 14.59
Predicted TBV: 218.60	Actual TBV: 234.86	Difference: 16.26
Predicted TBV: 207.10	Actual TBV: 194.67	Difference: 12.43
Predicted TBV: 331.65	Actual TBV: 345.78	Difference: 14.13
Predicted TBV: 199.60	Actual TBV: 188.30	Difference: 11.30
Predicted TBV: 154.94	Actual TBV: 149.17	Difference: 5.77
Predicted TBV: 166.02	Actual TBV: 171.12	Difference: 5.10
Predicted TBV: 214.30	Actual TBV: 233.84	Difference: 19.54
Predicted TBV: 252.42	Actual TBV: 254.71	Difference: 2.29
Predicted TBV: 242.98	Actual TBV: 255.48	Difference: 12.50
Predicted TBV: 276.21	Actual TBV: 278.41	Difference: 2.20
Predicted TBV: 124.75	Actual TBV: 111.38	Difference: 13.37
Predicted TBV: 222.00	Actual TBV: 215.93	Difference: 6.07
Predicted TBV: 238.79	Actual TBV: 259.25	Difference: 20.46
Predicted TBV: 227.56	Actual TBV: 217.71	Difference: 9.85
Predicted TBV: 153.35	Actual TBV: 135.08	Difference: 18.27
Predicted TBV: 132.58	Actual TBV: 122.70	Difference: 9.88
Predicted TBV: 233.35	Actual TBV: 261.73	Difference: 28.38
Predicted TBV: 143.22	Actual TBV: 145.49	Difference: 2.27
Predicted TBV: 137.83	Actual TBV: 120.88	Difference: 16.95
Predicted TBV: 263.30	Actual TBV: 282.63	Difference: 19.33
Predicted TBV: 279.78	Actual TBV: 296.85	Difference: 17.07
Predicted TBV: 241.21	Actual TBV: 234.41	Difference: 6.80
Predicted TBV: 175.68	Actual TBV: 162.92	Difference: 12.76
Predicted TBV: 139.77	Actual TBV: 132.57	Difference: 7.20
Predicted TBV: 181.74	Actual TBV: 180.00	Difference: 1.74
Predicted TBV: 280.76	Actual TBV: 286.97	Difference: 6.21
Predicted TBV: 256.69	Actual TBV: 225.25	Difference: 31.44
Predicted TBV: 185.07	Actual TBV: 187.35	Difference: 2.28
Predicted TBV: 143.83	Actual TBV: 141.36	Difference: 2.47
Predicted TBV: 206.25	Actual TBV: 185.07	Difference: 21.18
Predicted TBV: 126.23	Actual TBV: 117.04	Difference: 9.19
Predicted TBV: 249.49	Actual TBV: 224.48	Difference: 25.01
Predicted TBV: 157.73	Actual TBV: 157.30	Difference: 0.43
Predicted TBV: 193.11	Actual TBV: 185.44	Difference: 7.67
Predicted TBV: 197.12	Actual TBV: 186.32	Difference: 10.80
Predicted TBV: 212.93	Actual TBV: 191.36	Difference: 21.57
Predicted TBV: 188.06	Actual TBV: 184.19	Difference: 3.87

Evaluating model with 30 Bayesian runs...

Evaluating model with transforms and dropout...
Refused raster with error 12.30. TBV: 14.13.
Refused raster with error 10.54. TBV: 5.10.
Refused raster with error 11.51. TBV: 2.29.
Refused raster with error 10.97. TBV: 12.50.
Refused raster with error 11.94. TBV: 18.27.
Refused raster with error 13.71. TBV: 19.33.
Refused raster with error 14.33. TBV: 2.28.
Refused raster with error 14.04. TBV: 7.67.
Refused raster with error 13.07. TBV: 29.50.
Refused raster with error 10.55. TBV: 2.53.
Refused raster with error 11.91. TBV: 7.61.

Evaluating model with transforms only...
Refused raster with error 5.63. TBV: 20.59.
Refused raster with error 5.93. TBV: 14.13.
Refused raster with error 5.69. TBV: 5.10.
Refused raster with error 5.81. TBV: 2.29.
Refused raster with error 8.33. TBV: 18.27.
Refused raster with error 6.61. TBV: 19.33.
Refused raster with error 5.85. TBV: 12.76.
Refused raster with error 7.37. TBV: 2.28.
Refused raster with error 6.58. TBV: 7.67.
Refused raster with error 5.97. TBV: 29.50.
Refused raster with error 7.38. TBV: 6.09.
Refused raster with error 8.43. TBV: 7.61.

Evaluating model with dropout only...
Refused raster with error 14.92. TBV: 14.13.
Refused raster with error 12.58. TBV: 2.29.
Refused raster with error 9.57. TBV: 18.27.
Refused raster with error 13.11. TBV: 19.33.
Refused raster with error 14.32. TBV: 2.28.
Refused raster with error 12.80. TBV: 7.67.
Refused raster with error 12.14. TBV: 29.50.
Refused raster with error 10.20. TBV: 16.61.
Refused raster with error 13.04. TBV: 7.04.
Refused raster with error 11.37. TBV: 2.71.

- - - - - -
Non-bayesian prediction:
Total Raster Count: 58
Mean Absolute Error: 11.17 cc
Standard Deviation: 7.82 cc
Big error count (>30): 1
Big error mean: 31.44 cc
Big error std: 0.00 cc
- - - - - -

Bayesian prediction Dropout + Transforms Bayes:
Refused Raster Count: 11
Refused Raster Percentage: 18.97%
Refusal threshold: 10.35
Mean Absolute Error: 11.21 cc
Standard Deviation: 7.70 cc
Big error count (>30): 1
Big error mean: 31.44 cc
Big error std: 0.00 cc
- - - - - -

Bayesian prediction Transforms Bayes:
Refused Raster Count: 12
Refused Raster Percentage: 20.69%
Refusal threshold: 5.625
Mean Absolute Error: 10.92 cc
Standard Deviation: 7.73 cc
Big error count (>30): 1
Big error mean: 31.44 cc
Big error std: 0.00 cc
- - - - - -

Bayesian prediction Dropout Bayes:
Refused Raster Count: 10
Refused Raster Percentage: 17.24%
Refusal threshold: 9.0
Mean Absolute Error: 11.00 cc
Standard Deviation: 7.63 cc
Big error count (>30): 1
Big error mean: 31.44 cc
Big error std: 0.00 cc
- - - - - -

 - - - Fold 4/6 - - -

Train volume count: 296
Validation volume count: 74
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '29' '7' '41' '33' '76' '30' '87' '77' '46' '45' '50' '18' '32' '21'
 '85' '3' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.0741, Validation Loss: 1.0525, Dropout p: 0.30, Patience: 0/75            
Epoch [0002/0300], Training Loss: 0.9628, Validation Loss: 1.6557, Dropout p: 0.30, Patience: 0/75            
Epoch [0003/0300], Training Loss: 0.8868, Validation Loss: 1.0078, Dropout p: 0.30, Patience: 0/75            
Epoch [0004/0300], Training Loss: 0.7396, Validation Loss: 1.2992, Dropout p: 0.30, Patience: 0/75            
Epoch [0005/0300], Training Loss: 0.6361, Validation Loss: 0.5572, Dropout p: 0.30, Patience: 0/75            
Epoch [0006/0300], Training Loss: 0.5396, Validation Loss: 0.5146, Dropout p: 0.30, Patience: 0/75            
Epoch [0007/0300], Training Loss: 0.4911, Validation Loss: 0.3726, Dropout p: 0.30, Patience: 0/75            
Epoch [0008/0300], Training Loss: 0.4216, Validation Loss: 0.3138, Dropout p: 0.30, Patience: 0/75            
Epoch [0009/0300], Training Loss: 0.4510, Validation Loss: 0.2973, Dropout p: 0.30, Patience: 0/75            
Epoch [0010/0300], Training Loss: 0.4660, Validation Loss: 0.2927, Dropout p: 0.30, Patience: 0/75            
Epoch [0011/0300], Training Loss: 0.3882, Validation Loss: 0.2179, Dropout p: 0.30, Patience: 0/75            
Epoch [0012/0300], Training Loss: 0.3863, Validation Loss: 0.2477, Dropout p: 0.30, Patience: 0/75            
Epoch [0013/0300], Training Loss: 0.3417, Validation Loss: 0.3942, Dropout p: 0.30, Patience: 0/75            
Epoch [0014/0300], Training Loss: 0.3563, Validation Loss: 0.2758, Dropout p: 0.30, Patience: 0/75            
Epoch [0015/0300], Training Loss: 0.3811, Validation Loss: 0.2658, Dropout p: 0.30, Patience: 0/75            
Epoch [0016/0300], Training Loss: 0.3290, Validation Loss: 0.5045, Dropout p: 0.30, Patience: 0/75            
Epoch [0017/0300], Training Loss: 0.2827, Validation Loss: 0.2165, Dropout p: 0.30, Patience: 0/75            
Epoch [0018/0300], Training Loss: 0.3151, Validation Loss: 0.2492, Dropout p: 0.30, Patience: 0/75            
Epoch [0019/0300], Training Loss: 0.4027, Validation Loss: 0.2043, Dropout p: 0.30, Patience: 0/75            
Epoch [0020/0300], Training Loss: 0.3052, Validation Loss: 0.2140, Dropout p: 0.30, Patience: 0/75            
Epoch [0021/0300], Training Loss: 0.3726, Validation Loss: 0.2534, Dropout p: 0.30, Patience: 0/75            
Epoch [0022/0300], Training Loss: 0.3303, Validation Loss: 0.3344, Dropout p: 0.30, Patience: 0/75            
Epoch [0023/0300], Training Loss: 0.3023, Validation Loss: 0.2771, Dropout p: 0.30, Patience: 0/75            
Epoch [0024/0300], Training Loss: 0.2795, Validation Loss: 0.1652, Dropout p: 0.30, Patience: 0/75            
Epoch [0025/0300], Training Loss: 0.2468, Validation Loss: 0.2661, Dropout p: 0.30, Patience: 0/75            
Epoch [0026/0300], Training Loss: 0.2949, Validation Loss: 0.2143, Dropout p: 0.30, Patience: 0/75            
Epoch [0027/0300], Training Loss: 0.2420, Validation Loss: 0.3338, Dropout p: 0.30, Patience: 0/75            
Epoch [0028/0300], Training Loss: 0.3337, Validation Loss: 0.1693, Dropout p: 0.30, Patience: 0/75            
Epoch [0029/0300], Training Loss: 0.1922, Validation Loss: 0.3154, Dropout p: 0.30, Patience: 0/75            
Epoch [0030/0300], Training Loss: 0.2657, Validation Loss: 0.2384, Dropout p: 0.30, Patience: 0/75            
Epoch [0031/0300], Training Loss: 0.2188, Validation Loss: 0.1910, Dropout p: 0.30, Patience: 0/75            
Epoch [0032/0300], Training Loss: 0.2036, Validation Loss: 0.2316, Dropout p: 0.30, Patience: 0/75            
Epoch [0033/0300], Training Loss: 0.2284, Validation Loss: 0.2138, Dropout p: 0.30, Patience: 0/75            
Epoch [0034/0300], Training Loss: 0.2066, Validation Loss: 0.1940, Dropout p: 0.30, Patience: 0/75            
Epoch [0035/0300], Training Loss: 0.2214, Validation Loss: 0.2380, Dropout p: 0.30, Patience: 0/75            
Epoch [0036/0300], Training Loss: 0.1932, Validation Loss: 0.1940, Dropout p: 0.30, Patience: 0/75            
Epoch [0037/0300], Training Loss: 0.2210, Validation Loss: 0.1623, Dropout p: 0.30, Patience: 0/75            
Epoch [0038/0300], Training Loss: 0.2594, Validation Loss: 0.1805, Dropout p: 0.30, Patience: 0/75            
Epoch [0039/0300], Training Loss: 0.1832, Validation Loss: 0.1679, Dropout p: 0.30, Patience: 0/75            
Epoch [0040/0300], Training Loss: 0.1663, Validation Loss: 0.1653, Dropout p: 0.30, Patience: 0/75            
Epoch [0041/0300], Training Loss: 0.1697, Validation Loss: 0.2271, Dropout p: 0.30, Patience: 0/75            
Epoch [0042/0300], Training Loss: 0.1822, Validation Loss: 0.1544, Dropout p: 0.30, Patience: 0/75            
Epoch [0043/0300], Training Loss: 0.1486, Validation Loss: 0.1945, Dropout p: 0.30, Patience: 0/75            
Epoch [0044/0300], Training Loss: 0.1699, Validation Loss: 0.1695, Dropout p: 0.30, Patience: 0/75            
Epoch [0045/0300], Training Loss: 0.1638, Validation Loss: 0.1651, Dropout p: 0.30, Patience: 0/75            
Epoch [0046/0300], Training Loss: 0.1383, Validation Loss: 0.1927, Dropout p: 0.30, Patience: 0/75            
Epoch [0047/0300], Training Loss: 0.1687, Validation Loss: 0.1664, Dropout p: 0.30, Patience: 0/75            
Epoch [0048/0300], Training Loss: 0.2221, Validation Loss: 0.1760, Dropout p: 0.30, Patience: 0/75            
Epoch [0049/0300], Training Loss: 0.2019, Validation Loss: 0.1785, Dropout p: 0.30, Patience: 0/75            
Epoch [0050/0300], Training Loss: 0.1517, Validation Loss: 0.2703, Dropout p: 0.30, Patience: 0/75            
Epoch [0051/0300], Training Loss: 0.1806, Validation Loss: 0.1297, Dropout p: 0.30, Patience: 0/75            
Epoch [0052/0300], Training Loss: 0.1550, Validation Loss: 0.1264, Dropout p: 0.30, Patience: 0/75            
Epoch [0053/0300], Training Loss: 0.1334, Validation Loss: 0.1349, Dropout p: 0.30, Patience: 0/75            
Epoch [0054/0300], Training Loss: 0.1460, Validation Loss: 0.1643, Dropout p: 0.30, Patience: 0/75            
Epoch [0055/0300], Training Loss: 0.1297, Validation Loss: 0.1634, Dropout p: 0.30, Patience: 0/75            
Epoch [0056/0300], Training Loss: 0.1204, Validation Loss: 0.1710, Dropout p: 0.30, Patience: 0/75            
Epoch [0057/0300], Training Loss: 0.1396, Validation Loss: 0.1251, Dropout p: 0.30, Patience: 0/75            
Epoch [0058/0300], Training Loss: 0.1330, Validation Loss: 0.1463, Dropout p: 0.30, Patience: 0/75            
Epoch [0059/0300], Training Loss: 0.1110, Validation Loss: 0.1664, Dropout p: 0.30, Patience: 0/75            
Epoch [0060/0300], Training Loss: 0.1605, Validation Loss: 0.1178, Dropout p: 0.30, Patience: 0/75            
Epoch [0061/0300], Training Loss: 0.1227, Validation Loss: 0.1290, Dropout p: 0.30, Patience: 0/75            
Epoch [0062/0300], Training Loss: 0.1506, Validation Loss: 0.1634, Dropout p: 0.30, Patience: 0/75            
Epoch [0063/0300], Training Loss: 0.1460, Validation Loss: 0.1533, Dropout p: 0.30, Patience: 0/75            
Epoch [0064/0300], Training Loss: 0.1670, Validation Loss: 0.1583, Dropout p: 0.30, Patience: 0/75            
Epoch [0065/0300], Training Loss: 0.1405, Validation Loss: 0.1639, Dropout p: 0.30, Patience: 0/75            
Epoch [0066/0300], Training Loss: 0.1274, Validation Loss: 0.1850, Dropout p: 0.30, Patience: 0/75            
Epoch [0067/0300], Training Loss: 0.1421, Validation Loss: 0.1152, Dropout p: 0.30, Patience: 0/75            
Epoch [0068/0300], Training Loss: 0.1236, Validation Loss: 0.1503, Dropout p: 0.30, Patience: 0/75            
Epoch [0069/0300], Training Loss: 0.1188, Validation Loss: 0.1618, Dropout p: 0.30, Patience: 0/75            
Epoch [0070/0300], Training Loss: 0.1208, Validation Loss: 0.1430, Dropout p: 0.30, Patience: 0/75            
Epoch [0071/0300], Training Loss: 0.1043, Validation Loss: 0.1137, Dropout p: 0.30, Patience: 0/75            
Epoch [0072/0300], Training Loss: 0.0769, Validation Loss: 0.1156, Dropout p: 0.30, Patience: 0/75            
Epoch [0073/0300], Training Loss: 0.0949, Validation Loss: 0.1160, Dropout p: 0.30, Patience: 0/75            
Epoch [0074/0300], Training Loss: 0.0716, Validation Loss: 0.1234, Dropout p: 0.30, Patience: 0/75            
Epoch [0075/0300], Training Loss: 0.0748, Validation Loss: 0.1059, Dropout p: 0.30, Patience: 0/75            
Epoch [0076/0300], Training Loss: 0.1041, Validation Loss: 0.1084, Dropout p: 0.30, Patience: 0/75            
Epoch [0077/0300], Training Loss: 0.0948, Validation Loss: 0.1024, Dropout p: 0.30, Patience: 0/75            
Epoch [0078/0300], Training Loss: 0.0895, Validation Loss: 0.1128, Dropout p: 0.30, Patience: 0/75            
Epoch [0079/0300], Training Loss: 0.0932, Validation Loss: 0.0924, Dropout p: 0.30, Patience: 0/75            
Epoch [0080/0300], Training Loss: 0.0852, Validation Loss: 0.1012, Dropout p: 0.30, Patience: 0/75            
Epoch [0081/0300], Training Loss: 0.0793, Validation Loss: 0.1079, Dropout p: 0.30, Patience: 0/75            
Epoch [0082/0300], Training Loss: 0.0707, Validation Loss: 0.1302, Dropout p: 0.30, Patience: 0/75            
Epoch [0083/0300], Training Loss: 0.1016, Validation Loss: 0.1363, Dropout p: 0.30, Patience: 0/75            
Epoch [0084/0300], Training Loss: 0.0782, Validation Loss: 0.1336, Dropout p: 0.30, Patience: 0/75            
Epoch [0085/0300], Training Loss: 0.0744, Validation Loss: 0.1065, Dropout p: 0.30, Patience: 0/75            
Epoch [0086/0300], Training Loss: 0.0772, Validation Loss: 0.0973, Dropout p: 0.30, Patience: 0/75            
Epoch [0087/0300], Training Loss: 0.0788, Validation Loss: 0.1151, Dropout p: 0.30, Patience: 0/75            
Epoch [0088/0300], Training Loss: 0.0953, Validation Loss: 0.1174, Dropout p: 0.30, Patience: 0/75            
Epoch [0089/0300], Training Loss: 0.0844, Validation Loss: 0.1146, Dropout p: 0.30, Patience: 0/75            
Epoch [0090/0300], Training Loss: 0.0828, Validation Loss: 0.1119, Dropout p: 0.30, Patience: 0/75            
Epoch [0091/0300], Training Loss: 0.0789, Validation Loss: 0.1274, Dropout p: 0.30, Patience: 0/75            
Epoch [0092/0300], Training Loss: 0.0772, Validation Loss: 0.1453, Dropout p: 0.30, Patience: 0/75            
Epoch [0093/0300], Training Loss: 0.0949, Validation Loss: 0.1291, Dropout p: 0.30, Patience: 0/75            
Epoch [0094/0300], Training Loss: 0.0767, Validation Loss: 0.1236, Dropout p: 0.30, Patience: 0/75            
Epoch [0095/0300], Training Loss: 0.0659, Validation Loss: 0.1411, Dropout p: 0.30, Patience: 0/75            
Epoch [0096/0300], Training Loss: 0.0838, Validation Loss: 0.1206, Dropout p: 0.30, Patience: 0/75            
Epoch [0097/0300], Training Loss: 0.0716, Validation Loss: 0.1325, Dropout p: 0.30, Patience: 0/75            
Epoch [0098/0300], Training Loss: 0.0700, Validation Loss: 0.1183, Dropout p: 0.30, Patience: 0/75            
Epoch [0099/0300], Training Loss: 0.0713, Validation Loss: 0.1243, Dropout p: 0.30, Patience: 0/75            
Epoch [0100/0300], Training Loss: 0.0686, Validation Loss: 0.1262, Dropout p: 0.30, Patience: 0/75            
Epoch [0101/0300], Training Loss: 0.0675, Validation Loss: 0.1220, Dropout p: 0.30, Patience: 0/75            
Epoch [0102/0300], Training Loss: 0.0658, Validation Loss: 0.1074, Dropout p: 0.30, Patience: 0/75            
Epoch [0103/0300], Training Loss: 0.0679, Validation Loss: 0.1123, Dropout p: 0.30, Patience: 1/75            
Epoch [0104/0300], Training Loss: 0.0625, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 0/75            
Epoch [0105/0300], Training Loss: 0.0541, Validation Loss: 0.1088, Dropout p: 0.30, Patience: 1/75            
Epoch [0106/0300], Training Loss: 0.0630, Validation Loss: 0.1048, Dropout p: 0.30, Patience: 0/75            
Epoch [0107/0300], Training Loss: 0.0563, Validation Loss: 0.1118, Dropout p: 0.30, Patience: 1/75            
Epoch [0108/0300], Training Loss: 0.0734, Validation Loss: 0.1064, Dropout p: 0.30, Patience: 2/75            
Epoch [0109/0300], Training Loss: 0.0675, Validation Loss: 0.1117, Dropout p: 0.30, Patience: 3/75            
Epoch [0110/0300], Training Loss: 0.0563, Validation Loss: 0.1116, Dropout p: 0.30, Patience: 4/75            
Epoch [0111/0300], Training Loss: 0.0743, Validation Loss: 0.1407, Dropout p: 0.30, Patience: 5/75            
Epoch [0112/0300], Training Loss: 0.0573, Validation Loss: 0.1141, Dropout p: 0.30, Patience: 6/75            
Epoch [0113/0300], Training Loss: 0.0692, Validation Loss: 0.1148, Dropout p: 0.30, Patience: 7/75            
Epoch [0114/0300], Training Loss: 0.0591, Validation Loss: 0.1087, Dropout p: 0.30, Patience: 8/75            
Epoch [0115/0300], Training Loss: 0.0640, Validation Loss: 0.1128, Dropout p: 0.30, Patience: 9/75            
Epoch [0116/0300], Training Loss: 0.0783, Validation Loss: 0.1023, Dropout p: 0.30, Patience: 0/75            
Epoch [0117/0300], Training Loss: 0.0549, Validation Loss: 0.1121, Dropout p: 0.30, Patience: 1/75            
Epoch [0118/0300], Training Loss: 0.0597, Validation Loss: 0.1084, Dropout p: 0.30, Patience: 2/75            
Epoch [0119/0300], Training Loss: 0.0553, Validation Loss: 0.1088, Dropout p: 0.30, Patience: 3/75            
Epoch [0120/0300], Training Loss: 0.0492, Validation Loss: 0.1162, Dropout p: 0.30, Patience: 4/75            
Epoch [0121/0300], Training Loss: 0.0609, Validation Loss: 0.1160, Dropout p: 0.30, Patience: 5/75            
Epoch [0122/0300], Training Loss: 0.0592, Validation Loss: 0.1074, Dropout p: 0.30, Patience: 6/75            
Epoch [0123/0300], Training Loss: 0.0533, Validation Loss: 0.1190, Dropout p: 0.30, Patience: 7/75            
Epoch [0124/0300], Training Loss: 0.0613, Validation Loss: 0.1114, Dropout p: 0.30, Patience: 8/75            
Epoch [0125/0300], Training Loss: 0.0556, Validation Loss: 0.1128, Dropout p: 0.30, Patience: 9/75            
Epoch [0126/0300], Training Loss: 0.0595, Validation Loss: 0.1154, Dropout p: 0.30, Patience: 10/75            
Epoch [0127/0300], Training Loss: 0.0579, Validation Loss: 0.1145, Dropout p: 0.30, Patience: 11/75            
Epoch [0128/0300], Training Loss: 0.0512, Validation Loss: 0.1087, Dropout p: 0.30, Patience: 12/75            
Epoch [0129/0300], Training Loss: 0.0627, Validation Loss: 0.1080, Dropout p: 0.30, Patience: 13/75            
Epoch [0130/0300], Training Loss: 0.0524, Validation Loss: 0.1157, Dropout p: 0.30, Patience: 14/75            
Epoch [0131/0300], Training Loss: 0.0601, Validation Loss: 0.1107, Dropout p: 0.30, Patience: 15/75            
Epoch [0132/0300], Training Loss: 0.0441, Validation Loss: 0.1114, Dropout p: 0.30, Patience: 16/75            
Epoch [0133/0300], Training Loss: 0.0509, Validation Loss: 0.1098, Dropout p: 0.30, Patience: 17/75            
Epoch [0134/0300], Training Loss: 0.0553, Validation Loss: 0.1097, Dropout p: 0.30, Patience: 18/75            
Epoch [0135/0300], Training Loss: 0.0696, Validation Loss: 0.1099, Dropout p: 0.30, Patience: 19/75            
Epoch [0136/0300], Training Loss: 0.0518, Validation Loss: 0.1147, Dropout p: 0.30, Patience: 20/75            
Epoch [0137/0300], Training Loss: 0.0552, Validation Loss: 0.1121, Dropout p: 0.30, Patience: 21/75            
Epoch [0138/0300], Training Loss: 0.0622, Validation Loss: 0.1071, Dropout p: 0.30, Patience: 22/75            
Epoch [0139/0300], Training Loss: 0.0656, Validation Loss: 0.1111, Dropout p: 0.30, Patience: 23/75            
Epoch [0140/0300], Training Loss: 0.0499, Validation Loss: 0.1098, Dropout p: 0.30, Patience: 24/75            
Epoch [0141/0300], Training Loss: 0.0512, Validation Loss: 0.1061, Dropout p: 0.30, Patience: 25/75            
Epoch [0142/0300], Training Loss: 0.0667, Validation Loss: 0.1118, Dropout p: 0.30, Patience: 26/75            
Epoch [0143/0300], Training Loss: 0.0509, Validation Loss: 0.1102, Dropout p: 0.30, Patience: 27/75            
Epoch [0144/0300], Training Loss: 0.0523, Validation Loss: 0.1064, Dropout p: 0.30, Patience: 28/75            
Epoch [0145/0300], Training Loss: 0.0564, Validation Loss: 0.1113, Dropout p: 0.30, Patience: 29/75            
Epoch [0146/0300], Training Loss: 0.0587, Validation Loss: 0.1048, Dropout p: 0.30, Patience: 30/75            
Epoch [0147/0300], Training Loss: 0.0721, Validation Loss: 0.1080, Dropout p: 0.30, Patience: 31/75            
Epoch [0148/0300], Training Loss: 0.0583, Validation Loss: 0.1038, Dropout p: 0.30, Patience: 32/75            
Epoch [0149/0300], Training Loss: 0.0643, Validation Loss: 0.1047, Dropout p: 0.30, Patience: 33/75            
Epoch [0150/0300], Training Loss: 0.0447, Validation Loss: 0.1046, Dropout p: 0.30, Patience: 34/75            
Epoch [0151/0300], Training Loss: 0.0540, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 35/75            
Epoch [0152/0300], Training Loss: 0.0467, Validation Loss: 0.1078, Dropout p: 0.30, Patience: 36/75            
Epoch [0153/0300], Training Loss: 0.0500, Validation Loss: 0.1083, Dropout p: 0.30, Patience: 37/75            
Epoch [0154/0300], Training Loss: 0.0566, Validation Loss: 0.1072, Dropout p: 0.30, Patience: 38/75            
Epoch [0155/0300], Training Loss: 0.0574, Validation Loss: 0.1057, Dropout p: 0.30, Patience: 39/75            
Epoch [0156/0300], Training Loss: 0.0481, Validation Loss: 0.1055, Dropout p: 0.30, Patience: 40/75            
Epoch [0157/0300], Training Loss: 0.0530, Validation Loss: 0.1102, Dropout p: 0.30, Patience: 41/75            
Epoch [0158/0300], Training Loss: 0.0602, Validation Loss: 0.1072, Dropout p: 0.30, Patience: 42/75            
Epoch [0159/0300], Training Loss: 0.0449, Validation Loss: 0.1091, Dropout p: 0.30, Patience: 43/75            
Epoch [0160/0300], Training Loss: 0.0566, Validation Loss: 0.1112, Dropout p: 0.30, Patience: 44/75            
Epoch [0161/0300], Training Loss: 0.0601, Validation Loss: 0.1080, Dropout p: 0.30, Patience: 45/75            
Epoch [0162/0300], Training Loss: 0.0852, Validation Loss: 0.1086, Dropout p: 0.30, Patience: 46/75            
Epoch [0163/0300], Training Loss: 0.0598, Validation Loss: 0.1054, Dropout p: 0.30, Patience: 47/75            
Epoch [0164/0300], Training Loss: 0.0560, Validation Loss: 0.1055, Dropout p: 0.30, Patience: 48/75            
Epoch [0165/0300], Training Loss: 0.0809, Validation Loss: 0.1109, Dropout p: 0.30, Patience: 49/75            
Epoch [0166/0300], Training Loss: 0.0544, Validation Loss: 0.1077, Dropout p: 0.30, Patience: 50/75            
Epoch [0167/0300], Training Loss: 0.0505, Validation Loss: 0.1096, Dropout p: 0.30, Patience: 51/75            
Epoch [0168/0300], Training Loss: 0.0570, Validation Loss: 0.1104, Dropout p: 0.30, Patience: 52/75            
Epoch [0169/0300], Training Loss: 0.0663, Validation Loss: 0.1061, Dropout p: 0.30, Patience: 53/75            
Epoch [0170/0300], Training Loss: 0.0703, Validation Loss: 0.1089, Dropout p: 0.30, Patience: 54/75            
Epoch [0171/0300], Training Loss: 0.0434, Validation Loss: 0.1071, Dropout p: 0.30, Patience: 55/75            
Epoch [0172/0300], Training Loss: 0.0577, Validation Loss: 0.1109, Dropout p: 0.30, Patience: 56/75            
Epoch [0173/0300], Training Loss: 0.0549, Validation Loss: 0.1091, Dropout p: 0.30, Patience: 57/75            
Epoch [0174/0300], Training Loss: 0.0545, Validation Loss: 0.1088, Dropout p: 0.30, Patience: 58/75            
Epoch [0175/0300], Training Loss: 0.0686, Validation Loss: 0.1088, Dropout p: 0.30, Patience: 59/75            
Epoch [0176/0300], Training Loss: 0.0497, Validation Loss: 0.1078, Dropout p: 0.30, Patience: 60/75            
Epoch [0177/0300], Training Loss: 0.0550, Validation Loss: 0.1068, Dropout p: 0.30, Patience: 61/75            
Epoch [0178/0300], Training Loss: 0.0633, Validation Loss: 0.1107, Dropout p: 0.30, Patience: 62/75            
Epoch [0179/0300], Training Loss: 0.0396, Validation Loss: 0.1101, Dropout p: 0.30, Patience: 63/75            
Epoch [0180/0300], Training Loss: 0.0544, Validation Loss: 0.1074, Dropout p: 0.30, Patience: 64/75            
Epoch [0181/0300], Training Loss: 0.0581, Validation Loss: 0.1105, Dropout p: 0.30, Patience: 65/75            
Epoch [0182/0300], Training Loss: 0.0624, Validation Loss: 0.1064, Dropout p: 0.30, Patience: 66/75            
Epoch [0183/0300], Training Loss: 0.0542, Validation Loss: 0.1114, Dropout p: 0.30, Patience: 67/75            
Epoch [0184/0300], Training Loss: 0.0581, Validation Loss: 0.1087, Dropout p: 0.30, Patience: 68/75            
Epoch [0185/0300], Training Loss: 0.0502, Validation Loss: 0.1063, Dropout p: 0.30, Patience: 69/75            
Epoch [0186/0300], Training Loss: 0.0635, Validation Loss: 0.1063, Dropout p: 0.30, Patience: 70/75            
Epoch [0187/0300], Training Loss: 0.0443, Validation Loss: 0.1118, Dropout p: 0.30, Patience: 71/75            
Epoch [0188/0300], Training Loss: 0.0554, Validation Loss: 0.1083, Dropout p: 0.30, Patience: 72/75            
Epoch [0189/0300], Training Loss: 0.0511, Validation Loss: 0.1058, Dropout p: 0.30, Patience: 73/75            
Epoch [0190/0300], Training Loss: 0.0569, Validation Loss: 0.1107, Dropout p: 0.30, Patience: 74/75            
Epoch [0191/0300], Training Loss: 0.0843, Validation Loss: 0.1087, Dropout p: 0.30, Patience: 75/75            

Evaluating model on test set...
Predicted TBV: 267.70	Actual TBV: 274.21	Difference: 6.51
Predicted TBV: 309.80	Actual TBV: 297.34	Difference: 12.46
Predicted TBV: 206.73	Actual TBV: 193.54	Difference: 13.19
Predicted TBV: 275.04	Actual TBV: 288.02	Difference: 12.98
Predicted TBV: 243.83	Actual TBV: 217.43	Difference: 26.40
Predicted TBV: 255.11	Actual TBV: 244.76	Difference: 10.35
Predicted TBV: 223.76	Actual TBV: 227.86	Difference: 4.10
Predicted TBV: 219.80	Actual TBV: 216.03	Difference: 3.77
Predicted TBV: 321.09	Actual TBV: 287.25	Difference: 33.84
Predicted TBV: 286.66	Actual TBV: 254.41	Difference: 32.25
Predicted TBV: 186.81	Actual TBV: 177.02	Difference: 9.79
Predicted TBV: 313.96	Actual TBV: 318.79	Difference: 4.83
Predicted TBV: 228.06	Actual TBV: 220.38	Difference: 7.68
Predicted TBV: 314.13	Actual TBV: 301.98	Difference: 12.15
Predicted TBV: 169.97	Actual TBV: 174.04	Difference: 4.07
Predicted TBV: 226.17	Actual TBV: 241.29	Difference: 15.12
Predicted TBV: 210.59	Actual TBV: 214.35	Difference: 3.76
Predicted TBV: 198.80	Actual TBV: 208.83	Difference: 10.03
Predicted TBV: 195.53	Actual TBV: 181.75	Difference: 13.78
Predicted TBV: 232.30	Actual TBV: 238.17	Difference: 5.87
Predicted TBV: 170.89	Actual TBV: 152.74	Difference: 18.15
Predicted TBV: 131.08	Actual TBV: 118.78	Difference: 12.30
Predicted TBV: 367.68	Actual TBV: 330.32	Difference: 37.36
Predicted TBV: 215.20	Actual TBV: 219.78	Difference: 4.58
Predicted TBV: 184.62	Actual TBV: 189.84	Difference: 5.22
Predicted TBV: 114.87	Actual TBV: 103.13	Difference: 11.74
Predicted TBV: 203.38	Actual TBV: 207.10	Difference: 3.72
Predicted TBV: 258.80	Actual TBV: 256.54	Difference: 2.26
Predicted TBV: 243.66	Actual TBV: 236.15	Difference: 7.51
Predicted TBV: 248.31	Actual TBV: 219.26	Difference: 29.05
Predicted TBV: 263.73	Actual TBV: 269.49	Difference: 5.76
Predicted TBV: 196.95	Actual TBV: 199.59	Difference: 2.64
Predicted TBV: 182.47	Actual TBV: 180.37	Difference: 2.10
Predicted TBV: 205.71	Actual TBV: 209.36	Difference: 3.65
Predicted TBV: 192.49	Actual TBV: 182.50	Difference: 9.99
Predicted TBV: 200.81	Actual TBV: 208.69	Difference: 7.88
Predicted TBV: 195.24	Actual TBV: 205.68	Difference: 10.44
Predicted TBV: 265.38	Actual TBV: 274.21	Difference: 8.83
Predicted TBV: 167.17	Actual TBV: 173.22	Difference: 6.05
Predicted TBV: 169.20	Actual TBV: 178.10	Difference: 8.90

Evaluating model with 30 Bayesian runs...

Evaluating model with transforms and dropout...
Refused raster with error 11.58. TBV: 26.40.
Refused raster with error 11.30. TBV: 3.77.
Refused raster with error 14.24. TBV: 33.84.
Refused raster with error 11.45. TBV: 7.68.
Refused raster with error 12.20. TBV: 12.15.
Refused raster with error 10.91. TBV: 37.36.
Refused raster with error 12.63. TBV: 5.76.
Refused raster with error 10.88. TBV: 8.83.
Refused raster with error 11.63. TBV: 14.70.
Refused raster with error 11.15. TBV: 26.17.
Refused raster with error 10.65. TBV: 0.21.
Refused raster with error 17.65. TBV: 31.58.
Refused raster with error 13.50. TBV: 10.59.
Refused raster with error 11.37. TBV: 9.59.
Refused raster with error 13.19. TBV: 11.02.
Refused raster with error 11.83. TBV: 22.91.
Refused raster with error 10.50. TBV: 18.35.

Evaluating model with transforms only...
Refused raster with error 6.28. TBV: 26.40.
Refused raster with error 9.14. TBV: 3.77.
Refused raster with error 6.90. TBV: 32.25.
Refused raster with error 8.87. TBV: 12.15.
Refused raster with error 5.69. TBV: 10.44.
Refused raster with error 8.91. TBV: 6.05.
Refused raster with error 8.61. TBV: 26.17.
Refused raster with error 6.93. TBV: 21.40.
Refused raster with error 11.25. TBV: 31.58.
Refused raster with error 5.72. TBV: 9.59.
Refused raster with error 6.23. TBV: 11.02.
Refused raster with error 5.88. TBV: 18.35.

Evaluating model with dropout only...
Refused raster with error 14.18. TBV: 6.51.
Refused raster with error 10.67. TBV: 26.40.
Refused raster with error 14.70. TBV: 33.84.
Refused raster with error 9.50. TBV: 12.15.
Refused raster with error 10.29. TBV: 5.76.
Refused raster with error 13.05. TBV: 8.83.
Refused raster with error 10.08. TBV: 14.70.
Refused raster with error 10.42. TBV: 10.59.
Refused raster with error 9.63. TBV: 9.59.
Refused raster with error 10.67. TBV: 11.02.

- - - - - -
Non-bayesian prediction:
Total Raster Count: 74
Mean Absolute Error: 12.26 cc
Standard Deviation: 9.31 cc
Big error count (>30): 6
Big error mean: 33.56 cc
Big error std: 2.35 cc
- - - - - -

Bayesian prediction Dropout + Transforms Bayes:
Refused Raster Count: 17
Refused Raster Percentage: 22.97%
Refusal threshold: 10.35
Mean Absolute Error: 10.98 cc
Standard Deviation: 8.38 cc
Big error count (>30): 3
Big error mean: 32.86 cc
Big error std: 2.09 cc
- - - - - -

Bayesian prediction Transforms Bayes:
Refused Raster Count: 12
Refused Raster Percentage: 16.22%
Refusal threshold: 5.625
Mean Absolute Error: 11.26 cc
Standard Deviation: 8.93 cc
Big error count (>30): 4
Big error mean: 34.38 cc
Big error std: 2.48 cc
- - - - - -

Bayesian prediction Dropout Bayes:
Refused Raster Count: 10
Refused Raster Percentage: 13.51%
Refusal threshold: 9.0
Mean Absolute Error: 11.99 cc
Standard Deviation: 9.39 cc
Big error count (>30): 5
Big error mean: 33.51 cc
Big error std: 2.57 cc
- - - - - -

 - - - Fold 5/6 - - -

Train volume count: 321
Validation volume count: 49
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '29' '7' '41' '33' '76' '30' '87' '77' '79' '43' '47' '35' '16' '44'
 '26' '81' '19' '2' '20' '17' '49' '42' '84' '39' '86']

Epoch [0001/0300], Training Loss: 1.1484, Validation Loss: 0.4366, Dropout p: 0.30, Patience: 0/75            
Epoch [0002/0300], Training Loss: 1.0776, Validation Loss: 0.4108, Dropout p: 0.30, Patience: 0/75            
Epoch [0003/0300], Training Loss: 0.9397, Validation Loss: 0.3175, Dropout p: 0.30, Patience: 0/75            
Epoch [0004/0300], Training Loss: 0.7545, Validation Loss: 0.5738, Dropout p: 0.30, Patience: 0/75            
Epoch [0005/0300], Training Loss: 0.6203, Validation Loss: 0.7557, Dropout p: 0.30, Patience: 0/75            
Epoch [0006/0300], Training Loss: 0.6909, Validation Loss: 0.1403, Dropout p: 0.30, Patience: 0/75            
Epoch [0007/0300], Training Loss: 0.5788, Validation Loss: 0.5839, Dropout p: 0.30, Patience: 0/75            
Epoch [0008/0300], Training Loss: 0.4505, Validation Loss: 0.1296, Dropout p: 0.30, Patience: 0/75            
Epoch [0009/0300], Training Loss: 0.4744, Validation Loss: 0.1189, Dropout p: 0.30, Patience: 0/75            
Epoch [0010/0300], Training Loss: 0.3991, Validation Loss: 0.1716, Dropout p: 0.30, Patience: 0/75            
Epoch [0011/0300], Training Loss: 0.4531, Validation Loss: 0.1062, Dropout p: 0.30, Patience: 0/75            
Epoch [0012/0300], Training Loss: 0.3928, Validation Loss: 0.2068, Dropout p: 0.30, Patience: 0/75            
Epoch [0013/0300], Training Loss: 0.4103, Validation Loss: 0.0861, Dropout p: 0.30, Patience: 0/75            
Epoch [0014/0300], Training Loss: 0.2853, Validation Loss: 0.1991, Dropout p: 0.30, Patience: 0/75            
Epoch [0015/0300], Training Loss: 0.3933, Validation Loss: 0.0994, Dropout p: 0.30, Patience: 0/75            
Epoch [0016/0300], Training Loss: 0.3168, Validation Loss: 0.1488, Dropout p: 0.30, Patience: 0/75            
Epoch [0017/0300], Training Loss: 0.3873, Validation Loss: 0.0778, Dropout p: 0.30, Patience: 0/75            
Epoch [0018/0300], Training Loss: 0.3843, Validation Loss: 0.0701, Dropout p: 0.30, Patience: 0/75            
Epoch [0019/0300], Training Loss: 0.3569, Validation Loss: 0.1372, Dropout p: 0.30, Patience: 0/75            
Epoch [0020/0300], Training Loss: 0.2491, Validation Loss: 0.0714, Dropout p: 0.30, Patience: 0/75            
Epoch [0021/0300], Training Loss: 0.3200, Validation Loss: 0.1192, Dropout p: 0.30, Patience: 0/75            
Epoch [0022/0300], Training Loss: 0.2852, Validation Loss: 0.1328, Dropout p: 0.30, Patience: 0/75            
Epoch [0023/0300], Training Loss: 0.5380, Validation Loss: 0.0892, Dropout p: 0.30, Patience: 0/75            
Epoch [0024/0300], Training Loss: 0.2988, Validation Loss: 0.0876, Dropout p: 0.30, Patience: 0/75            
Epoch [0025/0300], Training Loss: 0.2385, Validation Loss: 0.0549, Dropout p: 0.30, Patience: 0/75            
Epoch [0026/0300], Training Loss: 0.2306, Validation Loss: 0.1062, Dropout p: 0.30, Patience: 0/75            
Epoch [0027/0300], Training Loss: 0.2974, Validation Loss: 0.0893, Dropout p: 0.30, Patience: 0/75            
Epoch [0028/0300], Training Loss: 0.2879, Validation Loss: 0.0794, Dropout p: 0.30, Patience: 0/75            
Epoch [0029/0300], Training Loss: 0.2277, Validation Loss: 0.0645, Dropout p: 0.30, Patience: 0/75            
Epoch [0030/0300], Training Loss: 0.2375, Validation Loss: 0.0912, Dropout p: 0.30, Patience: 0/75            
Epoch [0031/0300], Training Loss: 0.1955, Validation Loss: 0.0660, Dropout p: 0.30, Patience: 0/75            
Epoch [0032/0300], Training Loss: 0.3686, Validation Loss: 0.2618, Dropout p: 0.30, Patience: 0/75            
Epoch [0033/0300], Training Loss: 0.2245, Validation Loss: 0.0654, Dropout p: 0.30, Patience: 0/75            
Epoch [0034/0300], Training Loss: 0.2384, Validation Loss: 0.0619, Dropout p: 0.30, Patience: 0/75            
Epoch [0035/0300], Training Loss: 0.2745, Validation Loss: 0.0598, Dropout p: 0.30, Patience: 0/75            
Epoch [0036/0300], Training Loss: 0.3098, Validation Loss: 0.0607, Dropout p: 0.30, Patience: 0/75            
Epoch [0037/0300], Training Loss: 0.2213, Validation Loss: 0.0837, Dropout p: 0.30, Patience: 0/75            
Epoch [0038/0300], Training Loss: 0.2570, Validation Loss: 0.1460, Dropout p: 0.30, Patience: 0/75            
Epoch [0039/0300], Training Loss: 0.1873, Validation Loss: 0.0741, Dropout p: 0.30, Patience: 0/75            
Epoch [0040/0300], Training Loss: 0.2231, Validation Loss: 0.0881, Dropout p: 0.30, Patience: 0/75            
Epoch [0041/0300], Training Loss: 0.1736, Validation Loss: 0.0648, Dropout p: 0.30, Patience: 0/75            
Epoch [0042/0300], Training Loss: 0.2093, Validation Loss: 0.0737, Dropout p: 0.30, Patience: 0/75            
Epoch [0043/0300], Training Loss: 0.1588, Validation Loss: 0.0516, Dropout p: 0.30, Patience: 0/75            
Epoch [0044/0300], Training Loss: 0.2004, Validation Loss: 0.0472, Dropout p: 0.30, Patience: 0/75            
Epoch [0045/0300], Training Loss: 0.2029, Validation Loss: 0.0433, Dropout p: 0.30, Patience: 0/75            
Epoch [0046/0300], Training Loss: 0.2172, Validation Loss: 0.0927, Dropout p: 0.30, Patience: 0/75            
Epoch [0047/0300], Training Loss: 0.1490, Validation Loss: 0.0464, Dropout p: 0.30, Patience: 0/75            
Epoch [0048/0300], Training Loss: 0.3340, Validation Loss: 0.0941, Dropout p: 0.30, Patience: 0/75            
Epoch [0049/0300], Training Loss: 0.1877, Validation Loss: 0.0569, Dropout p: 0.30, Patience: 0/75            
Epoch [0050/0300], Training Loss: 0.2835, Validation Loss: 0.1261, Dropout p: 0.30, Patience: 0/75            
Epoch [0051/0300], Training Loss: 0.1329, Validation Loss: 0.0541, Dropout p: 0.30, Patience: 0/75            
Epoch [0052/0300], Training Loss: 0.1307, Validation Loss: 0.0771, Dropout p: 0.30, Patience: 0/75            
Epoch [0053/0300], Training Loss: 0.1174, Validation Loss: 0.0652, Dropout p: 0.30, Patience: 0/75            
Epoch [0054/0300], Training Loss: 0.1647, Validation Loss: 0.0479, Dropout p: 0.30, Patience: 0/75            
Epoch [0055/0300], Training Loss: 0.1438, Validation Loss: 0.0879, Dropout p: 0.30, Patience: 0/75            
Epoch [0056/0300], Training Loss: 0.1687, Validation Loss: 0.0500, Dropout p: 0.30, Patience: 0/75            
Epoch [0057/0300], Training Loss: 0.2514, Validation Loss: 0.0463, Dropout p: 0.30, Patience: 0/75            
Epoch [0058/0300], Training Loss: 0.1302, Validation Loss: 0.0554, Dropout p: 0.30, Patience: 0/75            
Epoch [0059/0300], Training Loss: 0.1921, Validation Loss: 0.0823, Dropout p: 0.30, Patience: 0/75            
Epoch [0060/0300], Training Loss: 0.1065, Validation Loss: 0.0660, Dropout p: 0.30, Patience: 0/75            
Epoch [0061/0300], Training Loss: 0.1078, Validation Loss: 0.0548, Dropout p: 0.30, Patience: 0/75            
Epoch [0062/0300], Training Loss: 0.1267, Validation Loss: 0.0912, Dropout p: 0.30, Patience: 0/75            
Epoch [0063/0300], Training Loss: 0.1262, Validation Loss: 0.0570, Dropout p: 0.30, Patience: 0/75            
Epoch [0064/0300], Training Loss: 0.1277, Validation Loss: 0.0394, Dropout p: 0.30, Patience: 0/75            
Epoch [0065/0300], Training Loss: 0.1651, Validation Loss: 0.0525, Dropout p: 0.30, Patience: 0/75            
Epoch [0066/0300], Training Loss: 0.1242, Validation Loss: 0.0980, Dropout p: 0.30, Patience: 0/75            
Epoch [0067/0300], Training Loss: 0.1416, Validation Loss: 0.0407, Dropout p: 0.30, Patience: 0/75            
Epoch [0068/0300], Training Loss: 0.1373, Validation Loss: 0.0621, Dropout p: 0.30, Patience: 0/75            
Epoch [0069/0300], Training Loss: 0.1201, Validation Loss: 0.0887, Dropout p: 0.30, Patience: 0/75            
Epoch [0070/0300], Training Loss: 0.1548, Validation Loss: 0.0541, Dropout p: 0.30, Patience: 0/75            
Epoch [0071/0300], Training Loss: 0.1856, Validation Loss: 0.0442, Dropout p: 0.30, Patience: 0/75            
Epoch [0072/0300], Training Loss: 0.1065, Validation Loss: 0.0661, Dropout p: 0.30, Patience: 0/75            
Epoch [0073/0300], Training Loss: 0.1136, Validation Loss: 0.0815, Dropout p: 0.30, Patience: 0/75            
Epoch [0074/0300], Training Loss: 0.1205, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 0/75            
Epoch [0075/0300], Training Loss: 0.0916, Validation Loss: 0.0389, Dropout p: 0.30, Patience: 0/75            
Epoch [0076/0300], Training Loss: 0.0909, Validation Loss: 0.0423, Dropout p: 0.30, Patience: 0/75            
Epoch [0077/0300], Training Loss: 0.1015, Validation Loss: 0.0877, Dropout p: 0.30, Patience: 0/75            
Epoch [0078/0300], Training Loss: 0.0964, Validation Loss: 0.0551, Dropout p: 0.30, Patience: 0/75            
Epoch [0079/0300], Training Loss: 0.1058, Validation Loss: 0.0577, Dropout p: 0.30, Patience: 0/75            
Epoch [0080/0300], Training Loss: 0.0745, Validation Loss: 0.0897, Dropout p: 0.30, Patience: 0/75            
Epoch [0081/0300], Training Loss: 0.1001, Validation Loss: 0.0776, Dropout p: 0.30, Patience: 0/75            
Epoch [0082/0300], Training Loss: 0.0783, Validation Loss: 0.0792, Dropout p: 0.30, Patience: 0/75            
Epoch [0083/0300], Training Loss: 0.0889, Validation Loss: 0.0732, Dropout p: 0.30, Patience: 0/75            
Epoch [0084/0300], Training Loss: 0.1970, Validation Loss: 0.0508, Dropout p: 0.30, Patience: 0/75            
Epoch [0085/0300], Training Loss: 0.0744, Validation Loss: 0.0600, Dropout p: 0.30, Patience: 0/75            
Epoch [0086/0300], Training Loss: 0.1242, Validation Loss: 0.0580, Dropout p: 0.30, Patience: 0/75            
Epoch [0087/0300], Training Loss: 0.1652, Validation Loss: 0.0587, Dropout p: 0.30, Patience: 0/75            
Epoch [0088/0300], Training Loss: 0.0962, Validation Loss: 0.1103, Dropout p: 0.30, Patience: 0/75            
Epoch [0089/0300], Training Loss: 0.0760, Validation Loss: 0.0540, Dropout p: 0.30, Patience: 0/75            
Epoch [0090/0300], Training Loss: 0.0715, Validation Loss: 0.0688, Dropout p: 0.30, Patience: 0/75            
Epoch [0091/0300], Training Loss: 0.0762, Validation Loss: 0.0596, Dropout p: 0.30, Patience: 0/75            
Epoch [0092/0300], Training Loss: 0.0793, Validation Loss: 0.0488, Dropout p: 0.30, Patience: 0/75            
Epoch [0093/0300], Training Loss: 0.0784, Validation Loss: 0.0737, Dropout p: 0.30, Patience: 0/75            
Epoch [0094/0300], Training Loss: 0.0879, Validation Loss: 0.0917, Dropout p: 0.30, Patience: 0/75            
Epoch [0095/0300], Training Loss: 0.0686, Validation Loss: 0.0756, Dropout p: 0.30, Patience: 0/75            
Epoch [0096/0300], Training Loss: 0.0736, Validation Loss: 0.0724, Dropout p: 0.30, Patience: 0/75            
Epoch [0097/0300], Training Loss: 0.0894, Validation Loss: 0.0419, Dropout p: 0.30, Patience: 0/75            
Epoch [0098/0300], Training Loss: 0.0650, Validation Loss: 0.0901, Dropout p: 0.30, Patience: 0/75            
Epoch [0099/0300], Training Loss: 0.0772, Validation Loss: 0.0812, Dropout p: 0.30, Patience: 0/75            
Epoch [0100/0300], Training Loss: 0.0572, Validation Loss: 0.0655, Dropout p: 0.30, Patience: 0/75            
Epoch [0101/0300], Training Loss: 0.0975, Validation Loss: 0.0698, Dropout p: 0.30, Patience: 0/75            
Epoch [0102/0300], Training Loss: 0.0785, Validation Loss: 0.0571, Dropout p: 0.30, Patience: 0/75            
Epoch [0103/0300], Training Loss: 0.2010, Validation Loss: 0.0741, Dropout p: 0.30, Patience: 1/75            
Epoch [0104/0300], Training Loss: 0.0796, Validation Loss: 0.0555, Dropout p: 0.30, Patience: 0/75            
Epoch [0105/0300], Training Loss: 0.0759, Validation Loss: 0.0724, Dropout p: 0.30, Patience: 1/75            
Epoch [0106/0300], Training Loss: 0.0683, Validation Loss: 0.0576, Dropout p: 0.30, Patience: 2/75            
Epoch [0107/0300], Training Loss: 0.0758, Validation Loss: 0.0613, Dropout p: 0.30, Patience: 3/75            
Epoch [0108/0300], Training Loss: 0.0694, Validation Loss: 0.0635, Dropout p: 0.30, Patience: 4/75            
Epoch [0109/0300], Training Loss: 0.0737, Validation Loss: 0.0632, Dropout p: 0.30, Patience: 5/75            
Epoch [0110/0300], Training Loss: 0.0648, Validation Loss: 0.0495, Dropout p: 0.30, Patience: 0/75            
Epoch [0111/0300], Training Loss: 0.0756, Validation Loss: 0.0515, Dropout p: 0.30, Patience: 1/75            
Epoch [0112/0300], Training Loss: 0.0604, Validation Loss: 0.0611, Dropout p: 0.30, Patience: 2/75            
Epoch [0113/0300], Training Loss: 0.0513, Validation Loss: 0.0628, Dropout p: 0.30, Patience: 3/75            
Epoch [0114/0300], Training Loss: 0.0501, Validation Loss: 0.0577, Dropout p: 0.30, Patience: 4/75            
Epoch [0115/0300], Training Loss: 0.0721, Validation Loss: 0.0621, Dropout p: 0.30, Patience: 5/75            
Epoch [0116/0300], Training Loss: 0.0579, Validation Loss: 0.0790, Dropout p: 0.30, Patience: 6/75            
Epoch [0117/0300], Training Loss: 0.0721, Validation Loss: 0.0631, Dropout p: 0.30, Patience: 7/75            
Epoch [0118/0300], Training Loss: 0.0781, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 8/75            
Epoch [0119/0300], Training Loss: 0.0565, Validation Loss: 0.0622, Dropout p: 0.30, Patience: 9/75            
Epoch [0120/0300], Training Loss: 0.0613, Validation Loss: 0.0647, Dropout p: 0.30, Patience: 10/75            
Epoch [0121/0300], Training Loss: 0.0723, Validation Loss: 0.0683, Dropout p: 0.30, Patience: 11/75            
Epoch [0122/0300], Training Loss: 0.0621, Validation Loss: 0.0709, Dropout p: 0.30, Patience: 12/75            
Epoch [0123/0300], Training Loss: 0.0607, Validation Loss: 0.0659, Dropout p: 0.30, Patience: 13/75            
Epoch [0124/0300], Training Loss: 0.0536, Validation Loss: 0.0498, Dropout p: 0.30, Patience: 14/75            
Epoch [0125/0300], Training Loss: 0.2312, Validation Loss: 0.0802, Dropout p: 0.30, Patience: 15/75            
Epoch [0126/0300], Training Loss: 0.0450, Validation Loss: 0.0685, Dropout p: 0.30, Patience: 16/75            
Epoch [0127/0300], Training Loss: 0.0446, Validation Loss: 0.0642, Dropout p: 0.30, Patience: 17/75            
Epoch [0128/0300], Training Loss: 0.0703, Validation Loss: 0.0704, Dropout p: 0.30, Patience: 18/75            
Epoch [0129/0300], Training Loss: 0.0730, Validation Loss: 0.0679, Dropout p: 0.30, Patience: 19/75            
Epoch [0130/0300], Training Loss: 0.0517, Validation Loss: 0.0622, Dropout p: 0.30, Patience: 20/75            
Epoch [0131/0300], Training Loss: 0.0594, Validation Loss: 0.0766, Dropout p: 0.30, Patience: 21/75            
Epoch [0132/0300], Training Loss: 0.0532, Validation Loss: 0.0672, Dropout p: 0.30, Patience: 22/75            
Epoch [0133/0300], Training Loss: 0.0476, Validation Loss: 0.0591, Dropout p: 0.30, Patience: 23/75            
Epoch [0134/0300], Training Loss: 0.1721, Validation Loss: 0.0736, Dropout p: 0.30, Patience: 24/75            
Epoch [0135/0300], Training Loss: 0.0492, Validation Loss: 0.0681, Dropout p: 0.30, Patience: 25/75            
Epoch [0136/0300], Training Loss: 0.0548, Validation Loss: 0.0579, Dropout p: 0.30, Patience: 26/75            
Epoch [0137/0300], Training Loss: 0.0522, Validation Loss: 0.0786, Dropout p: 0.30, Patience: 27/75            
Epoch [0138/0300], Training Loss: 0.0571, Validation Loss: 0.0718, Dropout p: 0.30, Patience: 28/75            
Epoch [0139/0300], Training Loss: 0.0571, Validation Loss: 0.0642, Dropout p: 0.30, Patience: 29/75            
Epoch [0140/0300], Training Loss: 0.0368, Validation Loss: 0.0696, Dropout p: 0.30, Patience: 30/75            
Epoch [0141/0300], Training Loss: 0.0429, Validation Loss: 0.0674, Dropout p: 0.30, Patience: 31/75            
Epoch [0142/0300], Training Loss: 0.0553, Validation Loss: 0.0654, Dropout p: 0.30, Patience: 32/75            
Epoch [0143/0300], Training Loss: 0.0731, Validation Loss: 0.0716, Dropout p: 0.30, Patience: 33/75            
Epoch [0144/0300], Training Loss: 0.0500, Validation Loss: 0.0753, Dropout p: 0.30, Patience: 34/75            
Epoch [0145/0300], Training Loss: 0.0442, Validation Loss: 0.0722, Dropout p: 0.30, Patience: 35/75            
Epoch [0146/0300], Training Loss: 0.0580, Validation Loss: 0.0628, Dropout p: 0.30, Patience: 36/75            
Epoch [0147/0300], Training Loss: 0.0695, Validation Loss: 0.0749, Dropout p: 0.30, Patience: 37/75            
Epoch [0148/0300], Training Loss: 0.0565, Validation Loss: 0.0830, Dropout p: 0.30, Patience: 38/75            
Epoch [0149/0300], Training Loss: 0.0436, Validation Loss: 0.0658, Dropout p: 0.30, Patience: 39/75            
Epoch [0150/0300], Training Loss: 0.0475, Validation Loss: 0.0732, Dropout p: 0.30, Patience: 40/75            
Epoch [0151/0300], Training Loss: 0.0520, Validation Loss: 0.0681, Dropout p: 0.30, Patience: 41/75            
Epoch [0152/0300], Training Loss: 0.0478, Validation Loss: 0.0734, Dropout p: 0.30, Patience: 42/75            
Epoch [0153/0300], Training Loss: 0.0440, Validation Loss: 0.0652, Dropout p: 0.30, Patience: 43/75            
Epoch [0154/0300], Training Loss: 0.0626, Validation Loss: 0.0653, Dropout p: 0.30, Patience: 44/75            
Epoch [0155/0300], Training Loss: 0.0493, Validation Loss: 0.0707, Dropout p: 0.30, Patience: 45/75            
Epoch [0156/0300], Training Loss: 0.0461, Validation Loss: 0.0697, Dropout p: 0.30, Patience: 46/75            
Epoch [0157/0300], Training Loss: 0.0568, Validation Loss: 0.0647, Dropout p: 0.30, Patience: 47/75            
Epoch [0158/0300], Training Loss: 0.2326, Validation Loss: 0.0790, Dropout p: 0.30, Patience: 48/75            
Epoch [0159/0300], Training Loss: 0.0981, Validation Loss: 0.0683, Dropout p: 0.30, Patience: 49/75            
Epoch [0160/0300], Training Loss: 0.0547, Validation Loss: 0.0636, Dropout p: 0.30, Patience: 50/75            
Epoch [0161/0300], Training Loss: 0.2480, Validation Loss: 0.0785, Dropout p: 0.30, Patience: 51/75            
Epoch [0162/0300], Training Loss: 0.0509, Validation Loss: 0.0676, Dropout p: 0.30, Patience: 52/75            
Epoch [0163/0300], Training Loss: 0.0491, Validation Loss: 0.0669, Dropout p: 0.30, Patience: 53/75            
Epoch [0164/0300], Training Loss: 0.0455, Validation Loss: 0.0703, Dropout p: 0.30, Patience: 54/75            
Epoch [0165/0300], Training Loss: 0.0532, Validation Loss: 0.0749, Dropout p: 0.30, Patience: 55/75            
Epoch [0166/0300], Training Loss: 0.0588, Validation Loss: 0.0727, Dropout p: 0.30, Patience: 56/75            
Epoch [0167/0300], Training Loss: 0.0496, Validation Loss: 0.0642, Dropout p: 0.30, Patience: 57/75            
Epoch [0168/0300], Training Loss: 0.0481, Validation Loss: 0.0685, Dropout p: 0.30, Patience: 58/75            
Epoch [0169/0300], Training Loss: 0.0582, Validation Loss: 0.0723, Dropout p: 0.30, Patience: 59/75            
Epoch [0170/0300], Training Loss: 0.0505, Validation Loss: 0.0739, Dropout p: 0.30, Patience: 60/75            
Epoch [0171/0300], Training Loss: 0.0460, Validation Loss: 0.0667, Dropout p: 0.30, Patience: 61/75            
Epoch [0172/0300], Training Loss: 0.0512, Validation Loss: 0.0675, Dropout p: 0.30, Patience: 62/75            
Epoch [0173/0300], Training Loss: 0.0574, Validation Loss: 0.0672, Dropout p: 0.30, Patience: 63/75            
Epoch [0174/0300], Training Loss: 0.0559, Validation Loss: 0.0641, Dropout p: 0.30, Patience: 64/75            
Epoch [0175/0300], Training Loss: 0.0632, Validation Loss: 0.0710, Dropout p: 0.30, Patience: 65/75            
Epoch [0176/0300], Training Loss: 0.0507, Validation Loss: 0.0646, Dropout p: 0.30, Patience: 66/75            
Epoch [0177/0300], Training Loss: 0.1221, Validation Loss: 0.0743, Dropout p: 0.30, Patience: 67/75            
Epoch [0178/0300], Training Loss: 0.0520, Validation Loss: 0.0692, Dropout p: 0.30, Patience: 68/75            
Epoch [0179/0300], Training Loss: 0.0451, Validation Loss: 0.0631, Dropout p: 0.30, Patience: 69/75            
Epoch [0180/0300], Training Loss: 0.0607, Validation Loss: 0.0753, Dropout p: 0.30, Patience: 70/75            
Epoch [0181/0300], Training Loss: 0.0435, Validation Loss: 0.0644, Dropout p: 0.30, Patience: 71/75            
Epoch [0182/0300], Training Loss: 0.0549, Validation Loss: 0.0656, Dropout p: 0.30, Patience: 72/75            
Epoch [0183/0300], Training Loss: 0.0476, Validation Loss: 0.0677, Dropout p: 0.30, Patience: 73/75            
Epoch [0184/0300], Training Loss: 0.0540, Validation Loss: 0.0683, Dropout p: 0.30, Patience: 74/75            
Epoch [0185/0300], Training Loss: 0.0590, Validation Loss: 0.0748, Dropout p: 0.30, Patience: 75/75            

Evaluating model on test set...
Predicted TBV: 213.11	Actual TBV: 222.61	Difference: 9.50
Predicted TBV: 424.68	Actual TBV: 449.16	Difference: 24.48
Predicted TBV: 204.18	Actual TBV: 197.64	Difference: 6.54
Predicted TBV: 179.73	Actual TBV: 162.77	Difference: 16.96
Predicted TBV: 213.46	Actual TBV: 211.20	Difference: 2.26
Predicted TBV: 213.02	Actual TBV: 217.47	Difference: 4.45
Predicted TBV: 156.67	Actual TBV: 138.39	Difference: 18.28
Predicted TBV: 279.16	Actual TBV: 270.78	Difference: 8.38
Predicted TBV: 271.17	Actual TBV: 281.42	Difference: 10.25
Predicted TBV: 173.24	Actual TBV: 152.51	Difference: 20.73
Predicted TBV: 172.15	Actual TBV: 180.57	Difference: 8.42
Predicted TBV: 205.84	Actual TBV: 190.48	Difference: 15.36
Predicted TBV: 218.59	Actual TBV: 229.87	Difference: 11.28
Predicted TBV: 210.65	Actual TBV: 201.10	Difference: 9.55
Predicted TBV: 194.77	Actual TBV: 186.07	Difference: 8.70
Predicted TBV: 248.01	Actual TBV: 243.00	Difference: 5.01
Predicted TBV: 240.05	Actual TBV: 257.03	Difference: 16.98
Predicted TBV: 161.10	Actual TBV: 166.79	Difference: 5.69
Predicted TBV: 196.80	Actual TBV: 205.72	Difference: 8.92
Predicted TBV: 201.37	Actual TBV: 206.72	Difference: 5.35
Predicted TBV: 235.25	Actual TBV: 231.37	Difference: 3.88
Predicted TBV: 230.27	Actual TBV: 221.39	Difference: 8.88
Predicted TBV: 183.83	Actual TBV: 180.80	Difference: 3.03
Predicted TBV: 167.27	Actual TBV: 151.91	Difference: 15.36
Predicted TBV: 234.29	Actual TBV: 253.94	Difference: 19.65
Predicted TBV: 157.58	Actual TBV: 137.38	Difference: 20.20
Predicted TBV: 249.15	Actual TBV: 252.89	Difference: 3.74
Predicted TBV: 191.43	Actual TBV: 206.11	Difference: 14.68
Predicted TBV: 255.95	Actual TBV: 277.75	Difference: 21.80
Predicted TBV: 208.89	Actual TBV: 200.34	Difference: 8.55
Predicted TBV: 191.65	Actual TBV: 195.75	Difference: 4.10
Predicted TBV: 178.25	Actual TBV: 182.91	Difference: 4.66
Predicted TBV: 298.36	Actual TBV: 303.83	Difference: 5.47
Predicted TBV: 187.95	Actual TBV: 184.64	Difference: 3.31
Predicted TBV: 150.01	Actual TBV: 160.92	Difference: 10.91
Predicted TBV: 278.08	Actual TBV: 297.35	Difference: 19.27
Predicted TBV: 167.58	Actual TBV: 180.60	Difference: 13.02
Predicted TBV: 263.76	Actual TBV: 262.10	Difference: 1.66
Predicted TBV: 175.61	Actual TBV: 161.64	Difference: 13.97
Predicted TBV: 233.39	Actual TBV: 249.58	Difference: 16.19

Evaluating model with 30 Bayesian runs...

Evaluating model with transforms and dropout...
Refused raster with error 15.69. TBV: 24.48.
Refused raster with error 10.40. TBV: 8.38.
Refused raster with error 11.00. TBV: 9.55.
Refused raster with error 11.73. TBV: 3.88.
Refused raster with error 12.97. TBV: 14.68.
Refused raster with error 11.21. TBV: 13.97.

Evaluating model with transforms only...
Refused raster with error 8.30. TBV: 24.48.
Refused raster with error 6.05. TBV: 9.55.
Refused raster with error 6.13. TBV: 5.69.
Refused raster with error 6.43. TBV: 13.97.
Refused raster with error 5.76. TBV: 16.19.

Evaluating model with dropout only...
Refused raster with error 12.91. TBV: 24.48.
Refused raster with error 11.17. TBV: 3.88.
Refused raster with error 12.22. TBV: 14.68.
Refused raster with error 10.45. TBV: 13.97.

- - - - - -
Non-bayesian prediction:
Total Raster Count: 49
Mean Absolute Error: 10.89 cc
Standard Deviation: 5.79 cc
Big error count (>30): 0
- - - - - -

Bayesian prediction Dropout + Transforms Bayes:
Refused Raster Count: 6
Refused Raster Percentage: 12.24%
Refusal threshold: 10.35
Mean Absolute Error: 10.67 cc
Standard Deviation: 5.66 cc
Big error count (>30): 0
- - - - - -

Bayesian prediction Transforms Bayes:
Refused Raster Count: 5
Refused Raster Percentage: 10.20%
Refusal threshold: 5.625
Mean Absolute Error: 10.54 cc
Standard Deviation: 5.62 cc
Big error count (>30): 0
- - - - - -

Bayesian prediction Dropout Bayes:
Refused Raster Count: 4
Refused Raster Percentage: 8.16%
Refusal threshold: 9.0
Mean Absolute Error: 10.59 cc
Standard Deviation: 5.54 cc
Big error count (>30): 0
- - - - - -

 - - - Fold 6/6 - - -

Train volume count: 341
Validation volume count: 29
Validation volumes: ['23' '48' '38' '1' '80' '22' '27' '36' '24' '28' '15' '78' '82' '25' '40'
 '31' '29' '7' '41' '33' '76' '30' '87' '77' '79' '43' '47' '35' '16' '44'
 '26' '81' '46' '45' '50' '18' '32' '21' '85' '3' '86']

Epoch [0001/0300], Training Loss: 1.1649, Validation Loss: 0.5664, Dropout p: 0.30, Patience: 0/75            
Epoch [0002/0300], Training Loss: 1.0477, Validation Loss: 0.7154, Dropout p: 0.30, Patience: 0/75            
Epoch [0003/0300], Training Loss: 0.9397, Validation Loss: 1.3217, Dropout p: 0.30, Patience: 0/75            
Epoch [0004/0300], Training Loss: 0.7795, Validation Loss: 0.4729, Dropout p: 0.30, Patience: 0/75            
Epoch [0005/0300], Training Loss: 0.6478, Validation Loss: 0.4750, Dropout p: 0.30, Patience: 0/75            
Epoch [0006/0300], Training Loss: 0.6091, Validation Loss: 0.3298, Dropout p: 0.30, Patience: 0/75            
Epoch [0007/0300], Training Loss: 0.5407, Validation Loss: 0.5471, Dropout p: 0.30, Patience: 0/75            
Epoch [0008/0300], Training Loss: 0.4437, Validation Loss: 0.3430, Dropout p: 0.30, Patience: 0/75            
Epoch [0009/0300], Training Loss: 0.3814, Validation Loss: 0.3817, Dropout p: 0.30, Patience: 0/75            
Epoch [0010/0300], Training Loss: 0.3988, Validation Loss: 0.4173, Dropout p: 0.30, Patience: 0/75            
Epoch [0011/0300], Training Loss: 0.3676, Validation Loss: 0.3995, Dropout p: 0.30, Patience: 0/75            
Epoch [0012/0300], Training Loss: 0.3476, Validation Loss: 0.5366, Dropout p: 0.30, Patience: 0/75            
Epoch [0013/0300], Training Loss: 0.3687, Validation Loss: 0.4081, Dropout p: 0.30, Patience: 0/75            
Epoch [0014/0300], Training Loss: 0.3347, Validation Loss: 0.3902, Dropout p: 0.30, Patience: 0/75            
Epoch [0015/0300], Training Loss: 0.4027, Validation Loss: 0.3907, Dropout p: 0.30, Patience: 0/75            
Epoch [0016/0300], Training Loss: 0.3093, Validation Loss: 0.4084, Dropout p: 0.30, Patience: 0/75            
Epoch [0017/0300], Training Loss: 0.3471, Validation Loss: 0.3877, Dropout p: 0.30, Patience: 0/75            
Epoch [0018/0300], Training Loss: 0.2542, Validation Loss: 0.3633, Dropout p: 0.30, Patience: 0/75            
Epoch [0019/0300], Training Loss: 0.3071, Validation Loss: 0.4586, Dropout p: 0.30, Patience: 0/75            
Epoch [0020/0300], Training Loss: 0.2665, Validation Loss: 0.4271, Dropout p: 0.30, Patience: 0/75            
Epoch [0021/0300], Training Loss: 0.3185, Validation Loss: 0.4073, Dropout p: 0.30, Patience: 0/75            
Epoch [0022/0300], Training Loss: 0.2454, Validation Loss: 0.3261, Dropout p: 0.30, Patience: 0/75            
Epoch [0023/0300], Training Loss: 0.1970, Validation Loss: 0.2765, Dropout p: 0.30, Patience: 0/75            
Epoch [0024/0300], Training Loss: 0.2631, Validation Loss: 0.6081, Dropout p: 0.30, Patience: 0/75            
Epoch [0025/0300], Training Loss: 0.2228, Validation Loss: 0.5248, Dropout p: 0.30, Patience: 0/75            
Epoch [0026/0300], Training Loss: 0.2821, Validation Loss: 0.4567, Dropout p: 0.30, Patience: 0/75            
Epoch [0027/0300], Training Loss: 0.2146, Validation Loss: 0.3854, Dropout p: 0.30, Patience: 0/75            
Epoch [0028/0300], Training Loss: 0.2471, Validation Loss: 0.3912, Dropout p: 0.30, Patience: 0/75            
Epoch [0029/0300], Training Loss: 0.2780, Validation Loss: 0.3943, Dropout p: 0.30, Patience: 0/75            
Epoch [0030/0300], Training Loss: 0.2131, Validation Loss: 0.4394, Dropout p: 0.30, Patience: 0/75            
Epoch [0031/0300], Training Loss: 0.1919, Validation Loss: 0.4463, Dropout p: 0.30, Patience: 0/75            
Epoch [0032/0300], Training Loss: 0.2220, Validation Loss: 0.3596, Dropout p: 0.30, Patience: 0/75            
Epoch [0033/0300], Training Loss: 0.2791, Validation Loss: 0.4442, Dropout p: 0.30, Patience: 0/75            
Epoch [0034/0300], Training Loss: 0.2344, Validation Loss: 0.4168, Dropout p: 0.30, Patience: 0/75            
Epoch [0035/0300], Training Loss: 0.2349, Validation Loss: 0.4194, Dropout p: 0.30, Patience: 0/75            
Epoch [0036/0300], Training Loss: 0.1534, Validation Loss: 0.3947, Dropout p: 0.30, Patience: 0/75            
Epoch [0037/0300], Training Loss: 0.1901, Validation Loss: 0.4328, Dropout p: 0.30, Patience: 0/75            
Epoch [0038/0300], Training Loss: 0.1881, Validation Loss: 0.3728, Dropout p: 0.30, Patience: 0/75            
Epoch [0039/0300], Training Loss: 0.2085, Validation Loss: 0.2518, Dropout p: 0.30, Patience: 0/75            
Epoch [0040/0300], Training Loss: 0.1292, Validation Loss: 0.3990, Dropout p: 0.30, Patience: 0/75            
Epoch [0041/0300], Training Loss: 0.1781, Validation Loss: 0.3139, Dropout p: 0.30, Patience: 0/75            
Epoch [0042/0300], Training Loss: 0.1867, Validation Loss: 0.2445, Dropout p: 0.30, Patience: 0/75            
Epoch [0043/0300], Training Loss: 0.1832, Validation Loss: 0.2679, Dropout p: 0.30, Patience: 0/75            
Epoch [0044/0300], Training Loss: 0.1750, Validation Loss: 0.2849, Dropout p: 0.30, Patience: 0/75            
Epoch [0045/0300], Training Loss: 0.1931, Validation Loss: 0.3558, Dropout p: 0.30, Patience: 0/75            
Epoch [0046/0300], Training Loss: 0.1596, Validation Loss: 0.3689, Dropout p: 0.30, Patience: 0/75            
Epoch [0047/0300], Training Loss: 0.1592, Validation Loss: 0.3662, Dropout p: 0.30, Patience: 0/75            
Epoch [0048/0300], Training Loss: 0.1509, Validation Loss: 0.2078, Dropout p: 0.30, Patience: 0/75            
Epoch [0049/0300], Training Loss: 0.1516, Validation Loss: 0.4226, Dropout p: 0.30, Patience: 0/75            
Epoch [0050/0300], Training Loss: 0.1489, Validation Loss: 0.4944, Dropout p: 0.30, Patience: 0/75            
Epoch [0051/0300], Training Loss: 0.1580, Validation Loss: 0.4364, Dropout p: 0.30, Patience: 0/75            
Epoch [0052/0300], Training Loss: 0.1511, Validation Loss: 0.3711, Dropout p: 0.30, Patience: 0/75            
Epoch [0053/0300], Training Loss: 0.1221, Validation Loss: 0.3668, Dropout p: 0.30, Patience: 0/75            
Epoch [0054/0300], Training Loss: 0.1270, Validation Loss: 0.3700, Dropout p: 0.30, Patience: 0/75            
Epoch [0055/0300], Training Loss: 0.1450, Validation Loss: 0.3919, Dropout p: 0.30, Patience: 0/75            
Epoch [0056/0300], Training Loss: 0.1093, Validation Loss: 0.4025, Dropout p: 0.30, Patience: 0/75            
Epoch [0057/0300], Training Loss: 0.1270, Validation Loss: 0.3876, Dropout p: 0.30, Patience: 0/75            
Epoch [0058/0300], Training Loss: 0.0939, Validation Loss: 0.3695, Dropout p: 0.30, Patience: 0/75            
Epoch [0059/0300], Training Loss: 0.1091, Validation Loss: 0.3768, Dropout p: 0.30, Patience: 0/75            
Epoch [0060/0300], Training Loss: 0.1345, Validation Loss: 0.3759, Dropout p: 0.30, Patience: 0/75            
Epoch [0061/0300], Training Loss: 0.1036, Validation Loss: 0.3705, Dropout p: 0.30, Patience: 0/75            
Epoch [0062/0300], Training Loss: 0.0882, Validation Loss: 0.3870, Dropout p: 0.30, Patience: 0/75            
Epoch [0063/0300], Training Loss: 0.1002, Validation Loss: 0.3755, Dropout p: 0.30, Patience: 0/75            
Epoch [0064/0300], Training Loss: 0.0888, Validation Loss: 0.3466, Dropout p: 0.30, Patience: 0/75            
Epoch [0065/0300], Training Loss: 0.1131, Validation Loss: 0.3566, Dropout p: 0.30, Patience: 0/75            
Epoch [0066/0300], Training Loss: 0.0914, Validation Loss: 0.3775, Dropout p: 0.30, Patience: 0/75            
Epoch [0067/0300], Training Loss: 0.0900, Validation Loss: 0.3480, Dropout p: 0.30, Patience: 0/75            
Epoch [0068/0300], Training Loss: 0.0975, Validation Loss: 0.3851, Dropout p: 0.30, Patience: 0/75            
Epoch [0069/0300], Training Loss: 0.0840, Validation Loss: 0.3850, Dropout p: 0.30, Patience: 0/75            
Epoch [0070/0300], Training Loss: 0.0847, Validation Loss: 0.3710, Dropout p: 0.30, Patience: 0/75            
Epoch [0071/0300], Training Loss: 0.0996, Validation Loss: 0.3204, Dropout p: 0.30, Patience: 0/75            
Epoch [0072/0300], Training Loss: 0.0873, Validation Loss: 0.3738, Dropout p: 0.30, Patience: 0/75            
Epoch [0073/0300], Training Loss: 0.1013, Validation Loss: 0.4635, Dropout p: 0.30, Patience: 0/75            
Epoch [0074/0300], Training Loss: 0.0939, Validation Loss: 0.3776, Dropout p: 0.30, Patience: 0/75            
Epoch [0075/0300], Training Loss: 0.1020, Validation Loss: 0.3445, Dropout p: 0.30, Patience: 0/75            
Epoch [0076/0300], Training Loss: 0.1006, Validation Loss: 0.4040, Dropout p: 0.30, Patience: 0/75            
Epoch [0077/0300], Training Loss: 0.0808, Validation Loss: 0.4037, Dropout p: 0.30, Patience: 0/75            
Epoch [0078/0300], Training Loss: 0.1009, Validation Loss: 0.3894, Dropout p: 0.30, Patience: 0/75            
Epoch [0079/0300], Training Loss: 0.0805, Validation Loss: 0.4208, Dropout p: 0.30, Patience: 0/75            
Epoch [0080/0300], Training Loss: 0.0832, Validation Loss: 0.3443, Dropout p: 0.30, Patience: 0/75            
Epoch [0081/0300], Training Loss: 0.0757, Validation Loss: 0.3585, Dropout p: 0.30, Patience: 0/75            
Epoch [0082/0300], Training Loss: 0.0919, Validation Loss: 0.3538, Dropout p: 0.30, Patience: 0/75            
Epoch [0083/0300], Training Loss: 0.0863, Validation Loss: 0.2846, Dropout p: 0.30, Patience: 0/75            
Epoch [0084/0300], Training Loss: 0.0684, Validation Loss: 0.3246, Dropout p: 0.30, Patience: 0/75            
Epoch [0085/0300], Training Loss: 0.1005, Validation Loss: 0.3682, Dropout p: 0.30, Patience: 0/75            
Epoch [0086/0300], Training Loss: 0.0746, Validation Loss: 0.3816, Dropout p: 0.30, Patience: 0/75            
Epoch [0087/0300], Training Loss: 0.0690, Validation Loss: 0.4015, Dropout p: 0.30, Patience: 0/75            
Epoch [0088/0300], Training Loss: 0.0937, Validation Loss: 0.3564, Dropout p: 0.30, Patience: 0/75            
Epoch [0089/0300], Training Loss: 0.0813, Validation Loss: 0.3427, Dropout p: 0.30, Patience: 0/75            
Epoch [0090/0300], Training Loss: 0.0833, Validation Loss: 0.3291, Dropout p: 0.30, Patience: 0/75            
Epoch [0091/0300], Training Loss: 0.0779, Validation Loss: 0.2277, Dropout p: 0.30, Patience: 0/75            
Epoch [0092/0300], Training Loss: 0.0741, Validation Loss: 0.3853, Dropout p: 0.30, Patience: 0/75            
Epoch [0093/0300], Training Loss: 0.0864, Validation Loss: 0.3806, Dropout p: 0.30, Patience: 0/75            
Epoch [0094/0300], Training Loss: 0.0907, Validation Loss: 0.3808, Dropout p: 0.30, Patience: 0/75            
Epoch [0095/0300], Training Loss: 0.0726, Validation Loss: 0.3409, Dropout p: 0.30, Patience: 0/75            
Epoch [0096/0300], Training Loss: 0.0818, Validation Loss: 0.3536, Dropout p: 0.30, Patience: 0/75            
Epoch [0097/0300], Training Loss: 0.0599, Validation Loss: 0.3400, Dropout p: 0.30, Patience: 0/75            
Epoch [0098/0300], Training Loss: 0.0750, Validation Loss: 0.3774, Dropout p: 0.30, Patience: 0/75            
Epoch [0099/0300], Training Loss: 0.0642, Validation Loss: 0.3635, Dropout p: 0.30, Patience: 0/75            
Epoch [0100/0300], Training Loss: 0.0598, Validation Loss: 0.3712, Dropout p: 0.30, Patience: 0/75            
Epoch [0101/0300], Training Loss: 0.0649, Validation Loss: 0.3563, Dropout p: 0.30, Patience: 0/75            
Epoch [0102/0300], Training Loss: 0.0490, Validation Loss: 0.3718, Dropout p: 0.30, Patience: 1/75            
Epoch [0103/0300], Training Loss: 0.0610, Validation Loss: 0.3771, Dropout p: 0.30, Patience: 2/75            
Epoch [0104/0300], Training Loss: 0.0607, Validation Loss: 0.3865, Dropout p: 0.30, Patience: 3/75            
Epoch [0105/0300], Training Loss: 0.0657, Validation Loss: 0.3712, Dropout p: 0.30, Patience: 4/75            
Epoch [0106/0300], Training Loss: 0.0512, Validation Loss: 0.3367, Dropout p: 0.30, Patience: 0/75            
Epoch [0107/0300], Training Loss: 0.0753, Validation Loss: 0.3789, Dropout p: 0.30, Patience: 1/75            
Epoch [0108/0300], Training Loss: 0.0604, Validation Loss: 0.3592, Dropout p: 0.30, Patience: 2/75            
Epoch [0109/0300], Training Loss: 0.0626, Validation Loss: 0.3635, Dropout p: 0.30, Patience: 3/75            
Epoch [0110/0300], Training Loss: 0.0627, Validation Loss: 0.3705, Dropout p: 0.30, Patience: 4/75            
Epoch [0111/0300], Training Loss: 0.0521, Validation Loss: 0.3627, Dropout p: 0.30, Patience: 5/75            
Epoch [0112/0300], Training Loss: 0.0739, Validation Loss: 0.3565, Dropout p: 0.30, Patience: 6/75            
Epoch [0113/0300], Training Loss: 0.0602, Validation Loss: 0.3493, Dropout p: 0.30, Patience: 7/75            
Epoch [0114/0300], Training Loss: 0.0645, Validation Loss: 0.3405, Dropout p: 0.30, Patience: 8/75            
Epoch [0115/0300], Training Loss: 0.0580, Validation Loss: 0.3638, Dropout p: 0.30, Patience: 9/75            
Epoch [0116/0300], Training Loss: 0.0539, Validation Loss: 0.3371, Dropout p: 0.30, Patience: 10/75            
Epoch [0117/0300], Training Loss: 0.0548, Validation Loss: 0.3365, Dropout p: 0.30, Patience: 0/75            
Epoch [0118/0300], Training Loss: 0.0603, Validation Loss: 0.3450, Dropout p: 0.30, Patience: 1/75            
Epoch [0119/0300], Training Loss: 0.0598, Validation Loss: 0.3660, Dropout p: 0.30, Patience: 2/75            
Epoch [0120/0300], Training Loss: 0.0641, Validation Loss: 0.3434, Dropout p: 0.30, Patience: 3/75            
Epoch [0121/0300], Training Loss: 0.0575, Validation Loss: 0.3498, Dropout p: 0.30, Patience: 4/75            
Epoch [0122/0300], Training Loss: 0.0706, Validation Loss: 0.3507, Dropout p: 0.30, Patience: 5/75            
Epoch [0123/0300], Training Loss: 0.0548, Validation Loss: 0.3695, Dropout p: 0.30, Patience: 6/75            
Epoch [0124/0300], Training Loss: 0.0605, Validation Loss: 0.3555, Dropout p: 0.30, Patience: 7/75            
Epoch [0125/0300], Training Loss: 0.0681, Validation Loss: 0.3668, Dropout p: 0.30, Patience: 8/75            
Epoch [0126/0300], Training Loss: 0.0590, Validation Loss: 0.3568, Dropout p: 0.30, Patience: 9/75            
Epoch [0127/0300], Training Loss: 0.0486, Validation Loss: 0.3653, Dropout p: 0.30, Patience: 10/75            
Epoch [0128/0300], Training Loss: 0.0625, Validation Loss: 0.3533, Dropout p: 0.30, Patience: 11/75            
Epoch [0129/0300], Training Loss: 0.0545, Validation Loss: 0.3681, Dropout p: 0.30, Patience: 12/75            
Epoch [0130/0300], Training Loss: 0.0727, Validation Loss: 0.3578, Dropout p: 0.30, Patience: 13/75            
Epoch [0131/0300], Training Loss: 0.0613, Validation Loss: 0.3626, Dropout p: 0.30, Patience: 14/75            
Epoch [0132/0300], Training Loss: 0.0510, Validation Loss: 0.3545, Dropout p: 0.30, Patience: 15/75            
Epoch [0133/0300], Training Loss: 0.0582, Validation Loss: 0.3617, Dropout p: 0.30, Patience: 16/75            
Epoch [0134/0300], Training Loss: 0.0515, Validation Loss: 0.3662, Dropout p: 0.30, Patience: 17/75            
Epoch [0135/0300], Training Loss: 0.0565, Validation Loss: 0.3466, Dropout p: 0.30, Patience: 18/75            
Epoch [0136/0300], Training Loss: 0.0529, Validation Loss: 0.3523, Dropout p: 0.30, Patience: 19/75            
Epoch [0137/0300], Training Loss: 0.0598, Validation Loss: 0.3558, Dropout p: 0.30, Patience: 20/75            
Epoch [0138/0300], Training Loss: 0.0574, Validation Loss: 0.3693, Dropout p: 0.30, Patience: 21/75            
Epoch [0139/0300], Training Loss: 0.0561, Validation Loss: 0.3672, Dropout p: 0.30, Patience: 22/75            
Epoch [0140/0300], Training Loss: 0.0684, Validation Loss: 0.3555, Dropout p: 0.30, Patience: 23/75            
Epoch [0141/0300], Training Loss: 0.0515, Validation Loss: 0.3620, Dropout p: 0.30, Patience: 24/75            
Epoch [0142/0300], Training Loss: 0.0580, Validation Loss: 0.3649, Dropout p: 0.30, Patience: 25/75            
Epoch [0143/0300], Training Loss: 0.0526, Validation Loss: 0.3622, Dropout p: 0.30, Patience: 26/75            
Epoch [0144/0300], Training Loss: 0.0696, Validation Loss: 0.3731, Dropout p: 0.30, Patience: 27/75            
Epoch [0145/0300], Training Loss: 0.0558, Validation Loss: 0.3605, Dropout p: 0.30, Patience: 28/75            
Epoch [0146/0300], Training Loss: 0.0485, Validation Loss: 0.3566, Dropout p: 0.30, Patience: 29/75            
Epoch [0147/0300], Training Loss: 0.0554, Validation Loss: 0.3599, Dropout p: 0.30, Patience: 30/75            
Epoch [0148/0300], Training Loss: 0.0633, Validation Loss: 0.3563, Dropout p: 0.30, Patience: 31/75            
Epoch [0149/0300], Training Loss: 0.0547, Validation Loss: 0.3594, Dropout p: 0.30, Patience: 32/75            
Epoch [0150/0300], Training Loss: 0.0620, Validation Loss: 0.3506, Dropout p: 0.30, Patience: 33/75            
Epoch [0151/0300], Training Loss: 0.0525, Validation Loss: 0.3450, Dropout p: 0.30, Patience: 34/75            
Epoch [0152/0300], Training Loss: 0.0499, Validation Loss: 0.3700, Dropout p: 0.30, Patience: 35/75            
Epoch [0153/0300], Training Loss: 0.0479, Validation Loss: 0.3667, Dropout p: 0.30, Patience: 36/75            
Epoch [0154/0300], Training Loss: 0.0638, Validation Loss: 0.3507, Dropout p: 0.30, Patience: 37/75            
Epoch [0155/0300], Training Loss: 0.0478, Validation Loss: 0.3519, Dropout p: 0.30, Patience: 38/75            
Epoch [0156/0300], Training Loss: 0.0671, Validation Loss: 0.3571, Dropout p: 0.30, Patience: 39/75            
Epoch [0157/0300], Training Loss: 0.0531, Validation Loss: 0.3531, Dropout p: 0.30, Patience: 40/75            
Epoch [0158/0300], Training Loss: 0.0636, Validation Loss: 0.3637, Dropout p: 0.30, Patience: 41/75            
Epoch [0159/0300], Training Loss: 0.0540, Validation Loss: 0.3539, Dropout p: 0.30, Patience: 42/75            
Epoch [0160/0300], Training Loss: 0.0576, Validation Loss: 0.3542, Dropout p: 0.30, Patience: 43/75            
Epoch [0161/0300], Training Loss: 0.0581, Validation Loss: 0.3525, Dropout p: 0.30, Patience: 44/75            
Epoch [0162/0300], Training Loss: 0.0448, Validation Loss: 0.3585, Dropout p: 0.30, Patience: 45/75            
Epoch [0163/0300], Training Loss: 0.0487, Validation Loss: 0.3483, Dropout p: 0.30, Patience: 46/75            
Epoch [0164/0300], Training Loss: 0.0640, Validation Loss: 0.3594, Dropout p: 0.30, Patience: 47/75            
Epoch [0165/0300], Training Loss: 0.0656, Validation Loss: 0.3605, Dropout p: 0.30, Patience: 48/75            
Epoch [0166/0300], Training Loss: 0.0499, Validation Loss: 0.3422, Dropout p: 0.30, Patience: 49/75            
Epoch [0167/0300], Training Loss: 0.0558, Validation Loss: 0.3646, Dropout p: 0.30, Patience: 50/75            
Epoch [0168/0300], Training Loss: 0.0521, Validation Loss: 0.3618, Dropout p: 0.30, Patience: 51/75            
Epoch [0169/0300], Training Loss: 0.0620, Validation Loss: 0.3641, Dropout p: 0.30, Patience: 52/75            
Epoch [0170/0300], Training Loss: 0.0475, Validation Loss: 0.3484, Dropout p: 0.30, Patience: 53/75            
Epoch [0171/0300], Training Loss: 0.0525, Validation Loss: 0.3586, Dropout p: 0.30, Patience: 54/75            
Epoch [0172/0300], Training Loss: 0.0610, Validation Loss: 0.3516, Dropout p: 0.30, Patience: 55/75            
Epoch [0173/0300], Training Loss: 0.0512, Validation Loss: 0.3655, Dropout p: 0.30, Patience: 56/75            
Epoch [0174/0300], Training Loss: 0.0596, Validation Loss: 0.3482, Dropout p: 0.30, Patience: 57/75            
Epoch [0175/0300], Training Loss: 0.0553, Validation Loss: 0.3579, Dropout p: 0.30, Patience: 58/75            
Epoch [0176/0300], Training Loss: 0.0561, Validation Loss: 0.3576, Dropout p: 0.30, Patience: 59/75            
Epoch [0177/0300], Training Loss: 0.0609, Validation Loss: 0.3601, Dropout p: 0.30, Patience: 60/75            
Epoch [0178/0300], Training Loss: 0.0575, Validation Loss: 0.3623, Dropout p: 0.30, Patience: 61/75            
Epoch [0179/0300], Training Loss: 0.0484, Validation Loss: 0.3612, Dropout p: 0.30, Patience: 62/75            
Epoch [0180/0300], Training Loss: 0.0774, Validation Loss: 0.3510, Dropout p: 0.30, Patience: 63/75            
Epoch [0181/0300], Training Loss: 0.0438, Validation Loss: 0.3601, Dropout p: 0.30, Patience: 64/75            
Epoch [0182/0300], Training Loss: 0.0722, Validation Loss: 0.3606, Dropout p: 0.30, Patience: 65/75            
Epoch [0183/0300], Training Loss: 0.0527, Validation Loss: 0.3639, Dropout p: 0.30, Patience: 66/75            
Epoch [0184/0300], Training Loss: 0.0490, Validation Loss: 0.3533, Dropout p: 0.30, Patience: 67/75            
Epoch [0185/0300], Training Loss: 0.0661, Validation Loss: 0.3591, Dropout p: 0.30, Patience: 68/75            
Epoch [0186/0300], Training Loss: 0.0516, Validation Loss: 0.3506, Dropout p: 0.30, Patience: 69/75            
Epoch [0187/0300], Training Loss: 0.0561, Validation Loss: 0.3494, Dropout p: 0.30, Patience: 70/75            
Epoch [0188/0300], Training Loss: 0.0631, Validation Loss: 0.3484, Dropout p: 0.30, Patience: 71/75            
Epoch [0189/0300], Training Loss: 0.0572, Validation Loss: 0.3618, Dropout p: 0.30, Patience: 72/75            
Epoch [0190/0300], Training Loss: 0.0570, Validation Loss: 0.3472, Dropout p: 0.30, Patience: 73/75            
Epoch [0191/0300], Training Loss: 0.0519, Validation Loss: 0.3577, Dropout p: 0.30, Patience: 74/75            
Epoch [0192/0300], Training Loss: 0.0545, Validation Loss: 0.3563, Dropout p: 0.30, Patience: 75/75            

Evaluating model on test set...
Predicted TBV: 184.32	Actual TBV: 199.14	Difference: 14.82
Predicted TBV: 114.33	Actual TBV: 138.90	Difference: 24.57
Predicted TBV: 250.22	Actual TBV: 237.24	Difference: 12.98
Predicted TBV: 130.73	Actual TBV: 122.49	Difference: 8.24
Predicted TBV: 190.53	Actual TBV: 187.45	Difference: 3.08
Predicted TBV: 108.95	Actual TBV: 101.02	Difference: 7.93
Predicted TBV: 121.52	Actual TBV: 111.96	Difference: 9.56
Predicted TBV: 205.62	Actual TBV: 220.32	Difference: 14.70
Predicted TBV: 81.12	Actual TBV: 144.29	Difference: 63.17
Predicted TBV: 205.67	Actual TBV: 199.69	Difference: 5.98
Predicted TBV: 269.02	Actual TBV: 246.98	Difference: 22.04
Predicted TBV: 266.61	Actual TBV: 266.85	Difference: 0.24
Predicted TBV: 211.48	Actual TBV: 190.90	Difference: 20.58
Predicted TBV: 204.44	Actual TBV: 197.25	Difference: 7.19
Predicted TBV: 265.39	Actual TBV: 236.37	Difference: 29.02
Predicted TBV: 222.83	Actual TBV: 224.11	Difference: 1.28
Predicted TBV: 167.93	Actual TBV: 185.05	Difference: 17.12
Predicted TBV: 208.23	Actual TBV: 204.55	Difference: 3.68
Predicted TBV: 173.74	Actual TBV: 159.53	Difference: 14.21
Predicted TBV: 213.37	Actual TBV: 198.08	Difference: 15.29
Predicted TBV: 224.55	Actual TBV: 211.79	Difference: 12.76
Predicted TBV: 211.69	Actual TBV: 210.79	Difference: 0.90
Predicted TBV: 215.60	Actual TBV: 188.56	Difference: 27.04
Predicted TBV: 230.13	Actual TBV: 228.15	Difference: 1.98
Predicted TBV: 191.63	Actual TBV: 171.22	Difference: 20.41
Predicted TBV: 309.85	Actual TBV: 282.68	Difference: 27.17
Predicted TBV: 187.76	Actual TBV: 190.81	Difference: 3.05
Predicted TBV: 238.12	Actual TBV: 227.74	Difference: 10.38
Predicted TBV: 218.78	Actual TBV: 222.40	Difference: 3.62

Evaluating model with 30 Bayesian runs...

Evaluating model with transforms and dropout...
Refused raster with error 10.84. TBV: 27.04.
Refused raster with error 13.31. TBV: 27.17.
Refused raster with error 14.77. TBV: 10.38.

Evaluating model with transforms only...
Refused raster with error 8.77. TBV: 63.17.
Refused raster with error 6.78. TBV: 27.04.
Refused raster with error 9.58. TBV: 27.17.
Refused raster with error 10.56. TBV: 10.38.

Evaluating model with dropout only...
Refused raster with error 9.08. TBV: 10.38.

- - - - - -
Non-bayesian prediction:
Total Raster Count: 29
Mean Absolute Error: 13.90 cc
Standard Deviation: 12.59 cc
Big error count (>30): 1
Big error mean: 63.17 cc
Big error std: 0.00 cc
- - - - - -

Bayesian prediction Dropout + Transforms Bayes:
Refused Raster Count: 3
Refused Raster Percentage: 10.34%
Refusal threshold: 10.35
Mean Absolute Error: 13.02 cc
Standard Deviation: 12.73 cc
Big error count (>30): 1
Big error mean: 63.17 cc
Big error std: 0.00 cc
- - - - - -

Bayesian prediction Transforms Bayes:
Refused Raster Count: 4
Refused Raster Percentage: 13.79%
Refusal threshold: 5.625
Mean Absolute Error: 11.01 cc
Standard Deviation: 8.00 cc
Big error count (>30): 0
- - - - - -

Bayesian prediction Dropout Bayes:
Refused Raster Count: 1
Refused Raster Percentage: 3.45%
Refusal threshold: 9.0
Mean Absolute Error: 14.02 cc
Standard Deviation: 12.79 cc
Big error count (>30): 1
Big error mean: 63.17 cc
Big error std: 0.00 cc
- - - - - -

